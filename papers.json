[{"content":{"TLDR":"\nRecorded sounds routed in space.\n\nThis began on July 18, 2019.  We were driving on the Bellarine Peninsula, southwest of Melbourne, when, on a whim, I said, \u201cLet\u2019s go down to Swan Bay.\u201d  So we did, and in the car park was a pelican.  It came right up to me, and we had a good eye-to-eye session.  In Hinduism, having eye contact with a statue of a god is called Darshan, and the pelican and I seemed to have that kind of a relationship.  Maybe it wanted food, but it stayed well beyond the time when it was clear that I didn\u2019t have any food to give.  So we just sat there, in silent communion, for about 20 minutes.  It struck me that there was a complete juxtaposition of non-intersecting consciousnesses there.  We were both comfortable in each other\u2019s presence, but as for the details of our communication, I would be at a loss to say.\n\nI have frequently written pieces in the past where various layers of sound are juxtaposed, which layers frequently could be viewed as having nothing to do with each other.  And I\u2019m happy to let those layers of sound co-exist.  Maybe like the pelican and I, they eventually begin to make their own kind of sense.  So here we have five different layers, each of which is made in a different way, and each of which has its own kind of sound motion through space.  The five layers are: One: \u201cFreehand,\u201d which are freehand drawings made in Procreate and then transferred into Virtual ANS3.  That\u2019s a program, based on the ANS synthesizer in Moscow, which allows drawings to be read as spectrograms for sound.  Two: \u201cmiRacks,\u201d which are textures made from multiple microtonal transpositions of electronic melodies made in 2018-9 with Dhalang MG (microtonal synthesis and composing program).  These melodies are placed in Antonio Tuzzi\u2019s \u201cSussudio\u201d multiple sample module in the iOS version of VCVRack, called \u201cmiRack.\u201d  Up to six different versions of each melody can be heard at once \u2013 and these melodies are transposed to pitch levels determined by the original microtonal scale the melody was in.  Three: \u201cPiano Sequences\u201d \u2013 a sampled piano in Thumbjam is tuned to a Harry Partch 29-note-to-the-octave scale \u2013 the one he tuned his Diamond Marimba to.  Two textures are made \u2013 first, a set of chords (mostly stacked fifths), played on a Diamond Marimba style keyboard (made in Lemur); and second, a monophonic melody controlled by the Markov chain generation module in Dhalang MG.  If the first texture seems to contain more than a passing reference to some pieces by Howard Skempton, the second may blushingly nod in the direction of John Cage\u2019s \u201cCheap Imitation,\u201d which is itself a faintly embarrassed cousin of Erik Satie\u2019s \u201cSocrate.\u201d  The two textures are juxtaposed for the final sequence in the piece.  Four: \u201cVirtual ANS Streetscapes.\u201d  For many months now, I\u2019ve been wandering around Google Maps, seeing streetscapes, some familiar, some never seen before.  Occasionally, I\u2019ll take a screen-capture of a particular street scene.  Then, I\u2019ll load those photos into Virtual ANS, and treat the photos, usually with a 3-step process \u2013 edge detect, posterize, and contrast \u2013 sometimes in that order, sometimes in another order.  The result will be to reduce the street photography to a series of outlines, which outlines can be heard as spectrographs \u2013 quite active ones, with a lot of variation in them (depending on the complexity of the photographs).  The three photos used here are Mohawk St in Cohoes, New York; a landscape photo of Johnstown, Pennsylvania, and the corner of 59th and 3rd in Manhattan.  Each produces a differently articulated sound-scape which form structural bases on which to pile the other textures.  Finally, Five: \u201cMusique Concrete Demos,\u201d which are textures made with various real-world sounds, recorded with my iPhone, that I used to demonstrate various sound modification techniques to my classes.  A back-door screen spring, a Navajo Burden Basket, with many small, high-pitched aluminum bells, a malfunctioning Yogurt fridge in a local supermarket, the sound of corn popping, and the sound of ice-cubes being stirred while making iced-tea.  Each one is time stretched, and most of them are equalized quite high to contrast with the other textures (the exception is the Yogurt fridge, which has an insane amount of bass boost added to its already bass heavy texture).  Originally, each of these five layers was then individually processed with the GRM Tools Spaces 3D plugin \u2013 each has its own path around an 8-channel space.  This was intended for a performance on May 22 at the Acousmonium at the INA-GRM in Paris, which features 40 loudspeakers on separate channels.  So with five layers having different independent 8-channel sound routings, the 40 channels of the hall would be totally occupied with moving sounds.  This performance did not happen because the concert was cancelled due to the corona virus.  It is anticipated that the piece will happen at the rescheduled concert on Saturday October 10, circumstances permitting.  However, I also fed each of the five channels of sound independently through the GRM Tools Spaces 3D plugin, this time with 2-channel output, instead of 8-channels.  These stereo processings used different settings and performance techniques than the 8-channel processings did. These five stereo moving-sound channels are then mixed into a stereo mix to make a 2-channel version of the final work, and that\u2019s what is played at this conference.\n\nThe five layers don\u2019t all occur at once \u2013 there are lots of silences within the individual tracks \u2013 but the sound is continuous from beginning to end.  Some of the sounds are abstract, and some veer uncomfortably close to  narrative, but the overall effect is one of formerly unrelated sounds now regarding each other across space and time much like a composer and a pelican in a car park next to Swan Bay in the middle of winter, making their own kind of sense as they sit regarding each other.\n\n20 June 2020, Daylesford, VIC, Warren Burt\n\n","abstract":"\nRecorded sounds routed in space.\n\nThis began on July 18, 2019.  We were driving on the Bellarine Peninsula, southwest of Melbourne, when, on a whim, I said, \u201cLet\u2019s go down to Swan Bay.\u201d  So we did, and in the car park was a pelican.  It came right up to me, and we had a good eye-to-eye session.  In Hinduism, having eye contact with a statue of a god is called Darshan, and the pelican and I seemed to have that kind of a relationship.  Maybe it wanted food, but it stayed well beyond the time when it was clear that I didn\u2019t have any food to give.  So we just sat there, in silent communion, for about 20 minutes.  It struck me that there was a complete juxtaposition of non-intersecting consciousnesses there.  We were both comfortable in each other\u2019s presence, but as for the details of our communication, I would be at a loss to say.\n\nI have frequently written pieces in the past where various layers of sound are juxtaposed, which layers frequently could be viewed as having nothing to do with each other.  And I\u2019m happy to let those layers of sound co-exist.  Maybe like the pelican and I, they eventually begin to make their own kind of sense.  So here we have five different layers, each of which is made in a different way, and each of which has its own kind of sound motion through space.  The five layers are: One: \u201cFreehand,\u201d which are freehand drawings made in Procreate and then transferred into Virtual ANS3.  That\u2019s a program, based on the ANS synthesizer in Moscow, which allows drawings to be read as spectrograms for sound.  Two: \u201cmiRacks,\u201d which are textures made from multiple microtonal transpositions of electronic melodies made in 2018-9 with Dhalang MG (microtonal synthesis and composing program).  These melodies are placed in Antonio Tuzzi\u2019s \u201cSussudio\u201d multiple sample module in the iOS version of VCVRack, called \u201cmiRack.\u201d  Up to six different versions of each melody can be heard at once \u2013 and these melodies are transposed to pitch levels determined by the original microtonal scale the melody was in.  Three: \u201cPiano Sequences\u201d \u2013 a sampled piano in Thumbjam is tuned to a Harry Partch 29-note-to-the-octave scale \u2013 the one he tuned his Diamond Marimba to.  Two textures are made \u2013 first, a set of chords (mostly stacked fifths), played on a Diamond Marimba style keyboard (made in Lemur); and second, a monophonic melody controlled by the Markov chain generation module in Dhalang MG.  If the first texture seems to contain more than a passing reference to some pieces by Howard Skempton, the second may blushingly nod in the direction of John Cage\u2019s \u201cCheap Imitation,\u201d which is itself a faintly embarrassed cousin of Erik Satie\u2019s \u201cSocrate.\u201d  The two textures are juxtaposed for the final sequence in the piece.  Four: \u201cVirtual ANS Streetscapes.\u201d  For many months now, I\u2019ve been wandering around Google Maps, seeing streetscapes, some familiar, some never seen before.  Occasionally, I\u2019ll take a screen-capture of a particular street scene.  Then, I\u2019ll load those photos into Virtual ANS, and treat the photos, usually with a 3-step process \u2013 edge detect, posterize, and contrast \u2013 sometimes in that order, sometimes in another order.  The result will be to reduce the street photography to a series of outlines, which outlines can be heard as spectrographs \u2013 quite active ones, with a lot of variation in them (depending on the complexity of the photographs).  The three photos used here are Mohawk St in Cohoes, New York; a landscape photo of Johnstown, Pennsylvania, and the corner of 59th and 3rd in Manhattan.  Each produces a differently articulated sound-scape which form structural bases on which to pile the other textures.  Finally, Five: \u201cMusique Concrete Demos,\u201d which are textures made with various real-world sounds, recorded with my iPhone, that I used to demonstrate various sound modification techniques to my classes.  A back-door screen spring, a Navajo Burden Basket, with many small, high-pitched aluminum bells, a malfunctioning Yogurt fridge in a local supermarket, the sound of corn popping, and the sound of ice-cubes being stirred while making iced-tea.  Each one is time stretched, and most of them are equalized quite high to contrast with the other textures (the exception is the Yogurt fridge, which has an insane amount of bass boost added to its already bass heavy texture).  Originally, each of these five layers was then individually processed with the GRM Tools Spaces 3D plugin \u2013 each has its own path around an 8-channel space.  This was intended for a performance on May 22 at the Acousmonium at the INA-GRM in Paris, which features 40 loudspeakers on separate channels.  So with five layers having different independent 8-channel sound routings, the 40 channels of the hall would be totally occupied with moving sounds.  This performance did not happen because the concert was cancelled due to the corona virus.  It is anticipated that the piece will happen at the rescheduled concert on Saturday October 10, circumstances permitting.  However, I also fed each of the five channels of sound independently through the GRM Tools Spaces 3D plugin, this time with 2-channel output, instead of 8-channels.  These stereo processings used different settings and performance techniques than the 8-channel processings did. These five stereo moving-sound channels are then mixed into a stereo mix to make a 2-channel version of the final work, and that\u2019s what is played at this conference.\n\nThe five layers don\u2019t all occur at once \u2013 there are lots of silences within the individual tracks \u2013 but the sound is continuous from beginning to end.  Some of the sounds are abstract, and some veer uncomfortably close to  narrative, but the overall effect is one of formerly unrelated sounds now regarding each other across space and time much like a composer and a pelican in a car park next to Swan Bay in the middle of winter, making their own kind of sense as they sit regarding each other.\n\n20 June 2020, Daylesford, VIC, Warren Burt\n\n","authors":["Warren Burt"],"bandcamp":"","bio":"Warren Burt (b 1949): composer, performer, writer, instrument builder, sound poet.  Currently Coordinator of Post Graduate Studies in Music, Box Hill Institute, Melbourne.  Born in the US, moved to Australia in 1975.  Has been involved in music, video, community arts, community radio, education, etc. since arriving.  Currently living and working in Daylesford, Vic.","image":"https://spaces.hightail.com/receive/aVJf7W41c5","keywords":["music"],"pdf_url":"","recs":[],"session":["P4_Thursday"],"sessionpos":["4"],"soundcloud":"","title":"Darshan with a Pelican: Multiplicities (2020)","vimeo":"","website":"http://www.warrenburt.com","youtube":""},"forum":"83","id":"83"},{"content":{"TLDR":"\"Dark Path #4\" is an acousmatic work of electroacoustic music. The sounds used in the piece were recorded in a soundscape dear to the author located in the Italian region of Marche. They were processed then composed together to create the musical work. \"Dark Path #4\" can be defned as a journey through light, shadow, shape, color, drifts and landings.\n\n","abstract":"\"Dark Path #4\" is an acousmatic work of electroacoustic music. The sounds used in the piece were recorded in a soundscape dear to the author located in the Italian region of Marche. They were processed then composed together to create the musical work. \"Dark Path #4\" can be defned as a journey through light, shadow, shape, color, drifts and landings.\n\n","authors":["Anna Terzaroli"],"bandcamp":"","bio":"Anna Terzaroli studied Electronic Music and Composition at the Santa Cecilia Conservatory of Music in Rome, as a composer she is dedicated to contemporary acoustic and electroacoustic music.\n\nHer original works are selected and performed in concerts and festivals in Italy and abroad. Anna Terzaroli currently holds a Professor position in Electronic Music at Italian Conservatories. She is a member of the AIMI (Italian Computer Music Association) board.","image":"","keywords":["music"],"pdf_url":"","recs":[],"session":["P2_Tuesday"],"sessionpos":["3"],"soundcloud":"","title":"Dark Path #4","vimeo":"","website":"","youtube":""},"forum":"99","id":"99"},{"content":{"TLDR":"\u201cWhat gets us into trouble is not what we don\u2019t know. It\u2019s what we know for sure that just ain\u2019t so\u201d-Mark Twain. \u2018Encroaching\u2019 is a piece that deals with an experience of unpleasant truth - human\u2019s attitude to nature, consisting of sonic environment and marine data of four seasons of Haeundae Beach, Busan in 2019. Inspired by the beauty of the sea, the piece is firstly created from the recording of Haeundae beach last year. Such as sounds of waves, people amused and the motorbikes on the road were captured. After then, a film about climate change encouraged me to look back on the moment I simply enjoyed the sea while not knowing the actual condition of the environment. Thus, I used oceanographic observation data of the beach from April to December in 2019 in the piece, then such numerical data were translated into musical data through Supercollider. During the process, I was in the position of experiencer and interpreter as to the collage of what computer-generated marine data and raw sound of the sea deliver. Also, the varying gradations of swaying gestures in the piece manifest composer\u2019s changing experiences towards nature from a feeling of awe to the awareness of problems and become distant from the issues at some point. Therefore, \u2018Encroaching\u2019 is to invite listeners to explore and share demeanours to nature, acknowledging people\u2019s connections and gaps to the environment. The piece was presented on Telematic Festival Earth Day Art Model 2020.","abstract":"\u201cWhat gets us into trouble is not what we don\u2019t know. It\u2019s what we know for sure that just ain\u2019t so\u201d-Mark Twain. \u2018Encroaching\u2019 is a piece that deals with an experience of unpleasant truth - human\u2019s attitude to nature, consisting of sonic environment and marine data of four seasons of Haeundae Beach, Busan in 2019. Inspired by the beauty of the sea, the piece is firstly created from the recording of Haeundae beach last year. Such as sounds of waves, people amused and the motorbikes on the road were captured. After then, a film about climate change encouraged me to look back on the moment I simply enjoyed the sea while not knowing the actual condition of the environment. Thus, I used oceanographic observation data of the beach from April to December in 2019 in the piece, then such numerical data were translated into musical data through Supercollider. During the process, I was in the position of experiencer and interpreter as to the collage of what computer-generated marine data and raw sound of the sea deliver. Also, the varying gradations of swaying gestures in the piece manifest composer\u2019s changing experiences towards nature from a feeling of awe to the awareness of problems and become distant from the issues at some point. Therefore, \u2018Encroaching\u2019 is to invite listeners to explore and share demeanours to nature, acknowledging people\u2019s connections and gaps to the environment. The piece was presented on Telematic Festival Earth Day Art Model 2020.","authors":["Elsa Jaeyoung Park"],"bandcamp":"","bio":"Elsa Jaeyoung Park is a composer based in South Korea. She studied electroacoustic music at the University of Birmingham and Jazz Piano at Seoul Institute of the Arts. Her main interests focus on exploring uncomfortable or ignored emotions primarily in the composition of electroacoustic music and data sonification. Park\u2019s music has been heard in various locations including the UK, Germany, Australia, Korea, and USA. Her music has also been featured in international events such as International Computer Music Conference (ICMC), Seoul International Computer Music Festival (SICMF) and film festivals. Park won the best piece of ICMA 2018 regional award Asia-Oceanic at ICMC.","image":"https://drive.google.com/file/d/1zLnS8_WiOFnVa7EQbCf4z57kE_BaKy4z/view?usp=sharing","keywords":[""],"pdf_url":"","recs":[],"session":["P4_Thursday"],"sessionpos":["2"],"soundcloud":"https://soundcloud.com/elsa-park/encroaching","title":"Encroaching","vimeo":"","website":"http://www.elsapjy.com","youtube":""},"forum":"34","id":"34"},{"content":{"TLDR":"A No-Input Mixing Board (NIMB) is an instrument created by plugging an audio mixer's outputs to its inputs. The internal feedback creates a variety of tones and noises, and the dynamic audio-electrical interactions create potential for an exploratory sound practice.\n\nThe sound can be visualised with an oscilloscope (or in my case, oscilloscope software), and by panning multiple channels of mixer feedback in stereo, the sound can be visualised with a phase scope / lissajous figure.\n\nThis presentation is a mix of demonstration, how-to, and discussion. ","abstract":"A No-Input Mixing Board (NIMB) is an instrument created by plugging an audio mixer's outputs to its inputs. The internal feedback creates a variety of tones and noises, and the dynamic audio-electrical interactions create potential for an exploratory sound practice.\n\nThe sound can be visualised with an oscilloscope (or in my case, oscilloscope software), and by panning multiple channels of mixer feedback in stereo, the sound can be visualised with a phase scope / lissajous figure.\n\nThis presentation is a mix of demonstration, how-to, and discussion. ","authors":["Reuben Ingall"],"bandcamp":"","bio":"Reuben Ingall grew up in Canberra, and studied computer music and interactive digital media at the Centre for New Media Arts at The Australian National University. He makes a range of music with puredata patches, no-input mixer, guitar and voice, found sound, field recordings, etc.\nHis other projects include the monthly experimental music night Soundscapes, the radio program Subsequence, mixing and mastering local bands, and a mashup-DJ act. He has worked on music for installations, theatre, e-publications, film, and dance.","image":"https://www.dropbox.com/s/ndjrl4zh7yywcci/vid%20image.png?dl=0","keywords":["paper"],"pdf_url":"","recs":[],"session":["synth-nota-inter"],"sessionpos":["6"],"soundcloud":"","title":"No-Input Mixing Board","vimeo":"","website":"http://reubeningall.com/","youtube":"https://www.youtube.com/watch?v=v7e_s1le5co"},"forum":"7","id":"7"},{"content":{"TLDR":"Abletweet is a novel Max For Live device which uses the Twitter API and Node.JS to facilitate improvised generative electronic music performances inside Ableton Live. The generation of MIDI sequence data from tweet text content is shaped parametrically by the performer using musically appropriate functions such as scale and rhythmic quantisation. Keywords derived from Sentiment Analysis using the AFINN-165 wordlist and Emoji Sentiment Ranking are used to accent the generated sequences using minor and major triad chords in accordance with their word ranking. Designed with consideration to bi-directionality, existing MIDI clips created by the performer can also be encoded as tweets and sent over twitter to other users and imported into Ableton in real time - harnessing social media as communication platform for creativity, co-creation and performance. Abletweet seeks to both encourage artists to harness social media APIs as a performable bidirectional networking protocol and consider openly accessible data as an available medium with which to create musical works.","abstract":"Abletweet is a novel Max For Live device which uses the Twitter API and Node.JS to facilitate improvised generative electronic music performances inside Ableton Live. The generation of MIDI sequence data from tweet text content is shaped parametrically by the performer using musically appropriate functions such as scale and rhythmic quantisation. Keywords derived from Sentiment Analysis using the AFINN-165 wordlist and Emoji Sentiment Ranking are used to accent the generated sequences using minor and major triad chords in accordance with their word ranking. Designed with consideration to bi-directionality, existing MIDI clips created by the performer can also be encoded as tweets and sent over twitter to other users and imported into Ableton in real time - harnessing social media as communication platform for creativity, co-creation and performance. Abletweet seeks to both encourage artists to harness social media APIs as a performable bidirectional networking protocol and consider openly accessible data as an available medium with which to create musical works.","authors":["James Curtis"],"bandcamp":"","bio":"James Curtis is a graduate researcher and sessional lecturer at RMIT University, Melbourne. Having completed a double degree in Fine Art (Sound and Spatial Practice) and Design (Industrial - First Class Honours) James received the Vice Chancellor's Award for Academic Excellence for his research in sound and interaction design in 2017. James is currently undertaking PhD in the RMIT School of Design on a full-time scholarship award, his research focus is developing Artificial Intelligence-mediated creative design tools for musicians.","image":"https://www.dropbox.com/sh/7l9q57kuokzvku5/AABQPNpyQB7VdFX-sMpxrKeua?dl=0","keywords":["paper"],"pdf_url":"","recs":[],"session":["cult-comm-eco"],"sessionpos":["3"],"soundcloud":"","title":"Abletweet: Harnessing Social Media APIs  for Encoding, Co-Creating and Performing Improvised Generative Electronic Music","vimeo":"","website":"http://www.jcurtis.cc","youtube":""},"forum":"91","id":"91"},{"content":{"TLDR":"Aileron One is the output of a process consisting of cycles of music composition and software development, looking into the integration of human flight and artistic performance.\n\nAltitude serves as the foundational element on which the piece is built. The piece can be considered as a vertical structure rising to approximately 950 metres above the ground. A circular chord progression (Fmaj9 Fmaj9/E Am9 G) repeats along the up vector with instrumentation becoming denser in six, discrete, evenly spaced layers. To add variation and movement, a set of instruments are panned to four evenly spaced compass headings. Total velocity is then used to control the speed of melodic and percussive elements throughout the piece.\n\nThis spatial arrangement leads to interesting temporal-harmonic consequences: as the glider ascends, the repeating chord progression is played in one direction, as the glider descends the progression is played in the other direction. The rate at which the progression moves is dictated by the vertical velocity of the glider. The harmonic progression ascends at a steady rate as the glider is towed into the air, leading to a point of contrast when the tow is released and the glider transitions to a slow, steady descent. The musical effect of these moments is an aesthetic mirroring the dynamics of the aircraft.\n\nThe work as it is presented is part of an active research effort and as such will continue to be refined. We foresee potential to create richer work by maximising mappings between musical elements and input data to result in complex, emergent musical results.","abstract":"Aileron One is the output of a process consisting of cycles of music composition and software development, looking into the integration of human flight and artistic performance.\n\nAltitude serves as the foundational element on which the piece is built. The piece can be considered as a vertical structure rising to approximately 950 metres above the ground. A circular chord progression (Fmaj9 Fmaj9/E Am9 G) repeats along the up vector with instrumentation becoming denser in six, discrete, evenly spaced layers. To add variation and movement, a set of instruments are panned to four evenly spaced compass headings. Total velocity is then used to control the speed of melodic and percussive elements throughout the piece.\n\nThis spatial arrangement leads to interesting temporal-harmonic consequences: as the glider ascends, the repeating chord progression is played in one direction, as the glider descends the progression is played in the other direction. The rate at which the progression moves is dictated by the vertical velocity of the glider. The harmonic progression ascends at a steady rate as the glider is towed into the air, leading to a point of contrast when the tow is released and the glider transitions to a slow, steady descent. The musical effect of these moments is an aesthetic mirroring the dynamics of the aircraft.\n\nThe work as it is presented is part of an active research effort and as such will continue to be refined. We foresee potential to create richer work by maximising mappings between musical elements and input data to result in complex, emergent musical results.","authors":["Robert Jarvis"],"bandcamp":"","bio":"Robert Jarvis is an accomplished audio-visual artist based in Melbourne, Australia. He works across live video performance, music, animation and software development, with a focus on the development of tools for live audio-visual performance. He is currently a PhD candidate at RMIT University where he is exploring the intersection of gliding flight and musical performance.","image":"https://bobzeal.sg3.quickconnect.to/d/s/560821687040389482/YoQoliO5HDhsWYKjgOymVvcZ8HCks6le-GLaA49Z0yAc_","keywords":["music"],"pdf_url":"","recs":[],"session":["C4_Thursday"],"sessionpos":["1"],"soundcloud":"","title":"AILERON ONE","vimeo":"","website":"https://zeal.co","youtube":"https://youtu.be/ywen9LQ6Wn4"},"forum":"65","id":"65"},{"content":{"TLDR":"An improvisation where ambient/drone guitar meets glitchy electronica. Minimalist electric guitar, micro-synths and Axoloti microcontroller are processed using analog and digital stompboxes and further mangled and augmented using Plogue Bidule modular audio software.\n\nThe performer \u201cbegin(s) anywhere\u201d and relies on spontaneity and serendipity, pushing to to the edge of discomfort and vulnerability. Layers of asynchronous loops build to create complex and beautiful textures.\n\nPerformed in one take, no edits. Recorded on 19th June 2020.","abstract":"An improvisation where ambient/drone guitar meets glitchy electronica. Minimalist electric guitar, micro-synths and Axoloti microcontroller are processed using analog and digital stompboxes and further mangled and augmented using Plogue Bidule modular audio software.\n\nThe performer \u201cbegin(s) anywhere\u201d and relies on spontaneity and serendipity, pushing to to the edge of discomfort and vulnerability. Layers of asynchronous loops build to create complex and beautiful textures.\n\nPerformed in one take, no edits. Recorded on 19th June 2020.","authors":["Damian Mason"],"bandcamp":"https://dimunit.bandcamp.com/track/acmc2020","bio":"Damian Mason has been creating and performing music since the early 1990s. He has played in pop/shoegaze, industrial and ambient/electronic bands, and composed for film, TV and theatre. He currently works in community services and makes music on his days off while his kids are at school. His current project Dim Unit has been his main focus for the past 5 years.","image":"https://www.dropbox.com/s/37dztu5qcln4kpe/Dim%20Unit%20ACMC2020%20image.jpg?dl=0","keywords":["music"],"pdf_url":"","recs":[],"session":["P1_Monday"],"sessionpos":["5"],"soundcloud":"","title":"Dim Unit","vimeo":"","website":"http://www.damianmason.com.au","youtube":""},"forum":"92","id":"92"},{"content":{"TLDR":"Between of composition and improvisation (computer generative sounds and algorithmic synthesizer). Blending of ambient, drone, everyday sound / soundscape, and West Java music (gamelan, bonang, saron, and sundanese flute).\n\nComputer-based digital signal processing for manipulation the sound materials in micro sampling, granular synthesis, change the pitch/octave, reverse, play at different speeds, and create droning sounds (less effect / minimalist approach). Inspired by nature, organic structure, climate change, and landscape.","abstract":"Between of composition and improvisation (computer generative sounds and algorithmic synthesizer). Blending of ambient, drone, everyday sound / soundscape, and West Java music (gamelan, bonang, saron, and sundanese flute).\n\nComputer-based digital signal processing for manipulation the sound materials in micro sampling, granular synthesis, change the pitch/octave, reverse, play at different speeds, and create droning sounds (less effect / minimalist approach). Inspired by nature, organic structure, climate change, and landscape.","authors":["Fahmi Mursyid"],"bandcamp":"https://ideologikal.bandcamp.com","bio":"Fahmi Mursyid is a contemporary musician, composer, sound designer and producer based in Bandung, Indonesia. He began releasing recordings under various monikers and labels since 2011 until now. Fahmi uses found objects and computers to create the glitches, sampling, granular synthesis and electronic sound characterising in his work.","image":"41","keywords":["music"],"pdf_url":"","recs":[],"session":["P2_Tuesday"],"sessionpos":["4"],"soundcloud":"","title":"Diantara","vimeo":"","website":"https://www.patreon.com/ideologikal","youtube":""},"forum":"41","id":"41"},{"content":{"TLDR":"Black Summer and Black (Midi) Matter and a pair of performance pieces that explore themes on glitch, error, deconstruction and technology as it pertains to live performance and social themes.","abstract":"Black Summer and Black (Midi) Matter and a pair of performance pieces that explore themes on glitch, error, deconstruction and technology as it pertains to live performance and social themes.","authors":["Mark Oliveiro","Andrew Smith"],"bandcamp":"","bio":"Epic tech, noise rituals, ambience of system overload. Mark is interested in explorations of reconstructing lost or imagined performance traditions.\n\nDr. Mark Oliveiro is Academic Lecture in Music Composition at the Australian Institute of Music.","image":"https://www.dropbox.com/s/jbsziegefkl8lhl/MARKFUN20123.jpg?dl=0","keywords":["music"],"pdf_url":"","recs":[],"session":["TBA"],"sessionpos":["TBA"],"soundcloud":"","title":"Black Summer, Black Midi","vimeo":"","website":"https://markoliveiro.net","youtube":""},"forum":"62","id":"62"},{"content":{"TLDR":"Chromaticity is an installation and performance featuring photographic images, live clarinet improvisation and a generative soundscape incorporating instrumental samples and environmental field recordings from the You Yangs, Victoria, Australia. The images, live improvisation and recordings form a visual and aural installation to create an emotional and environmental response to the landscape and a sense of connection with the essence of the You Yangs.\n\nField recordings of the You Yang environment inform the \u201csonic postcard\u201d nature of Chromaticity\u2019s generative soundscape. Features such as the drone of a major highway, trains, a working quarry, children playing, bush-walking and mountain-bike riding area and other anthropogenic sounds are retained to reveal the current day sonic space.\n\nThe three images of the You Yangs, taken at dawn, meridian and dusk, reveal a dynamic and vivid spectrum of landscape colours. Each image informs a 5-minute movement of the work. A fourth image is projected directly onto the performer - in a sense, immersing them in the landscape.\n\nFor each image, parameters were assigned for sample and note duration, mode, tempo, sound density and pitch. The electromagnetic frequencies of the light spectrum were related to frequencies of sound waves and the saturation of colour determined the intensity of the sounds. Images were mapped and assigned notes with sounds of interest within the field recordings inspiring small motifs and cells to be sampled - performed on a range of instruments (Bass Clarinet, Taegum, Flute and small percussion instruments).\n\nA Max/MSP patch controls playback of field recordings associated with the time of day of each movement and provides a mechanism by which the sampled motifs and cells are triggered to form a harmonic backdrop for the clarinet improvisation. The contours of the landscape, as captured by Millen, are mapped to derive data input for control of parameters within the Max patch, and to suggest target notes for the clarinetist's improvisation in a small window within the patch. Thus, the patch containing this window, along with the images and mapping of their contours becomes a supplementary score for the performer to follow during live performance (along with the composer\u2019s notes describing tonality and motifs of each section).\n\nPremiere:\n\nThe installation was premiered at the 4-hour Geelong After Dark pop-up arts festival held on Friday the 5th of May 2017 between 6 pm and 10 pm by the City of Greater Geelong. Approximately 250 people engaged with the installation during the 4-hour event. There were numerous comments from the public wishing to reconnect and visit a place they hadn't visited in many years or decades, even though they see it on their daily horizon.\n\nThe event was free, and participation and interaction with the artists was encouraged. The audience - young, elderly, dancing in the space, pretending to be trees, absorbed in conversation with artists and other participants, asking questions about music composition, the You Yangs, noise, \u201cear-cleaning\u201d, noise pollution, generative computer programming, anthropogenic sounds and more - helped the implications of the Chromaticity installation crystallise on the night: increased public awareness of You Yangs, environment and the effects of noise pollution; public interaction with artists and their art, public conversations about the creative process be it visual art, photography, music or composition; sparking new interest in contemporary art music, computer programming, improvisation and live performance.","abstract":"Chromaticity is an installation and performance featuring photographic images, live clarinet improvisation and a generative soundscape incorporating instrumental samples and environmental field recordings from the You Yangs, Victoria, Australia. The images, live improvisation and recordings form a visual and aural installation to create an emotional and environmental response to the landscape and a sense of connection with the essence of the You Yangs.\n\nField recordings of the You Yang environment inform the \u201csonic postcard\u201d nature of Chromaticity\u2019s generative soundscape. Features such as the drone of a major highway, trains, a working quarry, children playing, bush-walking and mountain-bike riding area and other anthropogenic sounds are retained to reveal the current day sonic space.\n\nThe three images of the You Yangs, taken at dawn, meridian and dusk, reveal a dynamic and vivid spectrum of landscape colours. Each image informs a 5-minute movement of the work. A fourth image is projected directly onto the performer - in a sense, immersing them in the landscape.\n\nFor each image, parameters were assigned for sample and note duration, mode, tempo, sound density and pitch. The electromagnetic frequencies of the light spectrum were related to frequencies of sound waves and the saturation of colour determined the intensity of the sounds. Images were mapped and assigned notes with sounds of interest within the field recordings inspiring small motifs and cells to be sampled - performed on a range of instruments (Bass Clarinet, Taegum, Flute and small percussion instruments).\n\nA Max/MSP patch controls playback of field recordings associated with the time of day of each movement and provides a mechanism by which the sampled motifs and cells are triggered to form a harmonic backdrop for the clarinet improvisation. The contours of the landscape, as captured by Millen, are mapped to derive data input for control of parameters within the Max patch, and to suggest target notes for the clarinetist's improvisation in a small window within the patch. Thus, the patch containing this window, along with the images and mapping of their contours becomes a supplementary score for the performer to follow during live performance (along with the composer\u2019s notes describing tonality and motifs of each section).\n\nPremiere:\n\nThe installation was premiered at the 4-hour Geelong After Dark pop-up arts festival held on Friday the 5th of May 2017 between 6 pm and 10 pm by the City of Greater Geelong. Approximately 250 people engaged with the installation during the 4-hour event. There were numerous comments from the public wishing to reconnect and visit a place they hadn't visited in many years or decades, even though they see it on their daily horizon.\n\nThe event was free, and participation and interaction with the artists was encouraged. The audience - young, elderly, dancing in the space, pretending to be trees, absorbed in conversation with artists and other participants, asking questions about music composition, the You Yangs, noise, \u201cear-cleaning\u201d, noise pollution, generative computer programming, anthropogenic sounds and more - helped the implications of the Chromaticity installation crystallise on the night: increased public awareness of You Yangs, environment and the effects of noise pollution; public interaction with artists and their art, public conversations about the creative process be it visual art, photography, music or composition; sparking new interest in contemporary art music, computer programming, improvisation and live performance.","authors":["Vicki Hallett","Jem Savage","Ferne Millen"],"bandcamp":"","bio":"**Vicki Hallett** is a composer, musician and sound artist who graduated from the Victorian College of the Arts and the University of Melbourne. She has composed, produced and performed in live concerts, solo recordings ranging from chamber music to sound art and acoustic ecology. Through a unique approach combining acoustic ecology, scientific analysis and innovative performance practices, Hallett reshapes the role of interdisciplinary research. This exploration has led her to develop a collaborative concept with Cornell University's Elephant Listening Project.  In 2017, Hallett attended the international residency, Sonic Mmabolela, where she performed on Mabolel Rock with a pod of Hippopotami.\nWebsite: <https://www.vickihallett.com>\u2028\n\n**Ferne Millen** attended The Victorian College of Arts (Melbourne University) completing a Bachelor in Visual & Performing arts(major. photography & theatre). Millen was a finalist in the 2015 National Photographic Portrait Prize for her photograph of \u201cWho\u2019s that lady?\u201d, has twice been a finalist in the National Photographic Portrait Prize as well as a dual finalist in the 2014 Moran Prize in Sydney. Her clients including Deakin University, Porsche, Citi Power, City of Melbourne, Tennis Australia, Geelong Arts Centre, Universal Music and many individuals, musicians and artists.\n\n**Jem Savage** is a musician and sound artist for improvised and new music. His performances effortlessly blend live processing, looping and interactive visuals with highly developed instrumental techniques-often leveraging proprietary software and hardware devises including iPSi, the Isomorphic Pitch-Shifting Interface. Savage has performed or collaborated with a unique cross-section of improvisers, experimental musicians and composers including the AAO, Gian Slater\u2019s Invenio and Barney McAll. Savage is currently a PhD candidate within the Faculty of VCA and MCM at the University of Melbourne.","image":"29","keywords":["music"],"pdf_url":"","recs":[],"session":["C3_Wednesday"],"sessionpos":["3"],"soundcloud":"","title":"Chromaticity","vimeo":"","website":"http://www.vickihallett.com/","youtube":"https://youtu.be/b4SJwhJBb4c  "},"forum":"29","id":"29"},{"content":{"TLDR":"Confabulation:\n\n1. Engage in conversation; talk.\n2. A memory error defined as the production of fabricated, distorted, or misinterpreted memories about oneself or the world, without the conscious intention to deceive.\nInput sensitivity monitoring coupled to envelope generators and modifiers mapped to parameters within effects units and blocks allows for interplay with the effects themselves.\n\nA conversation can be had with the machine.\n\nThough the saxophone is the primary sound-source and performative control element, the acoustic sound of the instrument itself is not necessarily featured. Choice as to when or if the acoustic sound of the saxophone is involved is made on an improvisational basis in response to the sonic territory of the moment, and forms part of the performative outcome.\n\nConceptually, the work speaks to the tensions inherent in the digital/real world intersection with which we constantly contend.\n\nAccess to the digital world is via familiar objects, phone, tablet, computer, but what lies beyond is ever changing, leading to a form of cognitive dissonance and engaging the mind as it comes to terms with whatever \u201creality\u201d is presented.\n\nA familiar object/sight, the saxophone/player, creates a sound so far removed from expectation that a form of cognitive dissonance is created, the observer is engaged as the mind comes to terms with this novel form of \u201creality\u201d.\n\nSolo confabulation.\n\nPresented are four short improvisational pieces representative of the practice as outlined:\n\n1. Smooth Times in Wowville: Amplitude at input (i.e. sax breath control) tied to several parameters, mostly pitch related. The acoustic sound converses with the machine pixies as they constantly try escape\u2026. This effect is very delicately balanced (and a LOT of fun to play with) small variations in amplitude mapped to big changes allowing the possibility to play almost silently and still create much sound.\n2. Ahoy: Basically a random LFO tied to filter/s creates a harsh rhythm to battle against and play with. Based on Zappa\u2019s guitar sound from the \u201cShip Ahoy\u201d guitar solo.\n3. Diamond Flower: The speed of the \u201cstutter\u201d effect is reliant on amplitude at input, the graph overlay is a real-time capture of the envelope parameter reacting to amplitude of sax through the pickup. A damping factor is applied to the envelope response to allow for more precise control.\n4. Harnk: Synthesis and randomness..due to the nature of the pitch follower and how it reads the complex noise from the sax, the synth doesn\u2019t always go where you\u2019d think it should\u2026.\n\n\n","abstract":"Confabulation:\n\n1. Engage in conversation; talk.\n2. A memory error defined as the production of fabricated, distorted, or misinterpreted memories about oneself or the world, without the conscious intention to deceive.\nInput sensitivity monitoring coupled to envelope generators and modifiers mapped to parameters within effects units and blocks allows for interplay with the effects themselves.\n\nA conversation can be had with the machine.\n\nThough the saxophone is the primary sound-source and performative control element, the acoustic sound of the instrument itself is not necessarily featured. Choice as to when or if the acoustic sound of the saxophone is involved is made on an improvisational basis in response to the sonic territory of the moment, and forms part of the performative outcome.\n\nConceptually, the work speaks to the tensions inherent in the digital/real world intersection with which we constantly contend.\n\nAccess to the digital world is via familiar objects, phone, tablet, computer, but what lies beyond is ever changing, leading to a form of cognitive dissonance and engaging the mind as it comes to terms with whatever \u201creality\u201d is presented.\n\nA familiar object/sight, the saxophone/player, creates a sound so far removed from expectation that a form of cognitive dissonance is created, the observer is engaged as the mind comes to terms with this novel form of \u201creality\u201d.\n\nSolo confabulation.\n\nPresented are four short improvisational pieces representative of the practice as outlined:\n\n1. Smooth Times in Wowville: Amplitude at input (i.e. sax breath control) tied to several parameters, mostly pitch related. The acoustic sound converses with the machine pixies as they constantly try escape\u2026. This effect is very delicately balanced (and a LOT of fun to play with) small variations in amplitude mapped to big changes allowing the possibility to play almost silently and still create much sound.\n2. Ahoy: Basically a random LFO tied to filter/s creates a harsh rhythm to battle against and play with. Based on Zappa\u2019s guitar sound from the \u201cShip Ahoy\u201d guitar solo.\n3. Diamond Flower: The speed of the \u201cstutter\u201d effect is reliant on amplitude at input, the graph overlay is a real-time capture of the envelope parameter reacting to amplitude of sax through the pickup. A damping factor is applied to the envelope response to allow for more precise control.\n4. Harnk: Synthesis and randomness..due to the nature of the pitch follower and how it reads the complex noise from the sax, the synth doesn\u2019t always go where you\u2019d think it should\u2026.\n\n\n","authors":["Carl Polke"],"bandcamp":"","bio":"Carl Polke: A musician/sound-designer/performer living in Melbourne, Australia, whose current performative practise involves free improvisation utilising digital audio effects with alto saxophone as the primary sound source. At a conceptual level it interrogates the relationship between society/the individual and the digital realm. The work is perceived as evolving sound-design, and is presented as both solo and as part of improvisational duo The Confabulations. His project The Endless Guitar Solo received funding from The City of Melbourne Quick Response Arts grant and is currently in development, and he is collaborative/co-artist in the Audible Lockdown project awarded a 2020 Peggy Glanville-Hicks Commission. With over 30 years\u2019 experience in the physical theatre/circus and live theatre industries, he has worked with many of Australia\u2019s leading companies including Circus Oz, Legs on the Wall, Phunktional, The Flying Fruit Fly Circus many more. His work is known for its animated style and theatrical sensibility, and has achieved both national and international recognition, winning a Canadian Jesse award and received Helpmann nominations.","image":"https://drive.google.com/file/d/1vbeogYcREo1Qf9K3epjfNcleRxZ49wNR/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["C4_Thursday"],"sessionpos":["2"],"soundcloud":"","title":"Solo : Confabulate","vimeo":"","website":"https://www.carlpolke.com","youtube":"https://youtu.be/gZsaFtYompo"},"forum":"94","id":"94"},{"content":{"TLDR":"Corrupted Vinyl is an acousmatic, fixed media composition. It is noise based, and programmatic in that it musicalises the sonic deconstruction of a vinyl record player. Spectromorphological processes are central to the sonic structuring of the work, with most manipulations pertaining to sonic artifacts often heard in vinyl reproduction. There are two overarching sections: A (0:00 - 3:33) and B (3:34 - 5:10); with an additional coda (5:11 - 6:23).\n\n","abstract":"Corrupted Vinyl is an acousmatic, fixed media composition. It is noise based, and programmatic in that it musicalises the sonic deconstruction of a vinyl record player. Spectromorphological processes are central to the sonic structuring of the work, with most manipulations pertaining to sonic artifacts often heard in vinyl reproduction. There are two overarching sections: A (0:00 - 3:33) and B (3:34 - 5:10); with an additional coda (5:11 - 6:23).\n\n","authors":["Adam Melzer"],"bandcamp":"","bio":"Adam Melzer is a young, Sydney-based composer. His compositional work is varied, but more recently has focussed in on both acousmatic and experimental music domains. He graduated from the Sydney Conservatorium of Music in 2019 with the result of a First Class Bachelor of Composition (Honours). In addition, he has put on his own concerts as a part of Audioshape; was a featured artist in the November 2019 playlist \"Evolving Waves\", curated by Making Waves New Music; and was a featured artist throughout the 2020 Hibernation Festival with the contribution of three livestreams and three audiovisual works.","image":"2","keywords":["music"],"pdf_url":"","recs":[],"session":["P2_Tuesday"],"sessionpos":["1"],"soundcloud":"","title":"Corrupted Vinyl","vimeo":"","website":"https://www.facebook.com/AdamMelzerMusic/?ref=bookmarks","youtube":""},"forum":"2","id":"2"},{"content":{"TLDR":"For this piece, the inauguration speeches of US presidents. Obama (2009) and Trump (2017) were edited into single words (over 4000 of them). and then placed alphabetically. It is \"constraint music\", more  overtly processual than political. Still, it is impossible not to compare the delivery and content of the two speeches. Perhaps most surprising are the number of words used by both presidents: not just \"and\" (82 occurrences) and \"the\" (75) but also \"blood\", \"God\" and \"winter\".  Given that the duration of the two speeches is over 35 minutes, a lot of space was closed up between the words and in a number of places the audio is accelerated to varying degrees.  For the most part the instrumental parts are derived directly from the audio without any attempt at extra-musical commentary. ","abstract":"For this piece, the inauguration speeches of US presidents. Obama (2009) and Trump (2017) were edited into single words (over 4000 of them). and then placed alphabetically. It is \"constraint music\", more  overtly processual than political. Still, it is impossible not to compare the delivery and content of the two speeches. Perhaps most surprising are the number of words used by both presidents: not just \"and\" (82 occurrences) and \"the\" (75) but also \"blood\", \"God\" and \"winter\".  Given that the duration of the two speeches is over 35 minutes, a lot of space was closed up between the words and in a number of places the audio is accelerated to varying degrees.  For the most part the instrumental parts are derived directly from the audio without any attempt at extra-musical commentary. ","authors":["Kirsten Smith - flute, Lindsay Vickery - bass clarinet, Jameson Feakes - electric guitar, Catherine Ashley - harp, Erik Griswold - prepared piano and Vanessa Tomlinson - percussion"],"bandcamp":"","bio":"This recording was made","image":"https://www.dropbox.com/s/n3m3ru3lej1rv5v/410219616a7877bb080ce42897c99b00.jpg?dl=0","keywords":["music"],"pdf_url":"","recs":[],"session":["P3_Wednesday"],"sessionpos":["2"],"soundcloud":"","title":"torbuammpa [2019] for flute, clarinet, electric guitar, harp, piano, percussion and recording.","vimeo":"","website":"https://www.lindsayvickery.com","youtube":""},"forum":"6","id":"6"},{"content":{"TLDR":"i-c-u-curve2020 is browser-based piece of generative audio-visual art. Navigating to https://tobeheard.github.io/i-c-u-curve2020/ you, the audience will be asked permission to access your camera as the composition begins and you slowly become part of the curve.\nBuilding on from both Unresolved I... and Movement (Wellington, 2019), this new iteration explores humanity and inclusion through the lens of a pandemic. As COVID-19 moves rapidly across the globe, the advice to \u201cflatten the curve\u201d and slow the spread is through social isolation. In this isolation we are asked to consider each other, as within this virus no one is exempt, all humans are at risk of infection. \n\ni-c-u-curve2020 is a generative multimedia installation exploring isolation, inclusion and humanity using real-time data. Through live webcam, the audience is submersed and morphs into the data, creating an evolving soundscape with an evolving visual landscape. \nIn its inaugural iterations, static pieces of text were used to create the sound and visual compositions, a webcam, responded to the viewer by submersing their image within the text. The effect, made the audience complicit in the information held within the work, creating a narrative of inclusion. i-c-u-curve2020 builds on these earlier iterations by using dynamic data, the constantly changing information pulled from the COVID19 API webpage. \n\nArtist Laurie Anderson said, \u201cMy job is to make images and leave the decision- making and conclusion-drawing to other people\u201d. This underscores my motivation to use data as a medium or parameter. Through creative abstraction and methods of visualisation and sonification, the result is a work of generative art that seeks to uncover meaning not necessarily implicit in the raw data. As a multimodal artist a driver for my work is to make the unheard heard and the unseen seen. i-c-u-curve2020 uses COVID19 data as both composition and brush and looks to highlight the ultimate inclusive humanity through the isolationist individual. \nAccessing website API data from COVID19 cases and status for both the sound and vision creates a dynamic real-time generative installation. The piece becomes site responsive as the data is mapped to display the audience. The sound element of the piece is also derived from the data. Case numbers set the parameters for the synthesised tones built within the browsers with Tone.js\n\nThese elements of shifting data give the composition a rhythm and generative evolution through time. The results are driving and rhythmic, as the complexity of the image increases revealing the audience image onscreen and within the composition. \nPicasso once said \u201cThere is no abstract art. You must always start with something. Afterward you can remove all traces of reality.\u201d While i-c-u-curve2020 definitely starts with something, it uses the method of abstraction not to remove reality, but to highlight it. \n\nAccess Notes: https://tobeheard.github.io/i-c-u-curve2020/ This is a live webpage link. The piece can be accessed at anytime by anyone with the link. Viewers will be asked for access to their cameras so as to display the generative visual. The soundscape continues for 12min. The image will continue to draw as long as the page is open. All browsers with the exception of Internet Explorer have been tested and to work (internet explorer is not supported). The sound is best heard on headphones or through a pair of HiFi/studio stereo speakers (laptop and bluetooth not recommended!). Due to the efficiency of different browsers and the general speed of individual internet access and computer processors speed, each audience members experience may be different. It is recommended not to have multiple browsers or windows/tabs open for the fullest experience of this work. Thank you, enjoy.\n","abstract":"i-c-u-curve2020 is browser-based piece of generative audio-visual art. Navigating to https://tobeheard.github.io/i-c-u-curve2020/ you, the audience will be asked permission to access your camera as the composition begins and you slowly become part of the curve.\nBuilding on from both Unresolved I... and Movement (Wellington, 2019), this new iteration explores humanity and inclusion through the lens of a pandemic. As COVID-19 moves rapidly across the globe, the advice to \u201cflatten the curve\u201d and slow the spread is through social isolation. In this isolation we are asked to consider each other, as within this virus no one is exempt, all humans are at risk of infection. \n\ni-c-u-curve2020 is a generative multimedia installation exploring isolation, inclusion and humanity using real-time data. Through live webcam, the audience is submersed and morphs into the data, creating an evolving soundscape with an evolving visual landscape. \nIn its inaugural iterations, static pieces of text were used to create the sound and visual compositions, a webcam, responded to the viewer by submersing their image within the text. The effect, made the audience complicit in the information held within the work, creating a narrative of inclusion. i-c-u-curve2020 builds on these earlier iterations by using dynamic data, the constantly changing information pulled from the COVID19 API webpage. \n\nArtist Laurie Anderson said, \u201cMy job is to make images and leave the decision- making and conclusion-drawing to other people\u201d. This underscores my motivation to use data as a medium or parameter. Through creative abstraction and methods of visualisation and sonification, the result is a work of generative art that seeks to uncover meaning not necessarily implicit in the raw data. As a multimodal artist a driver for my work is to make the unheard heard and the unseen seen. i-c-u-curve2020 uses COVID19 data as both composition and brush and looks to highlight the ultimate inclusive humanity through the isolationist individual. \nAccessing website API data from COVID19 cases and status for both the sound and vision creates a dynamic real-time generative installation. The piece becomes site responsive as the data is mapped to display the audience. The sound element of the piece is also derived from the data. Case numbers set the parameters for the synthesised tones built within the browsers with Tone.js\n\nThese elements of shifting data give the composition a rhythm and generative evolution through time. The results are driving and rhythmic, as the complexity of the image increases revealing the audience image onscreen and within the composition. \nPicasso once said \u201cThere is no abstract art. You must always start with something. Afterward you can remove all traces of reality.\u201d While i-c-u-curve2020 definitely starts with something, it uses the method of abstraction not to remove reality, but to highlight it. \n\nAccess Notes: https://tobeheard.github.io/i-c-u-curve2020/ This is a live webpage link. The piece can be accessed at anytime by anyone with the link. Viewers will be asked for access to their cameras so as to display the generative visual. The soundscape continues for 12min. The image will continue to draw as long as the page is open. All browsers with the exception of Internet Explorer have been tested and to work (internet explorer is not supported). The sound is best heard on headphones or through a pair of HiFi/studio stereo speakers (laptop and bluetooth not recommended!). Due to the efficiency of different browsers and the general speed of individual internet access and computer processors speed, each audience members experience may be different. It is recommended not to have multiple browsers or windows/tabs open for the fullest experience of this work. Thank you, enjoy.\n","authors":["sharyn brand"],"bandcamp":"","bio":"Sharyn Brand is an installation artist and collaborator focusing on sound-based urban ethnography, navigating the complexity of the world by uncovering the hidden and forgotten. In 2019 Sharyn received a Graduate Diploma of Music in Sonic Art from Te K\u014dk\u012b, New Zealand School of Music Victoria University, Wellington.\nCareer highlights: Movement Extract (2019), Te P\u0101taka Toi, Adam Art Gallery Wellington NZ; Colour Labyrinth (2018/19), ArtPlay\u2019s New Ideas Lab; Reflection (2017), Phantasmagoria, Bogong Sound Village; Maybe_Together\u2019s Small Voices Louder Perth International Festival (2017); Renae Shadler\u2019s Can You See What We See? Junction Arts Festival (2016); Blood.Sex.Tears. Women Of the World (2017).","image":"https://drive.google.com/file/d/1cg_JstMvB8lRihxy7HQwrgKb1V4vYqrd/view","keywords":["music"],"pdf_url":"","recs":[],"session":["C1_Monday"],"sessionpos":["4"],"soundcloud":"","title":"i-c-u-curve2020","vimeo":"","website":"https://tobeheard.github.io/i-c-u-curve2020/ ","youtube":""},"forum":"90","id":"90"},{"content":{"TLDR":"I've spent the last 7 years working on an algorithmic music generator within the PureData programming environment\n\nBy the end of this talk,  I want you to know: how to come up with a musical idea, and get a computer to create music that follows along to your musical intent\n\nMy music software allows me to create a completed musical work within 20 minutes, and has enabled the creation of hundreds of musical pieces\n\nI want to share some of the insights into how I get I'm able to use music theory terminology, as a way to compose generative music\n\nThe talk centres around using the 'Clave' or 'Rhythmic Key' as a structural device, to provide a framework to place musical ideas\n\nSome of the other concepts I explore are: harmonic contours, scale quantizers, pre-mediated scale modulation, functional harmony, melodic repetition, chord progressions, rhythmic embellishments, and deterministic-algorithmic music generation","abstract":"I've spent the last 7 years working on an algorithmic music generator within the PureData programming environment\n\nBy the end of this talk,  I want you to know: how to come up with a musical idea, and get a computer to create music that follows along to your musical intent\n\nMy music software allows me to create a completed musical work within 20 minutes, and has enabled the creation of hundreds of musical pieces\n\nI want to share some of the insights into how I get I'm able to use music theory terminology, as a way to compose generative music\n\nThe talk centres around using the 'Clave' or 'Rhythmic Key' as a structural device, to provide a framework to place musical ideas\n\nSome of the other concepts I explore are: harmonic contours, scale quantizers, pre-mediated scale modulation, functional harmony, melodic repetition, chord progressions, rhythmic embellishments, and deterministic-algorithmic music generation","authors":["Matthew Whitley / Skueue"],"bandcamp":"","bio":"My name is Matt, and I compose and perform under the artist name Skueue\n\nI'm a musician that makes generative, microtonal electronic dance music using the PureData programming environment","image":"https://drive.google.com/file/d/1k5GDnBgueRC3Nlr718zQxsu-9WRmMP1k/view?usp=sharing","keywords":["paper"],"pdf_url":"","recs":[],"session":["daws-live-algo"],"sessionpos":["5"],"soundcloud":"","title":"Algorithmic music generation: solving the right problem","vimeo":"","website":"https://skueue.bandcamp.com","youtube":"https://www.youtube.com/watch?v=7zo_iFMyydE"},"forum":"81","id":"81"},{"content":{"TLDR":"In 1977 Laurie Anderson created the tape-bow violin which used magnetic tape in place of horsehair on the violin\u2019s bow (Anderson, 1977). We ask about how this may be implemented with modern technologies, such as Arduino, motion sensors, Ambisonic sound-fields, and nichrome wire. Similar to Anderson, we take a postmodern punk approach to this design to create the P-Bow, or Profanity-Bow. This design allows the user to have two independent sources of control, including linear controllers, binary controllers, direction, and speed. The P-Bow has three parts: the bow, a wrist-mounted enclosure, and the violin. We used 3D-printed enclosures, three Nano-style microcontrollers, two BlueSMiRFs, nichrome wire, buttons, 6- and 9-Degrees-of-Freedom sensors (accelerometer, gyrometer, and compass), aluminium flashing strip, aluminium plate, knurled steel, and an LM317 as a Kelvin sensor. It was programmed in C++ for Arduino (arduino.cc, 2019) and connects via system serial ports to Max/MSP, and Max-for-Live and Ableton (2018). We discuss two preliminary works to chart the potential of the P-Bow, Cockwomble (Rose, 2020) and Tubular (Unknowing, 2020). This paper discusses the design, construction and development of the P-Bow and an evaluation of the current design.\n\nKeywords: Ambisonic; electroacoustic music; punk; interactive music; interface design.\n","abstract":"In 1977 Laurie Anderson created the tape-bow violin which used magnetic tape in place of horsehair on the violin\u2019s bow (Anderson, 1977). We ask about how this may be implemented with modern technologies, such as Arduino, motion sensors, Ambisonic sound-fields, and nichrome wire. Similar to Anderson, we take a postmodern punk approach to this design to create the P-Bow, or Profanity-Bow. This design allows the user to have two independent sources of control, including linear controllers, binary controllers, direction, and speed. The P-Bow has three parts: the bow, a wrist-mounted enclosure, and the violin. We used 3D-printed enclosures, three Nano-style microcontrollers, two BlueSMiRFs, nichrome wire, buttons, 6- and 9-Degrees-of-Freedom sensors (accelerometer, gyrometer, and compass), aluminium flashing strip, aluminium plate, knurled steel, and an LM317 as a Kelvin sensor. It was programmed in C++ for Arduino (arduino.cc, 2019) and connects via system serial ports to Max/MSP, and Max-for-Live and Ableton (2018). We discuss two preliminary works to chart the potential of the P-Bow, Cockwomble (Rose, 2020) and Tubular (Unknowing, 2020). This paper discusses the design, construction and development of the P-Bow and an evaluation of the current design.\n\nKeywords: Ambisonic; electroacoustic music; punk; interactive music; interface design.\n","authors":["Cloud Unknowing","Sophie Rose"],"bandcamp":"","bio":"Cloud Unknowing is a sound artist, sound engineer, maker, musician, and electronics enthusiast. He is studying a Bachelor of Sound Production at Box Hill Institute. Unknowing has worked in theatre and immersive theatre, installation, instrument design and building, and film. He often works with subversive themes with technical challenges, science fiction, and the retrofuture. In his spare time, he builds valve amplifiers, effects pedals, and collaborates with Sophie Rose.\n\nSophie Rose is a doctoral student at the University of Melbourne and contemporary vocals lecturer at Australian Institute of Music. She is a singer, extended vocal technique enthusiast, composer, improviser, performer, and maker. She explores the relationship between creative practice, interactive technology, and embodied cognition in her work. She performs and collaborates regularly with Cloud Unknowing and surrealist music collective Little Songs of the Mutilated.\n","image":"https://www.dropbox.com/s/zkdf3joh62xzdl4/2019%20Performance%20-%20Nothinge%20Gig%20March%2013%2030mins.jpg?dl=0","keywords":["paper"],"pdf_url":"","recs":[],"session":["spatial-perf"],"sessionpos":["3"],"soundcloud":"","title":"P-Bow: a profane electric violin and bow for use in Ambisonic audio environments","vimeo":"","website":"http://ampoule.audio/blog/","youtube":"https://youtu.be/PR0LPOKeQDI"},"forum":"72","id":"72"},{"content":{"TLDR":"In Terry Pratchett\u2019s Feet of Clay, Dorfl is a golem: a living machine who, since his mouth is sealed shut, cannot speak. Towards the end of the book, though, he is bought, freed, and given a voice. \n\nInterested in combining computer music with my experience as a chorister, I\u2019ve created an instrument which aims to do the same. It listens to the sound of someone singing, and deconstructs their voice into a series of harmonics. By randomly piecing these harmonics back together, Dorfl mimics the voice it heard, but adds an eerie quality of its own. To maintain the similarities between Dorfl and a human voice, the instrument slides randomly between notes, which are tuned to a Pythagorean scale.\n\nAs well as demonstrating the sound of a digital voice, Dorfl sings is an exploration of freedom in improvised music. The performers improvise throughout, constrained only by my whims as conductor. By conveying instructions with a small set of gestures, I can restrict the mode from which they can play, the settings of their instruments, and how melodically or atmospherically they should play. Although I never have complete control over how the choristers play, these broad instructions are enough to influence the overall sound and trajectory of the piece. This system was inspired by the political landscape Dorfl encounters: the local benevolent dictator rules in much the same way as I conduct.","abstract":"In Terry Pratchett\u2019s Feet of Clay, Dorfl is a golem: a living machine who, since his mouth is sealed shut, cannot speak. Towards the end of the book, though, he is bought, freed, and given a voice. \n\nInterested in combining computer music with my experience as a chorister, I\u2019ve created an instrument which aims to do the same. It listens to the sound of someone singing, and deconstructs their voice into a series of harmonics. By randomly piecing these harmonics back together, Dorfl mimics the voice it heard, but adds an eerie quality of its own. To maintain the similarities between Dorfl and a human voice, the instrument slides randomly between notes, which are tuned to a Pythagorean scale.\n\nAs well as demonstrating the sound of a digital voice, Dorfl sings is an exploration of freedom in improvised music. The performers improvise throughout, constrained only by my whims as conductor. By conveying instructions with a small set of gestures, I can restrict the mode from which they can play, the settings of their instruments, and how melodically or atmospherically they should play. Although I never have complete control over how the choristers play, these broad instructions are enough to influence the overall sound and trajectory of the piece. This system was inspired by the political landscape Dorfl encounters: the local benevolent dictator rules in much the same way as I conduct.","authors":["Composer and conductor: Abigail Thomas; Performers: Lynden Bassett, Weitong Huang, Jaime Langner, Miles Mclaughlin"],"bandcamp":"","bio":"Abigail is a computer science student at ANU, where she participated in the Laptop Ensemble (LENS) program last year. She has returned to the ensemble as a tutor in 2020. ","image":"","keywords":["music"],"pdf_url":"","recs":[],"session":["C2_Tuesday"],"sessionpos":["TBA"],"soundcloud":"","title":"Dorfl sings","vimeo":"","website":"","youtube":""},"forum":"38","id":"38"},{"content":{"TLDR":"In West Belfast lays the Peace Wall Belfast, a manifestation of multifaced messages on political, religious, and communal ideals represented by physical properties of cement, metal, fences, gates, and artwork. There have been discussions on initiatives to take down the walls, however, this remains a fragile state. When thinking about the connectivity of the surrounding spaces and communities, the placing of the Peace Wall(s) blocks any opportunity of cross-communication and produces disorienting effects. However, through alternative artistic approaches focusing on sound, there can be innovative capabilities of sharing these stories and spaces with spatial audio techniques. To use spatial audio to change the perception of these spaces brings forth alternative periods of reflections from stimulating another sensory tool other than sight. Forming two unique listening experiences that focus on the virtual abilities to combine auditory spaces into an immersive installation environment and binaural soundwalks to design site-specific augmentation of the sonic properties of the Peace Wall\u2019s surrounding spaces. These projects aim at using spatial audio and artistic practice to plan new approaches for conflict transformation in Northern Ireland.","abstract":"In West Belfast lays the Peace Wall Belfast, a manifestation of multifaced messages on political, religious, and communal ideals represented by physical properties of cement, metal, fences, gates, and artwork. There have been discussions on initiatives to take down the walls, however, this remains a fragile state. When thinking about the connectivity of the surrounding spaces and communities, the placing of the Peace Wall(s) blocks any opportunity of cross-communication and produces disorienting effects. However, through alternative artistic approaches focusing on sound, there can be innovative capabilities of sharing these stories and spaces with spatial audio techniques. To use spatial audio to change the perception of these spaces brings forth alternative periods of reflections from stimulating another sensory tool other than sight. Forming two unique listening experiences that focus on the virtual abilities to combine auditory spaces into an immersive installation environment and binaural soundwalks to design site-specific augmentation of the sonic properties of the Peace Wall\u2019s surrounding spaces. These projects aim at using spatial audio and artistic practice to plan new approaches for conflict transformation in Northern Ireland.","authors":["Georgios Varoutsos"],"bandcamp":"","bio":"Georgios Varoutsos (b.1991) is a sonic artist from Montreal, Canada. He is currently completing his Ph.D. studies in Music at the Sonic Arts Research Centre (SARC) at Queen\u2019s University Belfast, Northern Ireland. He has graduated with a Master\u2019s in Research, Pass with Distinction, in Arts & Humanities \u2013 Focus in Sonic Arts at Queen\u2019s University Belfast. He has also completed a BFA with Distinction in Electroacoustic Studies and a BA in Anthropology, both from Concordia University in Montreal, Canada.\n\nHe explores the field of sound through an extensive range of projects and performances. His audio creations derive from different inspirations such as field recordings, digital recordings, amplified sound materials, audio processing, synthesis, and experimental techniques. Georgios is merging his various backgrounds of study into research projects comprising immersive audio, sonification, urban arts, sonic arts, and socially engaged arts. This has been presented by using sound as a platform for cultural storytelling.\n","image":" https://drive.google.com/file/d/1qWfipMiKttzBcdSP-5MlyMN5INbdQUEA/view?usp=sharing","keywords":["paper"],"pdf_url":"","recs":[],"session":["spatial-perf"],"sessionpos":["5"],"soundcloud":"","title":"Peace Wall Belfast: Spatial Audio Representation of Divided Spaces and Soundwalks","vimeo":"","website":"https://www.georgiosvaroutsos.com","youtube":"https://youtu.be/88Q91XPFF_A"},"forum":"96","id":"96"},{"content":{"TLDR":"In West Belfast lays the Peace Wall Belfast, a manifestation of multifaced messages on political, religious, and communal ideals represented by physical properties of cement, metal, fences, gates, and artwork. There have been discussions on initiatives to take down the walls, however, this remains a fragile state. When thinking about the connectivity of the surrounding spaces and communities, the placing of the Peace Wall(s) blocks any opportunity of cross-communication and produces disorienting effects. However, through alternative artistic approaches focusing on sound, there can be innovative capabilities of sharing these stories and spaces with spatial audio techniques. To use spatial audio to change the perception of these spaces brings forth alternative periods of reflections from stimulating another sensory tool other than sight. Forming two unique listening experiences that focus on the virtual abilities to combine auditory spaces into an immersive installation environment and binaural soundwalks to design site-specific augmentation of the sonic properties of the Peace Wall\u2019s surrounding spaces. These projects aim at using spatial audio and artistic practice to plan new approaches for conflict transformation in Northern Ireland.\n\nDivided Spaces by Georgios Varoutsos is an audio immersive piece focusing on the Peace Wall between the Falls and Shankill road in Belfast, Northern Ireland. Representing connectivity of spaces and experiences through sound to highlight Past, Present, and Future relations with the Peace Wall(s). It hosts an immersive sonic round-table discussion on controversial issues concerned with the history of the \u2018Troubles\u2019 and the status of Peace Wall(s) around the country.\n\nThe piece compresses the large surface areas surrounding the Peace Wall into a room listening experience. Demonstrating the separation and isolation from either side\u2019s community spaces.\n","abstract":"In West Belfast lays the Peace Wall Belfast, a manifestation of multifaced messages on political, religious, and communal ideals represented by physical properties of cement, metal, fences, gates, and artwork. There have been discussions on initiatives to take down the walls, however, this remains a fragile state. When thinking about the connectivity of the surrounding spaces and communities, the placing of the Peace Wall(s) blocks any opportunity of cross-communication and produces disorienting effects. However, through alternative artistic approaches focusing on sound, there can be innovative capabilities of sharing these stories and spaces with spatial audio techniques. To use spatial audio to change the perception of these spaces brings forth alternative periods of reflections from stimulating another sensory tool other than sight. Forming two unique listening experiences that focus on the virtual abilities to combine auditory spaces into an immersive installation environment and binaural soundwalks to design site-specific augmentation of the sonic properties of the Peace Wall\u2019s surrounding spaces. These projects aim at using spatial audio and artistic practice to plan new approaches for conflict transformation in Northern Ireland.\n\nDivided Spaces by Georgios Varoutsos is an audio immersive piece focusing on the Peace Wall between the Falls and Shankill road in Belfast, Northern Ireland. Representing connectivity of spaces and experiences through sound to highlight Past, Present, and Future relations with the Peace Wall(s). It hosts an immersive sonic round-table discussion on controversial issues concerned with the history of the \u2018Troubles\u2019 and the status of Peace Wall(s) around the country.\n\nThe piece compresses the large surface areas surrounding the Peace Wall into a room listening experience. Demonstrating the separation and isolation from either side\u2019s community spaces.\n","authors":["Georgios Varoutsos"],"bandcamp":"","bio":"Georgios Varoutsos (b.1991) is a sonic artist from Montreal, Canada. He is currently completing his Ph.D. studies in Music at the Sonic Arts Research Centre (SARC) at Queen\u2019s University Belfast, Northern Ireland. He has graduated with a Master\u2019s in Research, Pass with Distinction, in Arts & Humanities \u2013 Focus in Sonic Arts at Queen\u2019s University Belfast. He has also completed a BFA with Distinction in Electroacoustic Studies and a BA in Anthropology, both from Concordia University in Montreal, Canada.\n\nHe explores the field of sound through an extensive range of projects and performances. His audio creations derive from different inspirations such as field recordings, digital recordings, amplified sound materials, audio processing, synthesis, and experimental techniques. Georgios is merging his various backgrounds of study into research projects comprising immersive audio, sonification, urban arts, sonic arts, and socially engaged arts. This has been presented by using sound as a platform for cultural storytelling.\n","image":"https://drive.google.com/file/d/1kn2P5RpzshmAKOTNYiHj7OKBCPMR7dbt/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["P3_Wednesday"],"sessionpos":["4"],"soundcloud":"https://soundcloud.com/georgiosvaroutsos/sets/divided-spaces","title":"Divided Spaces","vimeo":"","website":"https://www.georgiosvaroutsos.com","youtube":""},"forum":"98","id":"98"},{"content":{"TLDR":"Live streaming of soundscapes has in recent years become increasingly popular in acoustic ecology and ecoacoustics as a means of monitoring ecosystem activity and behaviour. This is particularly true of creative projects, where such livestreams is used in lieu of or in tandem with prerecorded content, often coupled with other ecological data sets that contextually inform audio processing approaches. However, where such projects employ multiple simultaneous livestreams, the soundscapes used are often from geographically separate locations, which overlooks the possibilities offered by a multiperspectival consideration of a singular site.\n\nOne such example of this latter approach is this author\u2019s installation, Strata, for the Sir Robert Helpmann Theatre in Mt Gambier, South Australia. Comprising two systems\u2014a three-tier microphone array at the Naracoorte Caves National Park (at subterranean, ground and canopy levels) and a counterpart speaker array in the Theatre\u2019s courtyard (at ground, mezzanine and roof levels)\u2014the installation draws together distinct soundscape layers of the Naracoorte Caves system. This paper discusses the employment and manipulation of multiple soundscape livestreams in the installation in Max 8, drawing particular attention to the inherent creative acts in streaming (with reference to Baradian agential realism) and the employment of complementary ecological data streams (such as BOM weather data) as a means of driving live audio processing. Potential future pathways for creative applications of soundscape live-streams, particularly those related to Arts-Science collaboration and community engagement, are summarily considered.","abstract":"Live streaming of soundscapes has in recent years become increasingly popular in acoustic ecology and ecoacoustics as a means of monitoring ecosystem activity and behaviour. This is particularly true of creative projects, where such livestreams is used in lieu of or in tandem with prerecorded content, often coupled with other ecological data sets that contextually inform audio processing approaches. However, where such projects employ multiple simultaneous livestreams, the soundscapes used are often from geographically separate locations, which overlooks the possibilities offered by a multiperspectival consideration of a singular site.\n\nOne such example of this latter approach is this author\u2019s installation, Strata, for the Sir Robert Helpmann Theatre in Mt Gambier, South Australia. Comprising two systems\u2014a three-tier microphone array at the Naracoorte Caves National Park (at subterranean, ground and canopy levels) and a counterpart speaker array in the Theatre\u2019s courtyard (at ground, mezzanine and roof levels)\u2014the installation draws together distinct soundscape layers of the Naracoorte Caves system. This paper discusses the employment and manipulation of multiple soundscape livestreams in the installation in Max 8, drawing particular attention to the inherent creative acts in streaming (with reference to Baradian agential realism) and the employment of complementary ecological data streams (such as BOM weather data) as a means of driving live audio processing. Potential future pathways for creative applications of soundscape live-streams, particularly those related to Arts-Science collaboration and community engagement, are summarily considered.","authors":["Jesse Budel"],"bandcamp":"","bio":"Jesse Budel is a composer and sound artist based in South Australia. He recently graduated with a PhD (with Dean\u2019s Commendation for Doctoral Thesis Excellence) at the Elder Conservatorium of Music, The University Of Adelaide, where his research focussed on ecological sound art. Developing works for diverse media and spaces, Jesse\u2019s concert works have performed by the Elder Conservatorium Wind Orchestra (AUS), the Australian String Quartet (AUS) and Corvus Ensemble (AK, USA). A previous Carclew Fellowship and Helpmann Academy Grant recipient, he remains highly active in the South Australian arts scene, curating both the Featherstone Sound Space and Murray Bridge Piano Sanctuary. Jesse currently serves as the Secretary for Australian Forum for Acoustic Ecology and World Forum for Acoustic Ecology.","image":"https://drive.google.com/file/d/1emFoT5DRZw61xJXOAW8hpZ6l4HH6LptY/view?usp=sharing","keywords":["paper"],"pdf_url":"","recs":[],"session":["cult-comm-eco"],"sessionpos":["2"],"soundcloud":"","title":"Braided Stream: Intertwining Soundscape Strata and Ecological Data in South Australia's Limestone Coast","vimeo":"","website":"http://www.jesse-budel.com","youtube":"https://www.youtube.com/watch?v=ysOwzb2uiCI"},"forum":"75","id":"75"},{"content":{"TLDR":"Magnetic Stripes is an audio-visual performance that highlights the potential of algorithmic generation, finding a middle ground between human performance and machine creation. This is achieved through multiple stochastic algorithms controlling various aspects of a performance, with curated specific states created by the artist. By giving control over some aspects of the performance to algorithmic generation, space is made for the artist to have clarity and focus on other musical elements. Magnetic Stripes simultaneously shows the subtlety of human performance and its effect on the experience, and the raw efficiency of machine generation. The generative elements of Magnetic Stripes are made obvious through musical and visual choices, yet are subtle in their control of the performance. All sound, visuals and controls are made with Cycling74\u2019s Max 8 software.\n\nThree probability-based algorithms have been designed for their distinctive effect on the respective musical elements they control. The algorithms are stochastic, but have been created with a rules-based mentality. They are a set of rules that manipulate percentage chances of upcoming musical choices that are inspired by the compositional work of John Cage and Brian Eno. They ensure no certainty in the performance, but possible controlled curves for the artist to react to.\n\nThe performance is a powerful, generative drone exploration. Eight channels of audio fuse together to create subtle ambience and moving sonic sweeps, in this contemplative 10-minute experience. They are six channels of simply affected wave table oscillators, and two noise generators. The resulting serene sound draws inspiration from musicians such as Todd Anderson-Kunert, Hobo Cubes and Ryoji Ikeda. The tones wash over the audience through their simplicity and medium movement. The changes in texture create moving sections and crescendos. Four of the oscillator drones (two triangle waves, one sawtooth and a square wave), feature algorithms that control their pitch, timbre and rhythm changes.\n\nA permutation probability algorithm generates pitch through curated percentages of likelihood. The proceeding notes are based on the preceding choices. For the square and sawtooth oscillators, this is a list of eight potential frequencies each, that are generated by the preceding three choices (starting with a random selection). The pitch choices are across a wide register for depth and variety. The two triangle oscillators have four potential choices, based on the preceding four selections (starting with a random selection). These frequencies are at the lower middle range to create a sense of warm\nundercurrents. The probability percentage lists are individually curated by the artist. The movement between them and the choices are generated by the machine. All of these drones have a second set multiplier that can be individually volume controlled by the artist, creating a possible thicker texture and harmony when necessary.\n\nThe timbre of each generative oscillator is manipulated by an algorithm\nthat prioritises selections based on how often they were made prior. The more a choice is made, the more likely it comes up in the future. Each drone has four potential shifts in the partials of the sound, that all start with equal chance of occurring, but the probabilities move and change, as the performance progresses (as with anything chance-based). This produces a sense of consistency over smaller blocks of time in the work, that the performer reacts to. The result is a feedback loop between performer and machine, that is different every time.\n\nThe rhythms of the drones are each controlled by curated, individual percentage-based probability algorithms. One for the length of each of the drones (four lists of percentages with four choices) and one for the space between each drone (four lists of percentages with four choices). These are set selections of percentages because Magnetic Stripes is meant to be deliberate, ambient and thought provoking.\n\nTwo additional sine wave channels have a constant pitch and no pulse. They create unity to the variety of constantly moving other drones. The sines have a second multiplier that can be controlled by the artist to add further texture. A white noise generator and a clicking-machine like texture (that is a blend of three separate beds); accompany the drones, to add body, where needed in the performance.\n\nThe aspects of Magnetic Stripes that the artist controls in real time are volume of each channel, the mix of each drone\u2019s harmonies, each drone\u2019s ADSR envelope, texture control (if a drone is on or off), and the panning between stereo outputs for each channel. These are musical elements that the artist determined should not be left up to chance, as they can create tension, build and shape to the performance.\n\nThe visuals are constantly changing, brightly coloured shapes, contrasted against a black background and space. They are generative and connected to the four algorithmic channels of audio. The sawtooth channel controls circular, radiating waves; the square drone controls grid-like rectangles with circular corners; the first triangle wave pushes and pulls multiple squares; and the second triangle channel controls haunting white bars and a simple red waveform. The audio of these channels dictates whether it\u2019s on screen and its volume controls their size. The artist\u2019s control of channel panning, determines the x axis starting point of each visual. The timbre changes determine if the visuals have a pixilation effect applied. This creates a sense of unity and synchronicity in the performance between the audio and the visuals.\n\nThe simplicity of the audio channels and the clear visual cues are used to highlight the algorithmic aspects to Magnetic Stripes and the performative controls of the artist. Gestural performative cues are created through audio-visual interaction. The piece sits somewhere between a curated experience, generative work and live performance. Algorithmic art usually either sits in an area of certain control of outcome or created with random at its core. However, in Magnetic Stripes, organisation and prioritisation of certain choices is balanced with chance to create a meaningful experience.","abstract":"Magnetic Stripes is an audio-visual performance that highlights the potential of algorithmic generation, finding a middle ground between human performance and machine creation. This is achieved through multiple stochastic algorithms controlling various aspects of a performance, with curated specific states created by the artist. By giving control over some aspects of the performance to algorithmic generation, space is made for the artist to have clarity and focus on other musical elements. Magnetic Stripes simultaneously shows the subtlety of human performance and its effect on the experience, and the raw efficiency of machine generation. The generative elements of Magnetic Stripes are made obvious through musical and visual choices, yet are subtle in their control of the performance. All sound, visuals and controls are made with Cycling74\u2019s Max 8 software.\n\nThree probability-based algorithms have been designed for their distinctive effect on the respective musical elements they control. The algorithms are stochastic, but have been created with a rules-based mentality. They are a set of rules that manipulate percentage chances of upcoming musical choices that are inspired by the compositional work of John Cage and Brian Eno. They ensure no certainty in the performance, but possible controlled curves for the artist to react to.\n\nThe performance is a powerful, generative drone exploration. Eight channels of audio fuse together to create subtle ambience and moving sonic sweeps, in this contemplative 10-minute experience. They are six channels of simply affected wave table oscillators, and two noise generators. The resulting serene sound draws inspiration from musicians such as Todd Anderson-Kunert, Hobo Cubes and Ryoji Ikeda. The tones wash over the audience through their simplicity and medium movement. The changes in texture create moving sections and crescendos. Four of the oscillator drones (two triangle waves, one sawtooth and a square wave), feature algorithms that control their pitch, timbre and rhythm changes.\n\nA permutation probability algorithm generates pitch through curated percentages of likelihood. The proceeding notes are based on the preceding choices. For the square and sawtooth oscillators, this is a list of eight potential frequencies each, that are generated by the preceding three choices (starting with a random selection). The pitch choices are across a wide register for depth and variety. The two triangle oscillators have four potential choices, based on the preceding four selections (starting with a random selection). These frequencies are at the lower middle range to create a sense of warm\nundercurrents. The probability percentage lists are individually curated by the artist. The movement between them and the choices are generated by the machine. All of these drones have a second set multiplier that can be individually volume controlled by the artist, creating a possible thicker texture and harmony when necessary.\n\nThe timbre of each generative oscillator is manipulated by an algorithm\nthat prioritises selections based on how often they were made prior. The more a choice is made, the more likely it comes up in the future. Each drone has four potential shifts in the partials of the sound, that all start with equal chance of occurring, but the probabilities move and change, as the performance progresses (as with anything chance-based). This produces a sense of consistency over smaller blocks of time in the work, that the performer reacts to. The result is a feedback loop between performer and machine, that is different every time.\n\nThe rhythms of the drones are each controlled by curated, individual percentage-based probability algorithms. One for the length of each of the drones (four lists of percentages with four choices) and one for the space between each drone (four lists of percentages with four choices). These are set selections of percentages because Magnetic Stripes is meant to be deliberate, ambient and thought provoking.\n\nTwo additional sine wave channels have a constant pitch and no pulse. They create unity to the variety of constantly moving other drones. The sines have a second multiplier that can be controlled by the artist to add further texture. A white noise generator and a clicking-machine like texture (that is a blend of three separate beds); accompany the drones, to add body, where needed in the performance.\n\nThe aspects of Magnetic Stripes that the artist controls in real time are volume of each channel, the mix of each drone\u2019s harmonies, each drone\u2019s ADSR envelope, texture control (if a drone is on or off), and the panning between stereo outputs for each channel. These are musical elements that the artist determined should not be left up to chance, as they can create tension, build and shape to the performance.\n\nThe visuals are constantly changing, brightly coloured shapes, contrasted against a black background and space. They are generative and connected to the four algorithmic channels of audio. The sawtooth channel controls circular, radiating waves; the square drone controls grid-like rectangles with circular corners; the first triangle wave pushes and pulls multiple squares; and the second triangle channel controls haunting white bars and a simple red waveform. The audio of these channels dictates whether it\u2019s on screen and its volume controls their size. The artist\u2019s control of channel panning, determines the x axis starting point of each visual. The timbre changes determine if the visuals have a pixilation effect applied. This creates a sense of unity and synchronicity in the performance between the audio and the visuals.\n\nThe simplicity of the audio channels and the clear visual cues are used to highlight the algorithmic aspects to Magnetic Stripes and the performative controls of the artist. Gestural performative cues are created through audio-visual interaction. The piece sits somewhere between a curated experience, generative work and live performance. Algorithmic art usually either sits in an area of certain control of outcome or created with random at its core. However, in Magnetic Stripes, organisation and prioritisation of certain choices is balanced with chance to create a meaningful experience.","authors":["Matthew Davis (Nervous Plaything)"],"bandcamp":"","bio":"Nervous Plaything is an experimental composer, focusing on a textural experience using heavily effected guitars, synthesizers and artist developed technologies. These self-developed technologies\u00a0blur the line between artist and machine; analog and digital, and allow for a more timbral compositional focus. Since 2011, he has performed at a selection of Brisbane venues,\u00a0as well as releasing recordings on\u00a0Bandcamp. More recently, he has been working on\u00a0Live Takes: a weekly, improvised live audio-visual collection\u00a0released on Vimeo. This\u00a0has been performed in the\u00a0Precursor\u00a0experimental art series, and envisioned as\u00a0Now-What\u00a0at\u00a0Backbone Youth Arts\u2019 Future30\u00a0festival.\u00a0","image":"56","keywords":["music"],"pdf_url":"","recs":[],"session":["C2_Tuesday"],"sessionpos":["TBA"],"soundcloud":"","title":"Magnetic Stripes","vimeo":"https://vimeo.com/429792651","website":"https://www.facebook.com/nervousplaything/","youtube":""},"forum":"56","id":"56"},{"content":{"TLDR":"Music for videogame play is a deeply connected notion of the development of both the visual action and the evolution of human interaction. The narrative action and human interaction evident in the Pokemon franchise is supported by the musical design. The pokemon videogame series saw humble beginnings in February 1996 as a pair of videogames for Nintendos Game Boy console. Despite the various differences and developments in the existing generations of the video games, there is a perceptual evolution in the musical design, of particular note is the contrast and development in the theme tune, as well as a consistency in the motivic materials, throughout the expansive series. This research adopts a case-study methodology for testing its theory against a real-world application to music for video game design. This theory is that of consolidated unity and contrast within parallel notions of visual and musical design, as applied to interaction and narrative. The characters within this franchise, the Pok\u00e9mon, evolve through the interaction with the human player, just as the theme tune evolves in the macro structure of this multi-generation video game franchise. The theory proposed concerning this video game is two-fold: (1) the central concept in the Pok\u00e9mon video game narrative is: character evolution; (2) the salient consideration for the design of music in this Pok\u00e9mon game is: evolution or thematic development. Out of all the music in Pok\u00e9mon, the main theme of the title track of each game is arguably the most iconic and well known. The leady melody created by Junichi Masuda makes an appearance in every generation\u2019s title theme till date. In this paper, I will be discussing about the evolution of theme music in the Pok\u00e9mon video games focusing mainly on the musical texture and aesthetic. ","abstract":"Music for videogame play is a deeply connected notion of the development of both the visual action and the evolution of human interaction. The narrative action and human interaction evident in the Pokemon franchise is supported by the musical design. The pokemon videogame series saw humble beginnings in February 1996 as a pair of videogames for Nintendos Game Boy console. Despite the various differences and developments in the existing generations of the video games, there is a perceptual evolution in the musical design, of particular note is the contrast and development in the theme tune, as well as a consistency in the motivic materials, throughout the expansive series. This research adopts a case-study methodology for testing its theory against a real-world application to music for video game design. This theory is that of consolidated unity and contrast within parallel notions of visual and musical design, as applied to interaction and narrative. The characters within this franchise, the Pok\u00e9mon, evolve through the interaction with the human player, just as the theme tune evolves in the macro structure of this multi-generation video game franchise. The theory proposed concerning this video game is two-fold: (1) the central concept in the Pok\u00e9mon video game narrative is: character evolution; (2) the salient consideration for the design of music in this Pok\u00e9mon game is: evolution or thematic development. Out of all the music in Pok\u00e9mon, the main theme of the title track of each game is arguably the most iconic and well known. The leady melody created by Junichi Masuda makes an appearance in every generation\u2019s title theme till date. In this paper, I will be discussing about the evolution of theme music in the Pok\u00e9mon video games focusing mainly on the musical texture and aesthetic. ","authors":["Madhuri Suresh"],"bandcamp":"","bio":"","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["musico-comp"],"sessionpos":["4"],"soundcloud":"","title":"The Evolution of Music in Pok\u00e9mon Console Games: How does musical texture and aesthetic of the Pok\u00e9mon videogame theme evolve from 1996- 2017?","vimeo":"","website":"","youtube":""},"forum":"68","id":"68"},{"content":{"TLDR":"My motivation for this work is to evolve the drum kit into something that operates in a way far removed from its origin. More of a timbral drone maker than a traditional drum. Using the computer as a sound processor, I take incoming audio from contact microphones connected to multiple drums (played by me) and feed it though a network of effects and resonating algorithms. I create feedback loops through amplifiers and acoustic drums and use this as live source material routed through the computer via various digital and hardware processors. This process creates a series of constantly evolving rhythmic and timbral landscapes all driven by the acoustic drums.\n\nI create an undulating ecosystem of sound that completely envelopes the listener. Many of the algorithmic processes I use are random and as such, every performance is different. I am constantly interacting with the computer; playing with it and playing against it in order to sculpt the arc of the performance. Despite being improvised, this particular work can also be tweaked to operate autonomously as an installation.\n\nPerforming on an instrument as resonant and physical as the drums, I am fascinated with how they \u2018feel\u2019 to play. The way a timbre of a particular note feels under my hands or feet is just as important as how it \u2018sounds\u2019. The issue of physicality and feel is an important one when it comes to music made on the computer and one which I think is a constant problem for performers in that arena. I am working on bridging that gap for myself by using acoustic drums but by also having amps next to me while I perform. I have been a drummer all my life I and have spent a large proportion of my time on stage next to amplifiers. Not only do they serve a utilitarian function in the music in terms of creating feedback loops to use as audio input for the computer but they help give me a very real and physical sense of what is going on in the music.\n\nAs someone who has never truely felt like they \u2018fit in\u2019 with any musical genre or scene, making solo work has really been the only way to truly find my voice as an artist. This particular solo work is just one part of what I do. It is the honest voice of someone trying to evolve the percussive art form through the use of computers whilst maintaining a very strong emotional and physical connection to the practice of making music.\n\n[Video](https://www.nicholasmeredith.com/watch)\n\n[Audio](https://www.nicholasmeredith.com/listen)\n\n","abstract":"My motivation for this work is to evolve the drum kit into something that operates in a way far removed from its origin. More of a timbral drone maker than a traditional drum. Using the computer as a sound processor, I take incoming audio from contact microphones connected to multiple drums (played by me) and feed it though a network of effects and resonating algorithms. I create feedback loops through amplifiers and acoustic drums and use this as live source material routed through the computer via various digital and hardware processors. This process creates a series of constantly evolving rhythmic and timbral landscapes all driven by the acoustic drums.\n\nI create an undulating ecosystem of sound that completely envelopes the listener. Many of the algorithmic processes I use are random and as such, every performance is different. I am constantly interacting with the computer; playing with it and playing against it in order to sculpt the arc of the performance. Despite being improvised, this particular work can also be tweaked to operate autonomously as an installation.\n\nPerforming on an instrument as resonant and physical as the drums, I am fascinated with how they \u2018feel\u2019 to play. The way a timbre of a particular note feels under my hands or feet is just as important as how it \u2018sounds\u2019. The issue of physicality and feel is an important one when it comes to music made on the computer and one which I think is a constant problem for performers in that arena. I am working on bridging that gap for myself by using acoustic drums but by also having amps next to me while I perform. I have been a drummer all my life I and have spent a large proportion of my time on stage next to amplifiers. Not only do they serve a utilitarian function in the music in terms of creating feedback loops to use as audio input for the computer but they help give me a very real and physical sense of what is going on in the music.\n\nAs someone who has never truely felt like they \u2018fit in\u2019 with any musical genre or scene, making solo work has really been the only way to truly find my voice as an artist. This particular solo work is just one part of what I do. It is the honest voice of someone trying to evolve the percussive art form through the use of computers whilst maintaining a very strong emotional and physical connection to the practice of making music.\n\n[Video](https://www.nicholasmeredith.com/watch)\n\n[Audio](https://www.nicholasmeredith.com/listen)\n\n","authors":["Nicholas Meredith"],"bandcamp":"","bio":"Nicholas Meredith\u2019s sound-world, although wet through with electronic and digital elements, draws significantly from the natural world: monolithic structures, human biology, water. Working with themes of uncertainty and environmental change.\nHis music is deeply personal and highly visceral. Moving from moments of stillness to complete sonic destruction. Meredith's history as a jazz drummer and improviser\n  \ncombined with his embrace of the infinite opportunities offered by technology infuse his compositions with a freedom of rhythm and structure that is energising.","image":"","keywords":["music"],"pdf_url":"","recs":[],"session":["C3_Wednesday"],"sessionpos":["5"],"soundcloud":"","title":"\u2018Critical Feedback - The evolution of the drums, feedback and the computer\u2019","vimeo":"","website":"https://www.nicholasmeredith.com/","youtube":""},"forum":"67","id":"67"},{"content":{"TLDR":"Piece No. 3 explores communication and miscommunication. The texture is formed by a web of improvisatory communicative links between performers. Soft-synths are directed to respond to each other and to physical instruments, which in turn interact with each other. These links are built flawed however, as each part\u2019s sonic content is only vaguely related to all the others\u2019, the synthesiser controls are alien, and the nature of the interactions is combative. Recordings of a human voice underscore this, attempting to express frustration, disillusionment, and panic, but being able to only in fragments. \n\nHopefully, the specific rules of interaction in the score are not central to fruitfully listening to the piece, and the (mis)communications between parts are audible to some degree. Further, while these connections create form and texture, and hold much meaning for me, the surface, the impression of the work is intended to reflect the messaging in the underpinning structures. \n\nPerformed by members of the ANU Laptop Ensemble and Canberra Experimental Music Studio: Abigail Thomas, Weitong Huang, Lynden Bassett (laptops), Jaime Langer (keyboard, laptop), and Miles McLaughlin (banjo, laptop) at the Lonsdale St Studio.\n","abstract":"Piece No. 3 explores communication and miscommunication. The texture is formed by a web of improvisatory communicative links between performers. Soft-synths are directed to respond to each other and to physical instruments, which in turn interact with each other. These links are built flawed however, as each part\u2019s sonic content is only vaguely related to all the others\u2019, the synthesiser controls are alien, and the nature of the interactions is combative. Recordings of a human voice underscore this, attempting to express frustration, disillusionment, and panic, but being able to only in fragments. \n\nHopefully, the specific rules of interaction in the score are not central to fruitfully listening to the piece, and the (mis)communications between parts are audible to some degree. Further, while these connections create form and texture, and hold much meaning for me, the surface, the impression of the work is intended to reflect the messaging in the underpinning structures. \n\nPerformed by members of the ANU Laptop Ensemble and Canberra Experimental Music Studio: Abigail Thomas, Weitong Huang, Lynden Bassett (laptops), Jaime Langer (keyboard, laptop), and Miles McLaughlin (banjo, laptop) at the Lonsdale St Studio.\n","authors":["Lynden Bassett"],"bandcamp":"https://lyndenbassett.bandcamp.com/track/piece-no-3","bio":"Lynden Bassett is an Indonesian-Australian composition student, currently studying an undergraduate degree at the ANU School of Music. His musical background is in punk and hardcore, and he plays in Canberra band, HYMMNN.\nThe Canberra Experimental Music Studio (Canberra EMS) is a group of performers, composers, and improvisers from the Canberra region, loosely based at the ANU School of Music. \nThe ANU Laptop Ensemble is operates out of the ANU Research School of Computer Science and the ANU School of Music. Members explore different ways of using the laptop in group performance.","image":"https://www.dropbox.com/s/2pkutr6l38p1icb/Screenshot%20%28295%29.png?dl=0","keywords":["music"],"pdf_url":"","recs":[],"session":["C4_Thursday"],"sessionpos":["4"],"soundcloud":"","title":"Piece No. 3","vimeo":"","website":"","youtube":""},"forum":"78","id":"78"},{"content":{"TLDR":"Program Notes: Presented in two distinct parts, this work is dedicated to two of the most original composers of the twentieth century: Pierre Boulez and Frank Zappa. Although their life trajectories were quite different, one an orchestral conductor and the other a rock musician, their paths crossed through their compositional activities. Pierre Boulez founded the French research institute IRCAM in 1977 and conducted several tracks on the Zappa album The Perfect Stranger in 1984. It was performed by IRCAM\u2019s Ensemble InterContemporain, with the title track commissioned by Boulez. Zappa was also very active in his use of the Synclavier in his custom-built studio. The Synclavier was an early digital synthesizer, polyphonic digital sampling system, and music workstation manufactured by New England Digital Corporation of Norwich, Vermont. The original design and development of the Synclavier prototype occurred at Dartmouth College with the collaboration of Jon Appleton, Professor of Digital Electronics, Sydney A. Alonso, and Cameron Jones, a software programmer and student. Thus Boulez and Zappa were both highly active in the use of technology in their music.\n\nThe work is organized in two separate parts. The first part is dedicated to Pierre Boulez and is subtitled \u201cPierre\u201d. The second part is dedicated to Frank Zappa and is subtitled \u201cFrank\u201d. Both parts were created using the composer\u2019s variation generation Max patch called Vari-Gen (see separate presentation talk). The technique, in both cases, was to record a keyboard improvisation, edit it and dissect its constituents, and create variations on the segments using the variation generator software. Then each piece was assembled to create the final composition.\n\nParts 1 & 2 are quite different from each other in style. Part 1 (Pierre) is a more homogenous electronic style work, which is not really an emulation of a Boulez piece, but is a nod to his use of electronics. Part 2 (Frank) uses drums, bass, and a lead instrument in a form that begins with a solid drum back beat, then morphs into a duet between drums (percussion) and lead instruments, followed by a duet between lead and bass, before a final return to solid drum beat, bass line and lead synths. The bass instrument in Part 2 uses a software emulation of a Synclavier by the Arturia company. This is a nod to Zappa\u2019s use of the Synclavier later in his life. Part 2 is more in the phrenetic, relentless Zappa style. The production style is very retro!\n","abstract":"Program Notes: Presented in two distinct parts, this work is dedicated to two of the most original composers of the twentieth century: Pierre Boulez and Frank Zappa. Although their life trajectories were quite different, one an orchestral conductor and the other a rock musician, their paths crossed through their compositional activities. Pierre Boulez founded the French research institute IRCAM in 1977 and conducted several tracks on the Zappa album The Perfect Stranger in 1984. It was performed by IRCAM\u2019s Ensemble InterContemporain, with the title track commissioned by Boulez. Zappa was also very active in his use of the Synclavier in his custom-built studio. The Synclavier was an early digital synthesizer, polyphonic digital sampling system, and music workstation manufactured by New England Digital Corporation of Norwich, Vermont. The original design and development of the Synclavier prototype occurred at Dartmouth College with the collaboration of Jon Appleton, Professor of Digital Electronics, Sydney A. Alonso, and Cameron Jones, a software programmer and student. Thus Boulez and Zappa were both highly active in the use of technology in their music.\n\nThe work is organized in two separate parts. The first part is dedicated to Pierre Boulez and is subtitled \u201cPierre\u201d. The second part is dedicated to Frank Zappa and is subtitled \u201cFrank\u201d. Both parts were created using the composer\u2019s variation generation Max patch called Vari-Gen (see separate presentation talk). The technique, in both cases, was to record a keyboard improvisation, edit it and dissect its constituents, and create variations on the segments using the variation generator software. Then each piece was assembled to create the final composition.\n\nParts 1 & 2 are quite different from each other in style. Part 1 (Pierre) is a more homogenous electronic style work, which is not really an emulation of a Boulez piece, but is a nod to his use of electronics. Part 2 (Frank) uses drums, bass, and a lead instrument in a form that begins with a solid drum back beat, then morphs into a duet between drums (percussion) and lead instruments, followed by a duet between lead and bass, before a final return to solid drum beat, bass line and lead synths. The bass instrument in Part 2 uses a software emulation of a Synclavier by the Arturia company. This is a nod to Zappa\u2019s use of the Synclavier later in his life. Part 2 is more in the phrenetic, relentless Zappa style. The production style is very retro!\n","authors":["David Hirst"],"bandcamp":"","bio":"David Hirst\u2019s electroacoustic music compositions have been performed in the United States, Canada, the UK, the Netherlands, New Zealand, South Korea and nationally across Australia. He studied computer music at La Trobe University, composition with Jonty Harrison at the University of Birmingham, and completed a PhD in electroacoustic music composition and analysis at the University of Melbourne. Hirst has worked lectured at the Tasmanian Conservatorium of Music, La Trobe University, and at the University of Melbourne. He is currently Honorary Principal Fellow at the Melbourne Conservatorium of Music, University of Melbourne. His most recent album, \u201cThe Shape of Water\u201d is available on iTunes and Spotify.","image":"10","keywords":["music"],"pdf_url":"","recs":[],"session":["C2_Tuesday"],"sessionpos":["TBA"],"soundcloud":"https://soundcloud.com/david-hirst-1","title":"Pierre and Frank (Parts 1&2) \u2013 a Video Music work in two parts","vimeo":"","website":"https://davidhirst.me","youtube":""},"forum":"10","id":"10"},{"content":{"TLDR":"Qualia was composed at CEMI studios \u2013 Center for Experimental Music and Intermedia at the University of North Texas in 2017. The composition explores the experience of music from perception to sensation; the physical process during which our sensory organs \u2013 those involved with sound, tactility, and vision in particular \u2013 respond to musically organized sound stimuli. Through this deep connection, sound, space, and audience are all engaged in a multidimensional experience. The motion and the meaning inherited in the sounds are not disconnected from the sounds and are not the reason for the sounds but are, in fact, the sound altogether. Energy, movement, and timbre become one; sound source identification, cause guessing, sound energies, gesture decoding, and extra-musical connotations are not independent of the sound but vital internal components of it. Qualia are claimed to be individual instances of subjective, conscious experience. The way it feels to have mental states such as hearing frequencies at the lower threshold of human hearing or a piercing sound, hearing a Bb note from a ship horn, as well as the granularity of a recorded sound. It is an exploration of time and space, internal and universal. In Qualia, I do not experience musical memory as a sequence of instances but as a sensory wholeness that lasts the entire duration of the piece. The experience of sound itself is not sequential; it bypasses past or future; time becomes a single omnipresent unity. In this state of consciousness, time dissolves. The vibrating air molecules from the speakers, the reflections in the physical space, and the audience are the sound.","abstract":"Qualia was composed at CEMI studios \u2013 Center for Experimental Music and Intermedia at the University of North Texas in 2017. The composition explores the experience of music from perception to sensation; the physical process during which our sensory organs \u2013 those involved with sound, tactility, and vision in particular \u2013 respond to musically organized sound stimuli. Through this deep connection, sound, space, and audience are all engaged in a multidimensional experience. The motion and the meaning inherited in the sounds are not disconnected from the sounds and are not the reason for the sounds but are, in fact, the sound altogether. Energy, movement, and timbre become one; sound source identification, cause guessing, sound energies, gesture decoding, and extra-musical connotations are not independent of the sound but vital internal components of it. Qualia are claimed to be individual instances of subjective, conscious experience. The way it feels to have mental states such as hearing frequencies at the lower threshold of human hearing or a piercing sound, hearing a Bb note from a ship horn, as well as the granularity of a recorded sound. It is an exploration of time and space, internal and universal. In Qualia, I do not experience musical memory as a sequence of instances but as a sensory wholeness that lasts the entire duration of the piece. The experience of sound itself is not sequential; it bypasses past or future; time becomes a single omnipresent unity. In this state of consciousness, time dissolves. The vibrating air molecules from the speakers, the reflections in the physical space, and the audience are the sound.","authors":["Panayiotis Kokoras"],"bandcamp":"","bio":"Kokoras is an internationally award-winning composer and computer music innovator, and currently an Associate Professor of composition and CEMI director (Center for Experimental Music and Intermedia) at the University of North Texas. Born in Greece, he studied classical guitar and composition in Athens, Greece and York, England; he taught for many years at Aristotle University in Thessaloniki. Kokoras's sound compositions use sound as the only structural unit. His concept of \"holophonic musical texture\" describes his goal that each independent sound (phonos), contributes equally into the synthesis of the total (holos). In both instrumental and electroacoustic writing, his music calls upon a \"virtuosity of sound,\" a hyper-idiomatic writing which emphasizes on the precise production of variable sound possibilities and the correct distinction between one timbre and another to convey the musical ideas and structure of the piece. His compositional output is also informed by musical research in Music Information Retrieval compositional strategies, Extended techniques, Tactile sound, Hyperidiomaticity, Robotics, Sound and Consciousness.","image":"14","keywords":["music"],"pdf_url":"","recs":[],"session":["P4_Thursday"],"sessionpos":["1"],"soundcloud":"https://soundcloud.com/pkokoras/qualia-tape","title":"Qualia","vimeo":"","website":"http://www.panayiotiskokoras.com","youtube":""},"forum":"14","id":"14"},{"content":{"TLDR":"Robert and Stephen create work made possible by software.\n\nThere is an immediacy to the process. Roberts vocals are captured on the fly into sound clips that are then spat back out complete with digital processing.","abstract":"Robert and Stephen create work made possible by software.\n\nThere is an immediacy to the process. Roberts vocals are captured on the fly into sound clips that are then spat back out complete with digital processing.","authors":["Robert Croft & Stephen Oakes"],"bandcamp":"","bio":"Croake is a collaborative project between Stephen Oakes and Robert Croft. The pair have been producing their unique compositions for over 10 years. ","image":"https://drive.google.com/file/d/1V5tlTQHrU4wlJnUtw8wxSWt1KMwlwaqM/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["C3_Wednesday"],"sessionpos":["4"],"soundcloud":"https://soundcloud.com/stephen-oakes-175115657","title":"Croake","vimeo":"","website":"","youtube":""},"forum":"79","id":"79"},{"content":{"TLDR":"Scene. A nondescript hum smothers the echoes of a distant playground. The eeriness intensifies as the afternoon loses the daily battle against the evening soundscape. \"Where is this?\". The sonic familiarity puts the name of a place on the tip of the tongue right as the movement changes, or does it? The hum left unannounced, that's a fact; now there's something else filling the space, something deeper, more spiritual even. Repetition gives way to flow; quietness prevails, for a while at least. Feedback turns into flares as the night falls. \"Is this still the West?\". Continental lines may or may not have been crossed, the only way to find out is to wander further, facing novelty until everything becomes known again. A faint sadness lingers in the air. End of scene.\nComposed & performed by Subespai (Mauri Edo)\n\nVisuals by Josh Paton\n\nA/V support by Matthew Syres","abstract":"Scene. A nondescript hum smothers the echoes of a distant playground. The eeriness intensifies as the afternoon loses the daily battle against the evening soundscape. \"Where is this?\". The sonic familiarity puts the name of a place on the tip of the tongue right as the movement changes, or does it? The hum left unannounced, that's a fact; now there's something else filling the space, something deeper, more spiritual even. Repetition gives way to flow; quietness prevails, for a while at least. Feedback turns into flares as the night falls. \"Is this still the West?\". Continental lines may or may not have been crossed, the only way to find out is to wander further, facing novelty until everything becomes known again. A faint sadness lingers in the air. End of scene.\nComposed & performed by Subespai (Mauri Edo)\n\nVisuals by Josh Paton\n\nA/V support by Matthew Syres","authors":["Subespai"],"bandcamp":"","bio":"Subespai is Mauri Edo, a Sydney-based sound artist working with found sound, repetition and volume to explore and express complex thoughts and ideas on existence, society and the individual.","image":"https://u.pcloud.link/publink/show?code=XZ5O9UkZDitquhSmSf0zp1z7g8yO9j74qFvX","keywords":["music"],"pdf_url":"","recs":[],"session":["C1_Monday"],"sessionpos":["1"],"soundcloud":"","title":"\"City circles\": sound collage on urban canvas","vimeo":"","website":"http://subespai.net/","youtube":"https://youtu.be/XdphlWifu5Q"},"forum":"1","id":"1"},{"content":{"TLDR":"Scrapes and Sighs (2019) makes use of common kitchen items such as metal fruit bowls, oven  trays  and  porcelain  cups.  Struck  in  the  manner  of  a  singing  bowl,  these everyday vessels produce pure harmonics which are teased out into strings of sound. The work progresses from its initial meditative section into a much busier world of gaudy, processed sound. These shards pivot and ricochet around the sound space in broken envelopes before settling back into a calmer mood.\n\nScrapes and Sighs was first composed for Multiple Monophonies, an installation featuring a multichannel monophonic spatial system. As such, the work was initially composed for twelve discrete monophonic channels, but here exists as a stereo work optimised for headphones.","abstract":"Scrapes and Sighs (2019) makes use of common kitchen items such as metal fruit bowls, oven  trays  and  porcelain  cups.  Struck  in  the  manner  of  a  singing  bowl,  these everyday vessels produce pure harmonics which are teased out into strings of sound. The work progresses from its initial meditative section into a much busier world of gaudy, processed sound. These shards pivot and ricochet around the sound space in broken envelopes before settling back into a calmer mood.\n\nScrapes and Sighs was first composed for Multiple Monophonies, an installation featuring a multichannel monophonic spatial system. As such, the work was initially composed for twelve discrete monophonic channels, but here exists as a stereo work optimised for headphones.","authors":["Alexis Weaver"],"bandcamp":"https://alexismarieweaver.bandcamp.com/album/scrapes-and-sighs","bio":"Alexis Weaver is an electroacoustic composer based in Sydney, Australia. Alexis draws on field recordings of animals, insects and everyday objects to create whimsical, adventurous radiophonic and acousmatic works. While her principal interest lies in composing fixed-media acousmatic music, she has also composed and collaborated on soundtracks for  animation, short film, radio, theatre, and dance. Alexis\u2019 work has been broadcast in Australia, France and Scotland, as well as featured on New Weird Australia\u2019s Solitary Wave (In) (2019) and RMN Classical\u2019s Electroacoustic and Beyond II (2017). While studying a Bachelor of Composition at the Sydney Conservatorium of Music, Alexis was awarded People\u2019s Choice Award and First Place in the 2015 and 2016 University of Sydney Verge Awards respectively for her acousmatic works. In January 2018, Alexis was awarded the National Council of Women\u2019s Australia Day Prize for her research undertaken during her Honours year on the visibility and practice of female electroacoustic composers. She is currently a Master of Music candidate at the Sydney Conservatorium, where she also teaches composition. Her research has focused on the transferral of high-quality acousmatic music to everyday, portable diffusion systems \u2013 naming this new, inclusive audio movement Small Diffusion. Alexis is co-founder of composer collective lost+sound, who in 2018 launched a concert series celebrating emerging experimental artists.","image":"https://drive.google.com/file/d/12eBf146ml4cZ6heTW30xoIKNSW0F1VOd/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["P1_Monday"],"sessionpos":["2"],"soundcloud":"","title":"Scrapes and Sighs","vimeo":"","website":"http://www.alexismarieweaver.com","youtube":""},"forum":"32","id":"32"},{"content":{"TLDR":"Sound as excitation of space\n\na flash through the unlit quiet\n\nearshot illuminations\n\nas shimmering series of spark\n\nThe first of a series of works comprising multiple personal binaural room impulse responses (BRIRs) - samples of space in the human spatial register - this piece explores the modulation and dynamic of the spatial image via predominantly abstract materials.\n\nSpaceless, abstract synthesis and feedback is given architectural animation through convolution with the BRIRs, imparting the composer\u2019s peculiar, cumulative directional filtering: an echo of occupied space now intaglio, a nebulous anatomical topography, a spatial mould through which the sonics are extruded and tamed.","abstract":"Sound as excitation of space\n\na flash through the unlit quiet\n\nearshot illuminations\n\nas shimmering series of spark\n\nThe first of a series of works comprising multiple personal binaural room impulse responses (BRIRs) - samples of space in the human spatial register - this piece explores the modulation and dynamic of the spatial image via predominantly abstract materials.\n\nSpaceless, abstract synthesis and feedback is given architectural animation through convolution with the BRIRs, imparting the composer\u2019s peculiar, cumulative directional filtering: an echo of occupied space now intaglio, a nebulous anatomical topography, a spatial mould through which the sonics are extruded and tamed.","authors":["Matthew Barnard"],"bandcamp":"","bio":"\nMatt Barnard is a composer primarily interested in the spatial parameter of sound in both binaural and ambisonic domains. He previously studied under Joseph Anderson, and is now a lecturer and researcher at the University of Hull and member of the Hull ElectroAcoustic Resonance Orchestra (HEARO).","image":"5","keywords":["music"],"pdf_url":"","recs":[],"session":["P1_Monday"],"sessionpos":["1"],"soundcloud":"https://soundcloud.com/mattt/illuminations-i-calibration","title":"Illuminations I: Calibration","vimeo":"","website":"https://soundcloud.com/mattt","youtube":""},"forum":"5","id":"5"},{"content":{"TLDR":"Striking a balance between interacting with gear on stage and performing with strong physical energy can be a major hurdle for many electronic musicians. In this talk, Becki reflects on the developments of her own live show to suggest some strategies to overcome this obstacle, with a specific focus on four key questions that any artist can use to help develop their on-stage performance.\n\nThe emphasis of this talk is on the ability of every artist to find a means of physical expression live that will enable them to preserve both the intricacy and detail of their arrangements as well as an on-stage electricity and present engagement with the audience.\n\nBecki will also examine the ways that different stage settings and strategies in stage lighting (from no-budget DIY ideas to higher-budget #goals) can support different movement styles for electronic sets. This talk is intended for artists who are already making music and want to translate it into a live show, artists who haven\u2019t made electronic music yet but who are keen to delve into that side of composition and performance, and artists who are already performing computer-based music live but would like a new lens through which to approach their physicality and stagecraft.","abstract":"Striking a balance between interacting with gear on stage and performing with strong physical energy can be a major hurdle for many electronic musicians. In this talk, Becki reflects on the developments of her own live show to suggest some strategies to overcome this obstacle, with a specific focus on four key questions that any artist can use to help develop their on-stage performance.\n\nThe emphasis of this talk is on the ability of every artist to find a means of physical expression live that will enable them to preserve both the intricacy and detail of their arrangements as well as an on-stage electricity and present engagement with the audience.\n\nBecki will also examine the ways that different stage settings and strategies in stage lighting (from no-budget DIY ideas to higher-budget #goals) can support different movement styles for electronic sets. This talk is intended for artists who are already making music and want to translate it into a live show, artists who haven\u2019t made electronic music yet but who are keen to delve into that side of composition and performance, and artists who are already performing computer-based music live but would like a new lens through which to approach their physicality and stagecraft.","authors":["Becki Whitton"],"bandcamp":"","bio":"Becki Whitton is an engineer, songwriter and producer working out of Melbourne\u2019s Rolling Stock Recording Rooms. She works in a range of styles from pop and hip hop (G Flip, Allday) to ambient and experimental music (Brambles, Rainbow Chan). For the last four years she has managed sound for Girls Rock! Camp ACT. In her solo electronic-choral project Aphir, Becki has been hailed by Triple J's Tim Shiel as part of \u2018Australia\u2019s new wave of female electronic innovators\u2019 and has performed around Australia and internationally including performances at Music Tech Fest (Berlin), Denmark Arts (WA), and Falls Festival.\n\n","image":"https://www.dropbox.com/s/tgisehs5tv8wjkf/image%20by%20Isabella%20Connelley.jpg?dl=0","keywords":["paper"],"pdf_url":"","recs":[],"session":["synth-nota-inter"],"sessionpos":["5"],"soundcloud":"","title":"Physical Movement in Computer-Based Live Performance","vimeo":"","website":"http://beckiwhitton.com  ","youtube":""},"forum":"84","id":"84"},{"content":{"TLDR":"Studies have revealed compelling relationships between experiences of the natural environment and positive health outcomes in adult communities, primarily around lowering stress responses and increasing feelings of wellbeing. These psychosocial health benefits are frequently described via key theoretical frameworks, including the biophilia hypothesis, attention-restoration theory and stress-reduction theory. A number of studies have evaluated technological nature and human wellbeing; however, the wellbeing benefits of immersive, multisensory virtual reality (VR) and augmented reality (AR) nature experiences are still emerging in the research. Additionally, broadening evidence around composed or conceptualised nature and human wellbeing urges new possibilities for artistic, abstract and creative experiences.\nInner Forest is a virtual nature artwork in development, adapted from its original immersive VR proposal to augmented reality (AR) as a result of COVID-19 impacts and restrictions. This AR format is self-directed, enabling accessibility and inclusion for diverse user groups, ages and abilities and is designed to be experienced on demand. Each experience of the artwork is unique to audience interactions/contributions whilst engaged with the AR environment. Nature-evocative AR vision, audio and haptic elements will be demonstrated at the ACMC 2020. Conference attendees are invited to view the Inner Forest artwork online via the supplied link, and contributors Susannah Langley and Jessica Laraine Williams (The University of Melbourne) will be demonstrating the work and discussing its background rationale in their paper. The design of the artwork is aligned with Browning et al.\u2019s 2014 Biophilic Design guidelines, which support contemporary approaches to nature that include living organisms, non-living components, and designed elements. Inner Forest ultimately aims to offer audiences with a creative, playful experience of virtual nature art usable in both actual nature and in scenarios where actual nature is inaccessible. Scaling of the project will include delivery via a mobile phone app, haptic touch booklet and aromatherapy diffuser to elicit a multi-sensory, portable experience of a virtual nature ecosystem. Future scaling for the artwork involves staging immersive virtual nature experiences using 360 degree/CAVE equipment at the Virtual Reality Learning Studio (VRLS), The University of Melbourne.\n\nFor further information on Inner Forest artwork on Vimeo, please use https://vimeo.com/431305714 and Inner Forest AR mobile app prototype, on Vimeo, https://vimeo.com/424461493 .\n","abstract":"Studies have revealed compelling relationships between experiences of the natural environment and positive health outcomes in adult communities, primarily around lowering stress responses and increasing feelings of wellbeing. These psychosocial health benefits are frequently described via key theoretical frameworks, including the biophilia hypothesis, attention-restoration theory and stress-reduction theory. A number of studies have evaluated technological nature and human wellbeing; however, the wellbeing benefits of immersive, multisensory virtual reality (VR) and augmented reality (AR) nature experiences are still emerging in the research. Additionally, broadening evidence around composed or conceptualised nature and human wellbeing urges new possibilities for artistic, abstract and creative experiences.\nInner Forest is a virtual nature artwork in development, adapted from its original immersive VR proposal to augmented reality (AR) as a result of COVID-19 impacts and restrictions. This AR format is self-directed, enabling accessibility and inclusion for diverse user groups, ages and abilities and is designed to be experienced on demand. Each experience of the artwork is unique to audience interactions/contributions whilst engaged with the AR environment. Nature-evocative AR vision, audio and haptic elements will be demonstrated at the ACMC 2020. Conference attendees are invited to view the Inner Forest artwork online via the supplied link, and contributors Susannah Langley and Jessica Laraine Williams (The University of Melbourne) will be demonstrating the work and discussing its background rationale in their paper. The design of the artwork is aligned with Browning et al.\u2019s 2014 Biophilic Design guidelines, which support contemporary approaches to nature that include living organisms, non-living components, and designed elements. Inner Forest ultimately aims to offer audiences with a creative, playful experience of virtual nature art usable in both actual nature and in scenarios where actual nature is inaccessible. Scaling of the project will include delivery via a mobile phone app, haptic touch booklet and aromatherapy diffuser to elicit a multi-sensory, portable experience of a virtual nature ecosystem. Future scaling for the artwork involves staging immersive virtual nature experiences using 360 degree/CAVE equipment at the Virtual Reality Learning Studio (VRLS), The University of Melbourne.\n\nFor further information on Inner Forest artwork on Vimeo, please use https://vimeo.com/431305714 and Inner Forest AR mobile app prototype, on Vimeo, https://vimeo.com/424461493 .\n","authors":["Susannah Langley","Jessica Laraine Williams","Ann Borda"],"bandcamp":"","bio":"Susannah Langley is a visual artist whose practice is rooted in experimental drawing, installation and sound, often using unconventional media such as conductive material, found objects, field recording and virtual reality to explore ideas of history, memory, movement, feeling, and space.\nSince 2013, she has collaborated on works that people can move through, and touch, to summon stories and soundscapes, and primarily taken the form of virtual reality experiences and large scale installations. These works have been featured in exhibitions, festivals, prizes and residencies both nationally and internationally. Susannah was the winner of the 2017 Paramor Art + Innovation Prize, Casula Powerhouse. In addition to her arts practice, she also delivers creative tech-based workshops to a variety of age groups.\nSusannah is a current Master Researcher at the Faculty of Fine Arts and Music, University of Melbourne researching drawing and sound in a virtual environment. In addition, she is a current artist in residence with the Centre of Projection Art, Melbourne and recent resident at Testing Grounds Studios, Southbank Melbourne. Website: https://www.susannahlangley.com/ Email:susannah.langley@student.unimelb.edu.au\n\nJessica Laraine Williams is a transdisciplinary researcher, visual artist, writer and part time PhD candidate at the Faculty of Fine Arts and Music, The University of Melbourne. In 2020, she is undertaking an academic associate/sessional tutor role with the Faculty, teaching within Critical and Theoretical Studies. Jess has been working for a decade in her physiotherapist profession, including hospital, rehabilitation and management roles. She now specialises in aged care physiotherapy part time. Jess is undertaking her doctoral research into methodologies of posthumanism in art. This includes collaborations with researchers across and beyond her home faculty and institution. Her broader creative work relates to interests in performative identity, relational cartography (systems) and institutional critique. Jess holds first class Honours degrees in both the Bachelor of Physiotherapy (Monash University) and the Bachelor of Fine Art (The Faculty of Fine Arts and Music, The University of Melbourne). She has written for art publications both academic and popular, such as the Conversation and Art+Australia.Website: www.jlogos.net  Email: jess.williams@unimelb.edu.au\n\nAnn Borda is Associate Professor in the Centre for the Digital Transformation of Health at The University of Melbourne and Fellow of the Australasian Institute of Digital Health. Ann has a PhD in information science from University College London which has served as a springboard for her commitment towards transdisciplinary scholarship.  She has held senior positions in computing and data-intensive research initiatives in the UK and Australia. Ann has extensive experience in mentorship, open knowledge, and social innovations in both smart health and cultural heritage.   Such interests were fostered during her time at the Science Museum London in building digital collections to support communication in public science and medical discovery, and in managing an open source software portfolio across several UK higher education institutions.  Ann presently sits on the Research and Policy Committee of the Climate and Health Alliance.  Among many knowledge exchange events, she has co-organised public forums on automation, well-being and society with the Alan Turing Institute. Recently Ann received an EPIC grant under the EU Horizon 2020 programme (ICT) to investigate advancing approaches to health and biomedical citizen science methods, platforms and capabilities.\nWebsite:  http://www.findanexpert.unimelb.edu.au/display/person197899\n","image":"https://www.dropbox.com/s/fv04o6l1ask5g69/Inner%20Forest_Gertrude%20Street%20Project%20Festival_2019.jpg?dl=0","keywords":["paper"],"pdf_url":"","recs":[],"session":["spatial-perf"],"sessionpos":["6"],"soundcloud":"","title":"Virtual Nature: Inner Forest","vimeo":"https://vimeo.com/431414088","website":"https://www.susannahlangley.com/ ","youtube":""},"forum":"103","id":"103"},{"content":{"TLDR":"Taken from a story by the same title in \u804a\u658b\u5fd7\u5f02 Strange Tales from a Chinese Studio (1740) published in Qing dynasty China by \u84b2\u677e\u9f84 Pu Songling (1640 ~ 1715), \u9b3c\u54ed Wailing Ghosts explores different live processing aspects of voice and non-pitched instruments in addition to acoustic and theatric performance. The piece employs text-painting through live and synthetic timbres to portray a narrative heard in Mandarin and Sanskrit such that even those unfamiliar with these natural languages can comprehend. The text used is mostly from the above tale with an added Buddhist mantra of Ksitigarbha at the end.\n\nTo briefly summarize the tale: at the time of the Xie Qian troubles, the residences of the nobility were all commandeered by the rebels including the residence of Commissioner Wang. When the government eventually retook the town, every porch was strewn with corpses and blood flowed from every doorway. Since then, Wang frequently saw ghosts in the day and night, hearing the ghosts wailing in various corners of the house. He eventually ordered a lengthy ritual performed to depart the wandering souls. Ever since, the hauntings ceased.\n\nI took a new approach in this piece when dealing with the text. Rather than setting the text in a traditional way, instead I act as narrator to present the text directly. Since Chinese is a melodic language, through the dramatic reading, the nuance of the melodic contour of the text can be easily perceived. The live-processed electronic sounds further amplify the character of the language. The Chinese opera gong used in this piece achieves multiple layers of functionality: from the timbral perspective, striking with mallet and palm brings out different levels of complex sounds; when used with singing, the gong acts as a filter giving the sound a special effect; from the theatric perspective, the gong acts as a mask, separating the narrator and the characters of the story.","abstract":"Taken from a story by the same title in \u804a\u658b\u5fd7\u5f02 Strange Tales from a Chinese Studio (1740) published in Qing dynasty China by \u84b2\u677e\u9f84 Pu Songling (1640 ~ 1715), \u9b3c\u54ed Wailing Ghosts explores different live processing aspects of voice and non-pitched instruments in addition to acoustic and theatric performance. The piece employs text-painting through live and synthetic timbres to portray a narrative heard in Mandarin and Sanskrit such that even those unfamiliar with these natural languages can comprehend. The text used is mostly from the above tale with an added Buddhist mantra of Ksitigarbha at the end.\n\nTo briefly summarize the tale: at the time of the Xie Qian troubles, the residences of the nobility were all commandeered by the rebels including the residence of Commissioner Wang. When the government eventually retook the town, every porch was strewn with corpses and blood flowed from every doorway. Since then, Wang frequently saw ghosts in the day and night, hearing the ghosts wailing in various corners of the house. He eventually ordered a lengthy ritual performed to depart the wandering souls. Ever since, the hauntings ceased.\n\nI took a new approach in this piece when dealing with the text. Rather than setting the text in a traditional way, instead I act as narrator to present the text directly. Since Chinese is a melodic language, through the dramatic reading, the nuance of the melodic contour of the text can be easily perceived. The live-processed electronic sounds further amplify the character of the language. The Chinese opera gong used in this piece achieves multiple layers of functionality: from the timbral perspective, striking with mallet and palm brings out different levels of complex sounds; when used with singing, the gong acts as a filter giving the sound a special effect; from the theatric perspective, the gong acts as a mask, separating the narrator and the characters of the story.","authors":["Li Tao "],"bandcamp":"","bio":"Li Tao is a composer and pianist from China. While Chinese traditional culture profoundly influences her, years of living in the U.S., culminating in the receipt of her Ph.D. in Music Composition from the University of Oregon in 2020, have formed her distinct multicultural musical language. Her primary interests include acoustic and electroacoustic composition, performance, and theoretical analysis of compositional techniques and aesthetics. She maintains a deep interest in the inner connections between composer, performer, and instrument. As an interdisciplinary performer, Tao is actively performing both classical and contemporary, acoustic and electroacoustic music in concerts and music festivals. Tao\u2019s music has been performed by numerous musicians and ensembles across Asia, Europe, North America, and Australia. ","image":"https://www.dropbox.com/s/5qimla2gky1t6vs/LI%20WG%20image.png?dl=0","keywords":["music"],"pdf_url":"","recs":[],"session":["C1_Monday"],"sessionpos":["2"],"soundcloud":"","title":"\u9b3c\u54ed Wailing Ghosts","vimeo":"","website":"https://taolimusic.com","youtube":"https://youtu.be/5hLfEN0BA9o"},"forum":"48","id":"48"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["Minchang Han"],"bandcamp":"","bio":"","image":"","keywords":["music"],"pdf_url":"","recs":[],"session":["P3_Wednesday"],"sessionpos":["3"],"soundcloud":"","title":"Unawakened Routine of a Salaryman","vimeo":"","website":"","youtube":""},"forum":"35","id":"35"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["Lindsay Vickery and Stuart James"],"bandcamp":"","bio":"","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["spatial-perf"],"sessionpos":["1"],"soundcloud":"","title":"The Decibel Scoreplayer as a portable medium for spatial music performance.","vimeo":"","website":"","youtube":""},"forum":"40","id":"40"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["Elise Reitze-Swensen"],"bandcamp":"","bio":" ","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["cult-comm-eco"],"sessionpos":["4"],"soundcloud":"","title":"Women of Music Production Perth","vimeo":"","website":"","youtube":""},"forum":"42","id":"42"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["Lindsay Vickery"],"bandcamp":"","bio":"","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["musico-comp"],"sessionpos":["2"],"soundcloud":"","title":"Musica ex machina: integrating the sonic pallet of machines with acoustic instruments.","vimeo":"","website":"","youtube":""},"forum":"43","id":"43"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["David Haberfeld"],"bandcamp":"","bio":"","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["musico-comp"],"sessionpos":["5"],"soundcloud":"","title":"Composing Real-Time Electronic Dance Music: an improvisation approach to composing Acid Techno","vimeo":"","website":"","youtube":""},"forum":"52","id":"52"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["Elise Reitze-Swensen"],"bandcamp":"","bio":"","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["daws-live-algo"],"sessionpos":["6"],"soundcloud":"","title":"Accessibility of Music Theory in Ableton Live","vimeo":"","website":"","youtube":""},"forum":"58","id":"58"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["Eugenia Stuart","Nicholas Trivett","Laurence Hughes"],"bandcamp":"","bio":"","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["cult-comm-eco"],"sessionpos":["1"],"soundcloud":"","title":"community development through online radio","vimeo":"","website":"","youtube":""},"forum":"63","id":"63"},{"content":{"TLDR":"TBA","abstract":"TBA","authors":["Andrea Gelido"],"bandcamp":"","bio":"","image":"","keywords":["music"],"pdf_url":"","recs":[],"session":["C1_Monday"],"sessionpos":["5"],"soundcloud":"","title":"Instrument of Failure - Gelido","vimeo":"","website":"","youtube":""},"forum":"85","id":"85"},{"content":{"TLDR":"The Audiovisual  \"Atlas of Uncertainty\" is based on the representation of 4 Classical elements, that typically refer to the concepts in Ancient Greece of earth, water, fire, earth and aether, which were proposed to explain the nature and complexity of all matter in terms of simpler substances.\n\nThe music that accompany the video is a sonic continuum ranging from unaltered natural sounds to entirely new sounds - or, more poetically -- from the real world to the realm of the imagination.\n\nIn \u201cAtlas of Uncertainty\u201d a microcosm of sounds, explored through some and max msp interfaces, becomes the hyletic universe of the work.\n\nHeterogeneous sound materials are explored through various techniques (granular, subtractive). The sounds and the images are here combined in well identifiable gestures.\n","abstract":"The Audiovisual  \"Atlas of Uncertainty\" is based on the representation of 4 Classical elements, that typically refer to the concepts in Ancient Greece of earth, water, fire, earth and aether, which were proposed to explain the nature and complexity of all matter in terms of simpler substances.\n\nThe music that accompany the video is a sonic continuum ranging from unaltered natural sounds to entirely new sounds - or, more poetically -- from the real world to the realm of the imagination.\n\nIn \u201cAtlas of Uncertainty\u201d a microcosm of sounds, explored through some and max msp interfaces, becomes the hyletic universe of the work.\n\nHeterogeneous sound materials are explored through various techniques (granular, subtractive). The sounds and the images are here combined in well identifiable gestures.\n","authors":["Massimo Vito Avantaggiato"],"bandcamp":"","bio":"His work revolves around research processes and combination of experimental video and experimental electronic music. He took a master degree in Electroacoustic Composition, Composition, Sound Engineering. He has won several prizes for his works in international composition competitions with concerts and academic presentations in over 90 countries.","image":"28","keywords":["music"],"pdf_url":"","recs":[],"session":["C1_Monday"],"sessionpos":["3"],"soundcloud":"","title":"Atlas of Uncertainty","vimeo":"https://vimeo.com/264567646","website":"https://vimeo.com/user22709645","youtube":""},"forum":"28","id":"28"},{"content":{"TLDR":"The Disquiet Of Melting is an audio-visual piece combining field recording with crowd-sourced vocal contributions from the Disquiet Junto, an online community of musicians. The piece is a sonic representation of the anxiety and unease invoked by climate change, particularly in light of the extreme weather events in Australia. The Disquiet Junto is a group created by author and sound artist Marc Weidenbaum as a weekly project where musicians respond to fast-turnaround assignments to compose and share compositions. This particular project (#419 - Dischoir) asked participants to create music from 113 vocal samples of held syllables shared by members of the Disquiet Junto. The field recording was made by freezing a hydrophone into a glass of water, then slowly pouring hot water over the frozen block. The vocal contributions were then layered on top of the field recording in Ableton Live 9. The resulting composition was turned into an audio visualisation using Trapcode Form in Adobe After Effects. Vocal contributions [in order of appearance]: Jet Jaguar Patricia Wolf Cray Samarobryn Atomboyd Precht Zoundsabari Ejkelly BellyFullOfStars Sevenism tja Zero Meaning KRSeward Vonna Wolf","abstract":"The Disquiet Of Melting is an audio-visual piece combining field recording with crowd-sourced vocal contributions from the Disquiet Junto, an online community of musicians. The piece is a sonic representation of the anxiety and unease invoked by climate change, particularly in light of the extreme weather events in Australia. The Disquiet Junto is a group created by author and sound artist Marc Weidenbaum as a weekly project where musicians respond to fast-turnaround assignments to compose and share compositions. This particular project (#419 - Dischoir) asked participants to create music from 113 vocal samples of held syllables shared by members of the Disquiet Junto. The field recording was made by freezing a hydrophone into a glass of water, then slowly pouring hot water over the frozen block. The vocal contributions were then layered on top of the field recording in Ableton Live 9. The resulting composition was turned into an audio visualisation using Trapcode Form in Adobe After Effects. Vocal contributions [in order of appearance]: Jet Jaguar Patricia Wolf Cray Samarobryn Atomboyd Precht Zoundsabari Ejkelly BellyFullOfStars Sevenism tja Zero Meaning KRSeward Vonna Wolf","authors":["C. Tsang"],"bandcamp":"","bio":"Born in 1982 in Hong Kong, C. Tsang (pronouns: they/them) is a nonbinary audio-visual artist living in Perth, Australia. Their work explores the emotional nature of landscape, and the main focus of their practice has been on their response to the natural landscape as a composer and performer, incorporating audio and visual elements of place into compositions, and using the landscape as a narrative device. C. has performed and exhibited their works in Australia, Asia, UK, Ireland and the USA as samarobryn, and has been nominated multiple times in the WAM Song Of The Year Awards in the experimental category. They were also nominated in the 2019 WAM Awards for Best Experimental Artist. They are currently a PhD candidate at the Western Australian Academy Of Performing Arts (Edith Cowan University).","image":"16","keywords":[""],"pdf_url":"","recs":[],"session":["C2_Tuesday"],"sessionpos":["1"],"soundcloud":"","title":"The Disquiet Of Melting","vimeo":"https://vimeo.com/390380677","website":"https://www.fb.me/samarobrynAU","youtube":""},"forum":"16","id":"16"},{"content":{"TLDR":"The Enabled System is designed to provide a way for small groups of untrained people, or people with disabilities, to engage in group musical improvisations through a computer-mediated performance environment. The system consists of a variety of physical controllers that send performance information to some computer software which transforms the inputs to produce control information for a collection of synthesisers. ","abstract":"The Enabled System is designed to provide a way for small groups of untrained people, or people with disabilities, to engage in group musical improvisations through a computer-mediated performance environment. The system consists of a variety of physical controllers that send performance information to some computer software which transforms the inputs to produce control information for a collection of synthesisers. ","authors":["Michael Spicer"],"bandcamp":"","bio":"Michael Spicer has a PhD Music  and a M.Sc in ComputerScience, and is constantly looking for ways to combine these two areas. He has been performing professionally as a keyboard/synthesizer/flute player since the late 1970\u2019s. He was a member of the popular Australian folk/rock group \u201cRedgum\u201d in the 1980\u2019s. He is currently teaching at Singapore Polytechnic and performing in Singapore with the improvisation group \u201cSonic Escapade\u201d.","image":"https://drive.google.com/file/d/1JzbR9M11phzct0IXSRamwxNAb2i8at79/view?usp=sharing","keywords":["paper"],"pdf_url":"","recs":[],"session":["synth-nota-inter"],"sessionpos":["2"],"soundcloud":"","title":"Making the Enabled System","vimeo":"","website":"https://sites.google.com/view/michaelspicerweblinks/home","youtube":"https://youtu.be/Fd0yvJpb7QY"},"forum":"30","id":"30"},{"content":{"TLDR":"The expressiveness, complexity and detail of the sounds of the natural world hold particular appeal for composers working at the nexus of music, technology and the environment. The innate spectral and spatial characteristics of these sounds offer a wealth of creative potential, but such qualities often derive their meaning from the contextual setting in which they originate. The tradition of soundscape composition promotes the tight integration between this contextual information and musical structure and denounces abstraction.  Its interpretation, therefore, requires explicable knowledge of environmental associations, and its meaning is inseparable from this context. Respecting and balancing these concerns with the traditions and techniques of acousmatic music poses a considerable challenge. This paper outlines a personal approach to composing with environmental sound that integrates reductive and contextual aesthetics into single compositional language. It discusses the strategies that led to the creation of a new software artefact and the musical value of the sounds produced through this technology.\n\nThe Distance Mixer is a custom SuperCollider class designed to assist in the composition of fixed-media works that seek to integrate environmental structures into electroacoustic composition. Its ongoing development reflects an attempt at maintaining the integrity of natural sound environments via acoustic partitioning while still permitting a degree of abstraction and transformation of sound materials. The Distance Mixer allows natural sound ecologies to inform the deployment of materials and encourages the practitioner to explore in real-time, the relationship between perspective, movement and distance and the influence of these spatial attributes on spectral space and the designation of acoustic niches.\n\nThe Distance Mixer applies the powerful spatial filtering tools of the Ambisonic Toolkit in novel ways to spatialise a variable number of sound sources and produces convincing spatial illusions and cogent aural landscapes. At its core is a distance variable that controls low-pass filter coefficients, amplitude scaling, soundfield quality, and the level of a simple convolution reverb for first-order ambisonic signals.\n\nThis paper will describe how the Distance Mixer emerged from an aesthetic discourse that consolidates the concepts of biomimicry, acoustic ecology and spectromorphology to explain the function of sound in the environment and its parallels with musical structure. The relationship between acoustic niches, perspectival space and spectromorphological qualities were of particular importance to its development. Through the immateriality of this theoretical framework, new approaches to composition emerged that had a productive and transformative impact on the creative process.\n\n","abstract":"The expressiveness, complexity and detail of the sounds of the natural world hold particular appeal for composers working at the nexus of music, technology and the environment. The innate spectral and spatial characteristics of these sounds offer a wealth of creative potential, but such qualities often derive their meaning from the contextual setting in which they originate. The tradition of soundscape composition promotes the tight integration between this contextual information and musical structure and denounces abstraction.  Its interpretation, therefore, requires explicable knowledge of environmental associations, and its meaning is inseparable from this context. Respecting and balancing these concerns with the traditions and techniques of acousmatic music poses a considerable challenge. This paper outlines a personal approach to composing with environmental sound that integrates reductive and contextual aesthetics into single compositional language. It discusses the strategies that led to the creation of a new software artefact and the musical value of the sounds produced through this technology.\n\nThe Distance Mixer is a custom SuperCollider class designed to assist in the composition of fixed-media works that seek to integrate environmental structures into electroacoustic composition. Its ongoing development reflects an attempt at maintaining the integrity of natural sound environments via acoustic partitioning while still permitting a degree of abstraction and transformation of sound materials. The Distance Mixer allows natural sound ecologies to inform the deployment of materials and encourages the practitioner to explore in real-time, the relationship between perspective, movement and distance and the influence of these spatial attributes on spectral space and the designation of acoustic niches.\n\nThe Distance Mixer applies the powerful spatial filtering tools of the Ambisonic Toolkit in novel ways to spatialise a variable number of sound sources and produces convincing spatial illusions and cogent aural landscapes. At its core is a distance variable that controls low-pass filter coefficients, amplitude scaling, soundfield quality, and the level of a simple convolution reverb for first-order ambisonic signals.\n\nThis paper will describe how the Distance Mixer emerged from an aesthetic discourse that consolidates the concepts of biomimicry, acoustic ecology and spectromorphology to explain the function of sound in the environment and its parallels with musical structure. The relationship between acoustic niches, perspectival space and spectromorphological qualities were of particular importance to its development. Through the immateriality of this theoretical framework, new approaches to composition emerged that had a productive and transformative impact on the creative process.\n\n","authors":["Nic McConaghy"],"bandcamp":"","bio":"Nic McConaghy is an audio technician and composer of acousmatic music. He is in the final stages of a PhD in composition at the Conservatorium of Music, University of Sydney, where he actively engages in researching numerous elements of music technology, composition, recording, electroacoustics and ecoacoustic composition. In a research capacity, his oeuvre encompasses an array of innovations based on field recordings and environmental sound composition across a broad range of activities involving the use of extant and emergent technologies.","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["spatial-perf"],"sessionpos":["4"],"soundcloud":"","title":"The Creation of the Distance Mixer","vimeo":"","website":"","youtube":""},"forum":"74","id":"74"},{"content":{"TLDR":"The global coronavirus (COVID-19) pandemic has brought into sharp relief not only the need for effective and inclusive online collaboration platforms but those that provide naturalistic and contextual extensions to the day-to-day \u201coffline\u201d milieu [1-3]. In this presentation, we explore \u201cWhat would an \u2018ideal\u2019 online collaboration platform provide for professional post-production in music?\u201d\n\nWe interviewed a group of music/sound practitioners who regularly work with complex multitrack digital audio projects and, in many cases, have limited or no facility for remote real-time collaboration directly through their music software. We collated the data produced from professionals working in Australian recording and post-production contexts through a series of user-focused interviews. This process ascertains their existing methods of collaboration/production and garners their perspectives of an \u201cideal\u201d environment for collaborative remote music post-production. In this talk, we present an analysis of their information and the subsequent outcomes. \nOverwhelmingly, the collaborative post-production practices described by the interviewees are asynchronous. That is, real-time collaboration does not occur even though it is highly desirable. Although some music production platforms now feature integrated collaboration methods, such as Avid\u2019s Cloud Collaboration [4] and Steinberg\u2019s VST Transit [5], audio and session/project file sharing via third-party cloud storage remains a popular means for studio engineers to disseminate work to their clients for feedback and approval. Using services such as Dropbox [6] and Google Drive [7], studio mixers/producers upload stems and a software-specific project file, or just an audio mix, for the client to download and audition. Any changes require further communication between the client and mixer/producer before any alterations can proceed. Upon completion, the uploading/downloading, auditioning, and feedback process begins again, until eventually reaching a mutually-satisfactory result, or, in some instances, the client depletes the budget. When integrating video into a project, and depending on a music production software\u2019s import/export capabilities, some practitioners prefer to share advanced authoring format (AAF) [8] or open media framework (OMF) [9] files instead, primarily to ensure video-audio synchronisation, but also to include basic automation, such as volume and panning changes. The interviewees also articulated a \u201cwish-list\u201d of features and capabilities given the opportunity to collaborate remotely with clients, producers, or other studios in real-time, highlighting the expected outcomes of such an environment.\n\nA conclusion to be drawn from the interviews is that, outside of remote one-to-one recording of an instrumentalist or vocalist, studio engineers do not engage in real-time remote collaboration simply because synchronous post-production collaboration methods do not currently exist. That is not to say that studio professionals have not contemplated what would constitute a practical remote post-production real-time collaborative environment. All the interviewees were forthcoming when asked to provide a wish-list of capabilities and features they would consider essential when working with a synchronous platform. Some common themes emerged from the various responses, particularly:\n\n- The ability to see and speak with the client during the session;\n- The ability to edit, mix and produce, and operate the music production software in-studio;\n- The ability to audition remote changes on-the-spot; and\n- The integrity and high fidelity of the project\u2019s audio files are maintained and protected.\n\nOne interviewee stated his expectation quite succinctly, saying he wanted the experience \u201cto be like I was sitting there in the studio\u201d with his project collaborators in a \u201cvirtual studio\u201d. One could well adopt this sentiment as an overarching vision for developing any remote collaboration platform, particularly one that operates in a real-time environment.\n\nOur research work has the potential to make this vision a reality. In 2018, we proposed a framework that had the potential to realise real-time collaboration on music production projects over the Internet [10]. Further refinement to the framework, mainly focusing on online group establishment and the creation of bilateral control data channels, led to our prototype implementation paper, presented at the 2019 Web Audio Conference [11]. Presently, we are working on a fully-functional online collaboration platform for professional post-production in music. In future, we expect to evaluate this platform in real-world \u201cvirtual studio\u201d environments with industry professionals/sound engineers and clients. \n\nSupporting Material\n\n1. Organisation for Economic Co-operation and Development. 2020. Education responses to COVID-19: Embracing digital learning and online collaboration.  (23 March 2020) Retrieved 5 May 2020 from https://www.oecd.org/coronavirus/policy-responses/education-responses-to-covid-19-embracing-digital-learning-and-online-collaboration-/\n2. Burris, P. 2020. COVID-19 Era Will Tell Us Much About Future of Collaboration Tools.  eWeek. Retrieved 5 May 2020 from https://www.eweek.com/enterprise-apps/covid-19-era-will-tell-us-much-about-future-of-collaboration-tools\n3. Wong, K. (2020, June). COVID-19 Pushes PLM/PDM to the Cloud: From bill of materials and file sharing to collaboration, many functions move to the cloud during lockdown. Digital Engineering 247, 26, 5, 23-25. Retrieved 15 June 2020, from http://search.ebscohost.com/login.aspx?direct=true&db=aps&AN=143695425&site=eds-live.\n4. Avid Technology Inc. 2020. Producing Software for Music - Cloud Collaboration - Pro Tools.  Retrieved 7 March 2020 from https://www.avid.com/pro-tools/cloud-collaboration\n5. Steinberg Media Technologies GmbH. 2020. VST Transit | Steinberg.  Retrieved 7 March 2020 from https://www.steinberg.net/en/products/vst/vst_transit.html?et_cid=15&et_lid=22&et_sub=VST%20Transit\n6. Dropbox Inc. n.d. Dropbox Professional.  Retrieved 7 March 2020 from https://www.dropbox.com/pro\n7. Google LLC. 2020. Google Drive: Free Cloud Storage for Personal Use.  Retrieved 7 March 2020 from https://www.google.com/drive/\n8. McLeish, D. and Tudor, P. 2004. The Advanced Authoring Format and its Relevance to the Exchange of Audio Editing Decisions. In Proceedings of the 25th International Conference of the Audio Engineering Society (London, United Kingdom, 17-19 June 2004). Audio Engineering Society Inc., New York, United States. Retrieved 9 January 2019 from http://www.aes.org/tmpFiles/elib/20190108/12824.pdf\n9. Lamaa, F. 1993. Open Media Framework Interchange. In Proceedings of the 8th Audio Engineering Society Conference UK (London, United Kingdom, 18-19 May 1993). Audio Engineering Society Inc., London, United Kingdom. Retrieved 9 January 2019 from http://www.aes.org/tmpFiles/elib/20190108/6134.pdf\n10. Stickland, S., Scott, N. and Athauda, R. 2018. A Framework for Real-Time Online Collaboration in Music Production. In Proceedings of the ACMC2018: Conference of the Australasian Computer Music Association (Perth, Australia, 6-9). Retrieved 20 March 2019 from https://computermusic.org.au/conferences/acmc-2018/\n11. Stickland, S., Athauda, R. and Scott, N. 2019. Design of a real-time multiparty DAW Collaboration Application using Web MIDI and WebRTC APIs. In Proceedings of the Web Audio Conference (WAC 2019) Diversity in Web Audio (NTNU, Trondheim, Norway, 4-6). Trondheim, Norway. Retrieved 1 May 2020 from https://www.ntnu.edu/documents/1282113268/1292502725/WAC_2019_proceedings.pdf\n\n","abstract":"The global coronavirus (COVID-19) pandemic has brought into sharp relief not only the need for effective and inclusive online collaboration platforms but those that provide naturalistic and contextual extensions to the day-to-day \u201coffline\u201d milieu [1-3]. In this presentation, we explore \u201cWhat would an \u2018ideal\u2019 online collaboration platform provide for professional post-production in music?\u201d\n\nWe interviewed a group of music/sound practitioners who regularly work with complex multitrack digital audio projects and, in many cases, have limited or no facility for remote real-time collaboration directly through their music software. We collated the data produced from professionals working in Australian recording and post-production contexts through a series of user-focused interviews. This process ascertains their existing methods of collaboration/production and garners their perspectives of an \u201cideal\u201d environment for collaborative remote music post-production. In this talk, we present an analysis of their information and the subsequent outcomes. \nOverwhelmingly, the collaborative post-production practices described by the interviewees are asynchronous. That is, real-time collaboration does not occur even though it is highly desirable. Although some music production platforms now feature integrated collaboration methods, such as Avid\u2019s Cloud Collaboration [4] and Steinberg\u2019s VST Transit [5], audio and session/project file sharing via third-party cloud storage remains a popular means for studio engineers to disseminate work to their clients for feedback and approval. Using services such as Dropbox [6] and Google Drive [7], studio mixers/producers upload stems and a software-specific project file, or just an audio mix, for the client to download and audition. Any changes require further communication between the client and mixer/producer before any alterations can proceed. Upon completion, the uploading/downloading, auditioning, and feedback process begins again, until eventually reaching a mutually-satisfactory result, or, in some instances, the client depletes the budget. When integrating video into a project, and depending on a music production software\u2019s import/export capabilities, some practitioners prefer to share advanced authoring format (AAF) [8] or open media framework (OMF) [9] files instead, primarily to ensure video-audio synchronisation, but also to include basic automation, such as volume and panning changes. The interviewees also articulated a \u201cwish-list\u201d of features and capabilities given the opportunity to collaborate remotely with clients, producers, or other studios in real-time, highlighting the expected outcomes of such an environment.\n\nA conclusion to be drawn from the interviews is that, outside of remote one-to-one recording of an instrumentalist or vocalist, studio engineers do not engage in real-time remote collaboration simply because synchronous post-production collaboration methods do not currently exist. That is not to say that studio professionals have not contemplated what would constitute a practical remote post-production real-time collaborative environment. All the interviewees were forthcoming when asked to provide a wish-list of capabilities and features they would consider essential when working with a synchronous platform. Some common themes emerged from the various responses, particularly:\n\n- The ability to see and speak with the client during the session;\n- The ability to edit, mix and produce, and operate the music production software in-studio;\n- The ability to audition remote changes on-the-spot; and\n- The integrity and high fidelity of the project\u2019s audio files are maintained and protected.\n\nOne interviewee stated his expectation quite succinctly, saying he wanted the experience \u201cto be like I was sitting there in the studio\u201d with his project collaborators in a \u201cvirtual studio\u201d. One could well adopt this sentiment as an overarching vision for developing any remote collaboration platform, particularly one that operates in a real-time environment.\n\nOur research work has the potential to make this vision a reality. In 2018, we proposed a framework that had the potential to realise real-time collaboration on music production projects over the Internet [10]. Further refinement to the framework, mainly focusing on online group establishment and the creation of bilateral control data channels, led to our prototype implementation paper, presented at the 2019 Web Audio Conference [11]. Presently, we are working on a fully-functional online collaboration platform for professional post-production in music. In future, we expect to evaluate this platform in real-world \u201cvirtual studio\u201d environments with industry professionals/sound engineers and clients. \n\nSupporting Material\n\n1. Organisation for Economic Co-operation and Development. 2020. Education responses to COVID-19: Embracing digital learning and online collaboration.  (23 March 2020) Retrieved 5 May 2020 from https://www.oecd.org/coronavirus/policy-responses/education-responses-to-covid-19-embracing-digital-learning-and-online-collaboration-/\n2. Burris, P. 2020. COVID-19 Era Will Tell Us Much About Future of Collaboration Tools.  eWeek. Retrieved 5 May 2020 from https://www.eweek.com/enterprise-apps/covid-19-era-will-tell-us-much-about-future-of-collaboration-tools\n3. Wong, K. (2020, June). COVID-19 Pushes PLM/PDM to the Cloud: From bill of materials and file sharing to collaboration, many functions move to the cloud during lockdown. Digital Engineering 247, 26, 5, 23-25. Retrieved 15 June 2020, from http://search.ebscohost.com/login.aspx?direct=true&db=aps&AN=143695425&site=eds-live.\n4. Avid Technology Inc. 2020. Producing Software for Music - Cloud Collaboration - Pro Tools.  Retrieved 7 March 2020 from https://www.avid.com/pro-tools/cloud-collaboration\n5. Steinberg Media Technologies GmbH. 2020. VST Transit | Steinberg.  Retrieved 7 March 2020 from https://www.steinberg.net/en/products/vst/vst_transit.html?et_cid=15&et_lid=22&et_sub=VST%20Transit\n6. Dropbox Inc. n.d. Dropbox Professional.  Retrieved 7 March 2020 from https://www.dropbox.com/pro\n7. Google LLC. 2020. Google Drive: Free Cloud Storage for Personal Use.  Retrieved 7 March 2020 from https://www.google.com/drive/\n8. McLeish, D. and Tudor, P. 2004. The Advanced Authoring Format and its Relevance to the Exchange of Audio Editing Decisions. In Proceedings of the 25th International Conference of the Audio Engineering Society (London, United Kingdom, 17-19 June 2004). Audio Engineering Society Inc., New York, United States. Retrieved 9 January 2019 from http://www.aes.org/tmpFiles/elib/20190108/12824.pdf\n9. Lamaa, F. 1993. Open Media Framework Interchange. In Proceedings of the 8th Audio Engineering Society Conference UK (London, United Kingdom, 18-19 May 1993). Audio Engineering Society Inc., London, United Kingdom. Retrieved 9 January 2019 from http://www.aes.org/tmpFiles/elib/20190108/6134.pdf\n10. Stickland, S., Scott, N. and Athauda, R. 2018. A Framework for Real-Time Online Collaboration in Music Production. In Proceedings of the ACMC2018: Conference of the Australasian Computer Music Association (Perth, Australia, 6-9). Retrieved 20 March 2019 from https://computermusic.org.au/conferences/acmc-2018/\n11. Stickland, S., Athauda, R. and Scott, N. 2019. Design of a real-time multiparty DAW Collaboration Application using Web MIDI and WebRTC APIs. In Proceedings of the Web Audio Conference (WAC 2019) Diversity in Web Audio (NTNU, Trondheim, Norway, 4-6). Trondheim, Norway. Retrieved 1 May 2020 from https://www.ntnu.edu/documents/1282113268/1292502725/WAC_2019_proceedings.pdf\n\n","authors":["Scott Stickland","Nathan Scott","Rukshan Athauda"],"bandcamp":"","bio":"Bios\nScott Stickland:\nScott Stickland is a third-year PhD (Music) candidate in the School of Creative Industries at The University of Newcastle (UoN), Australia. He has previously completed a Master of Music Technology (UoN) and a Bachelor of Education (Sec) \u2013 Music (Melbourne), and taught and coordinated music programs in Victorian secondary schools for 16 years. Scott has presented papers at ACMC2018 and WAC 2019 since commencing his PhD. He currently teaches audio and music production through his business, Monty Sound Production, and plays keyboards in the national touring band, Cool Change \u2013 The Ultimate Tribute.\nNathan Scott:\nNathan Scott is a lecturer in the School of Creative Industries at the University of Newcastle, Australia. He has interdisciplinary research interests spanning creative arts, technology, science, health and education. Nathan has presented and performed internationally, and has published in the areas of music, technology, education, gaming and the human voice. He has presented workshops in regional NSW (2003) and developed an online international postgraduate program supporting the use of technology in music contexts. He participated in the CHASS Expanding Horizons forum in Canberra (2006) and undertook a sub-project as part of an ALTC National Teaching Fellowship (2010).\nRukshan Athauda:\nDr Rukshan Athauda is a Senior Lecturer at the School of Electrical Engineering and Computing at The University of Newcastle (UoN), Australia. Dr Athauda\u2019s research interests span Database Systems, Technology-enhanced Learning, Cloud Computing and IT Security. Dr Athauda has published over 60 peer-reviewed research articles internationally. He has supervised 4 PhD completions at UoN and also undertaken a number of admin roles including Head of Discipline and Program Convenor. Prior to joining UoN, Dr Athauda has worked at Microsoft Corporation, USA, High-Performance Database Research Centre at Florida International University, USA and Sri Lanka Institute of Information Technology, Sri Lanka.\n","image":"https://uoneduau-my.sharepoint.com/:i:/g/personal/c3068031_uon_edu_au/ERIEZo-gNsZNqJyy7hiynjMBGOHJnm73zbgM6Peuib7tEg?e=X9nS4a","keywords":["paper"],"pdf_url":"","recs":[],"session":["daws-live-algo"],"sessionpos":["4"],"soundcloud":"","title":"Towards a vision for a virtual DAW collaboration studio for professional post-production music projects","vimeo":"","website":"https://www.montysp.com.au","youtube":""},"forum":"77","id":"77"},{"content":{"TLDR":"The live performance explores the iconic TB-303 Bassline Synthesizer through time and space with an all hardware live electronic dance music (EDM) performance. The performance will be a live improvisation with the TB-303 as the focused sound. Presenting real time sonic examples born from the unique TB-303 sound and demonstrate its significant contribution to the sonic development of EDM since Acid House.\n\nRoland\u2019s TB-303 Bassline synthesizer was manufactured as an electronic bass accompaniment machine. The TB-303 was conceived from the same idea as the drum machine. Although proving more difficult to program than a drum machine, the 303 did not resemble the bass guitar sound that guitarists were seeking at that time, ultimately leading to the initial in the early 1980s.\n\nFrom then DJs picked up the inexpensive and discontinued 303. The short repeating sequences of the 303 and its peculiar sound became appealing to the burgeoning producer. Further manipulating the limited real-time controls they were able to produce a squelching bubbling bass timbre that engaged dance floors and the sonic meaning of Acid House was born. Music technology companies have since continued to produce versions that emulate or clone the 303 in various forms. As much as Rock \u2019n Roll owes its existence to the electric guitar, EDM found its electric guitar in the form of the TB-303.\n\nThis particular performance extends the stylistic parameters of the genre of Acid through a Dub Music approach by slowing the tempo down and making use of spatial audio effects such as delays and reverbs to create further density . The performance makes use of 3x 303, consisting of an original TB-303, a modified TB-303 known as the Devilfish and a recent clone the TB-03. The iconic TR-808 drum machine provides the minimal drum pattern. This performance was recorded in a single take with no predetermined arrangement or form, all composed in real-time with no post production.","abstract":"The live performance explores the iconic TB-303 Bassline Synthesizer through time and space with an all hardware live electronic dance music (EDM) performance. The performance will be a live improvisation with the TB-303 as the focused sound. Presenting real time sonic examples born from the unique TB-303 sound and demonstrate its significant contribution to the sonic development of EDM since Acid House.\n\nRoland\u2019s TB-303 Bassline synthesizer was manufactured as an electronic bass accompaniment machine. The TB-303 was conceived from the same idea as the drum machine. Although proving more difficult to program than a drum machine, the 303 did not resemble the bass guitar sound that guitarists were seeking at that time, ultimately leading to the initial in the early 1980s.\n\nFrom then DJs picked up the inexpensive and discontinued 303. The short repeating sequences of the 303 and its peculiar sound became appealing to the burgeoning producer. Further manipulating the limited real-time controls they were able to produce a squelching bubbling bass timbre that engaged dance floors and the sonic meaning of Acid House was born. Music technology companies have since continued to produce versions that emulate or clone the 303 in various forms. As much as Rock \u2019n Roll owes its existence to the electric guitar, EDM found its electric guitar in the form of the TB-303.\n\nThis particular performance extends the stylistic parameters of the genre of Acid through a Dub Music approach by slowing the tempo down and making use of spatial audio effects such as delays and reverbs to create further density . The performance makes use of 3x 303, consisting of an original TB-303, a modified TB-303 known as the Devilfish and a recent clone the TB-03. The iconic TR-808 drum machine provides the minimal drum pattern. This performance was recorded in a single take with no predetermined arrangement or form, all composed in real-time with no post production.","authors":["David Haberfeld"],"bandcamp":"","bio":"David Haberfeld is an electronic dance music artist, producer, composer, performer, DJ, academic and educator, since the early 1990s. Best known for his productions and live performances under the artist moniker Honeysmack. In 1999, he was an Australian Record Industry Association (ARIA) finalist nominee for Best Dance Music Release for \"Walk on Acid\"\u2014which sampled Burt Bacharach's \"Walk on By\" earning David a co-writing credit with the Grammy and Academy awarded songwriter. His work as an energetic and colourful live electronic act has earned him a rare respect on the Australian live rock circuit, performing live electronica at festivals nationally and abroad.\n\nHis dynamic performances and productions are purely hardware based and centre on Roland\u2019s iconic machines of the 1980s including the TB-303, TR-909 and TR-808. These machines were pivotal in the development of electronic dance music, and David continues to push stylistic parameters and explore new contexts with these vintage machines through his research in composing real-time electronic dance music. His work strongly features modular synthesis and enjoys the creative possibilities each new configuration and performance can sonically provide. As an accomplished artist and academic he embodies a diverse mix of experiences and continues to explore and challenge new thinking around music, sound, media arts and creativity more broadly. David is in the final stages of his PhD in music composition at Monash University. ","image":"https://drive.google.com/file/d/1f820fVEy9q1FrBcIcWG4q2rMJH_UGAlA/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["C3_Wednesday"],"sessionpos":["2"],"soundcloud":"","title":"Acid Dub - Composing Real-Time Electronic Dance Music: How studio and performance based- practices combine to create Acid","vimeo":"","website":"https://www.davidhaberfeld.com/","youtube":"https://youtu.be/D6fVwILV8aw "},"forum":"39","id":"39"},{"content":{"TLDR":"The Shimmering Haze (2019) explores dichotomies of aural perspective, texture and sound source. The piece alludes to our human-induced plastic crisis through the exploration of small,  highly  active  sound  objects (which  I  name 'micro' sounds) embedded  within  vast, heavily  textured  sonic  backgrounds (which  I  have  termed 'macro' sounds). Comprised of a mix of synthesised sound and close recordings of sinks, kettles and coffee machines, The Shimmering Haze alludes to our man-made plastic crisis and conjures a sonic metaphor for shiny plastic glinting through hazy water. At times menacing, cloying, The Shimmering Haze also exhibits brief moments of linear simplicity \u2013 providing a welcome textural respite.\n\nThis work was composed as part of a larger portfolio for my Master of Music degree. With its highly detailed spatialisation and sound object placement, the piece is best listened to with quality headphones. While there also exists a stereo speaker-optimised mix, I have provided the headphone-optimised mix for the ACMC. ","abstract":"The Shimmering Haze (2019) explores dichotomies of aural perspective, texture and sound source. The piece alludes to our human-induced plastic crisis through the exploration of small,  highly  active  sound  objects (which  I  name 'micro' sounds) embedded  within  vast, heavily  textured  sonic  backgrounds (which  I  have  termed 'macro' sounds). Comprised of a mix of synthesised sound and close recordings of sinks, kettles and coffee machines, The Shimmering Haze alludes to our man-made plastic crisis and conjures a sonic metaphor for shiny plastic glinting through hazy water. At times menacing, cloying, The Shimmering Haze also exhibits brief moments of linear simplicity \u2013 providing a welcome textural respite.\n\nThis work was composed as part of a larger portfolio for my Master of Music degree. With its highly detailed spatialisation and sound object placement, the piece is best listened to with quality headphones. While there also exists a stereo speaker-optimised mix, I have provided the headphone-optimised mix for the ACMC. ","authors":["Alexis Weaver"],"bandcamp":"https://alexismarieweaver.bandcamp.com/album/the-shimmering-haze","bio":"Alexis Weaver is an electroacoustic composer based in Sydney, Australia. Alexis draws on field recordings of animals, insects and everyday objects to create whimsical, adventurous radiophonic and acousmatic works. While her principal interest lies in composing fixed-media acousmatic music, she has also composed and collaborated on soundtracks for  animation, short film, radio, theatre, and dance. Alexis\u2019 work has been broadcast in Australia, France and Scotland, as well as featured on New Weird Australia\u2019s Solitary Wave (In) (2019) and RMN Classical\u2019s Electroacoustic and Beyond II (2017). While studying a Bachelor of Composition at the Sydney Conservatorium of Music, Alexis was awarded People\u2019s Choice Award and First Place in the 2015 and 2016 University of Sydney Verge Awards respectively for her acousmatic works. In January 2018, Alexis was awarded the National Council of Women\u2019s Australia Day Prize for her research undertaken during her Honours year on the visibility and practice of female electroacoustic composers. She is currently a Master of Music candidate at the Sydney Conservatorium, where she also teaches composition. Her research has focused on the transferral of high-quality acousmatic music to everyday, portable diffusion systems \u2013 naming this new, inclusive audio movement Small Diffusion. Alexis is co-founder of composer collective lost+sound, who in 2018 launched a concert series celebrating emerging experimental artists.","image":"https://drive.google.com/file/d/12eBf146ml4cZ6heTW30xoIKNSW0F1VOd/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["P3_Wednesday"],"sessionpos":["1"],"soundcloud":"","title":"The Shimmering Haze","vimeo":"","website":"http://www.alexismarieweaver.com","youtube":""},"forum":"33","id":"33"},{"content":{"TLDR":"The Sky Is The Score #1 is the first in a series of pieces that use photographs of the sky as a graphic score. This stereo fixed media  piece is conceived to be played in a reverberant space, such as a church. It was initially mixed with no artificial reverb, but reverb has been added for playing online. The piece was created by recording several generative patches on a modular synth, and then shaping the resulting layers in a DAW so as to reflect my interpretations of various parts of the sky image. ","abstract":"The Sky Is The Score #1 is the first in a series of pieces that use photographs of the sky as a graphic score. This stereo fixed media  piece is conceived to be played in a reverberant space, such as a church. It was initially mixed with no artificial reverb, but reverb has been added for playing online. The piece was created by recording several generative patches on a modular synth, and then shaping the resulting layers in a DAW so as to reflect my interpretations of various parts of the sky image. ","authors":["Michael Spicer"],"bandcamp":"","bio":"Michael Spicer has a PhD Music and a M.Sc in Computer\nScience, and is constantly looking for ways to combine these two areas. He\nhas been performing professionally as a keyboard/synthesizer/flute player\nsince the late 1970\u2019s. He was a member of the popular Australian folk/rock\ngroup \u201cRedgum\u201d in the 1980\u2019s. He is currently teaching at Singapore Polytechnic and performing in Singapore with the improvisation group \u201cSonic Escapade\u201d.","image":"https://drive.google.com/file/d/1b3hq_lnk_aLVEXf6IcE_cGJeBVZvne0c/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["P2_Tuesday"],"sessionpos":["2"],"soundcloud":"","title":"The Sky Is The Score #1","vimeo":"","website":"https://sites.google.com/view/michaelspicerweblinks/home","youtube":""},"forum":"27","id":"27"},{"content":{"TLDR":"The totality of the acoustic material.\n\nExcavated, mutilated. Sublimated and deposited. The One that is fragmented and reduced to dust. Chalk blocks engraved and carved through the space and elasticity of time. Polvere nera is divided into four sections, bounded by sudden stops and static poses, in which there is an incessant dialogue between two opposing formal poles: bands and points. In the end the dialogue becomes union through a process of massification of the material that does not however cancel the intrinsic differences of the models employed. Polvere nera was constructed using noise, synthesis sounds and percussive sounds.\n","abstract":"The totality of the acoustic material.\n\nExcavated, mutilated. Sublimated and deposited. The One that is fragmented and reduced to dust. Chalk blocks engraved and carved through the space and elasticity of time. Polvere nera is divided into four sections, bounded by sudden stops and static poses, in which there is an incessant dialogue between two opposing formal poles: bands and points. In the end the dialogue becomes union through a process of massification of the material that does not however cancel the intrinsic differences of the models employed. Polvere nera was constructed using noise, synthesis sounds and percussive sounds.\n","authors":["Nicola Fumo Frattegiani"],"bandcamp":"","bio":"Born in Perugia, Nicola Fumo Frattegiani graduated with highest honours from D.A.M.S. (Academy of Arts, Music and Show) at the University of Bologna, with a thesis on Luigi Nono\u2019s work \u201cIntolleranza 1960\u201d.\nLater he has advanced post-graduate degree on \u201cThe musical cultures of 1900\u2019s\u201d at the University of Rome \u201cTor Vergata\u201d, a bachelor\u2019s degree cum laude on \u201cElectronic Music and New Technologies\u201d (course electroacoustic composition) at the \u201cFrancesco Morlacchi\u201d Conservatory of Music of Perugia and a Master\u2019s degree cum laude and special mention for artistic merit on \u201cElectronic Music and New Technologies\u201d at the \u201cLicinio Refice\u201d Conservatory of Music of Frosinone (course digital audiovisual composition).\nHis works have been presented at various national and international festivals including ICMC (South Korea), NYCEMF, ICMC-NYCEMF, New Music Miami Festival ISCM, Electroacoustic Barn Dance, WSU ElectroAcousticMiniFest (USA), SMC (Cyprus), Atempor\u00e1nea Festival, Foundation Destellos (Argentine), Festival Futura, Finale Prix Russolo (France), Synchresis Festival (Spain), Evimus (Germany), MUSLAB (Brazil), Echofluxx (Czech Republic), Audio Mostly, BFE/RMA Research Students' Conference, Convergence, Noisefloor Festival, SOUND/IMAGE Exploring Sonic and Audio-Visual Practice (United Kingdom), WOCMAT (Taiwan), Matera Intermedia Festival, Diffrazioni Firenze Multimedia Festival, XXII CIM Colloquium of Musical Informatics, Venice Biennale of Architecture, Soundscape of Work and of Play 9th International FKL symposium on soundscape, Moon in June, Macro Asilo Museum of contemporary art, Corsie Festival, Segnali Audio-visual arts and performance, Premio Nazionale delle Arti, Elettronicamente Beyond the Borders (Italy).\nAuthor and performer, his research deals with electroacoustic music, sound for images, video, art exhibition and in particular electroacoustic compositions for contemporary theatrical performance. Nicola collaborates with many artists and performers in several productions of live electroacoustic music, with whom he experimented many types of generation and manipulation of sound dimension. He also collaborates regularly with various recording studios and video production studios as a sound designer, sound engineer and re-recording mixer.\n","image":"53","keywords":["music"],"pdf_url":"","recs":[],"session":["P1_Monday"],"sessionpos":["4"],"soundcloud":"https://soundcloud.com/nicola-fumo-frattegiani/polvere-nera","title":"Polvere nera","vimeo":"","website":"https://www.nicolafumofrattegiani.com/","youtube":""},"forum":"53","id":"53"},{"content":{"TLDR":"These Would Be Other is a collaborative work spearheaded by Brigid Burke and Sophie Rose. Composed over a year, the piece uses video, live and pre-recorded audio, and electronics. Burke layers visuals and audio to further distort Chris Mann\u2019s text. The work was initially conceived as a tribute to Chris Mann\u2019s life and oeuvre and published in Open Space Magazine as Two for Chris.\n\nThese Would Be Other morphs the ideas of tribute and present-day considerations. The text was delivered by Sophie Rose at different points of exposure to the poem, from initial reading to experienced live delivery. This supplies an increasing complexity in delivery, pitch, and tonality in the pre-recorded. This complexity is further amplified by Burke\u2019s echolalic audio distortions. Burke superimposes multiple layers of sound and video to create a fractured image of the fractal and recursive nature of the text and hues.\n","abstract":"These Would Be Other is a collaborative work spearheaded by Brigid Burke and Sophie Rose. Composed over a year, the piece uses video, live and pre-recorded audio, and electronics. Burke layers visuals and audio to further distort Chris Mann\u2019s text. The work was initially conceived as a tribute to Chris Mann\u2019s life and oeuvre and published in Open Space Magazine as Two for Chris.\n\nThese Would Be Other morphs the ideas of tribute and present-day considerations. The text was delivered by Sophie Rose at different points of exposure to the poem, from initial reading to experienced live delivery. This supplies an increasing complexity in delivery, pitch, and tonality in the pre-recorded. This complexity is further amplified by Burke\u2019s echolalic audio distortions. Burke superimposes multiple layers of sound and video to create a fractured image of the fractal and recursive nature of the text and hues.\n","authors":["Brigid Burke","Sophie Rose"],"bandcamp":"","bio":"Dr. Brigid Burke\nBiography\n\nBrigid is an Australian composer, performance artist, clarinet soloist, visual artist, video artist and educator whose creative practice explores the use of acoustic sound and technology to enable media performances and installations that are rich in aural and visual nuances. Her work is widely presented in concerts, festivals, and radio broadcasts throughout Australia, Asia, Europe and the USA.\nRecently she has been a recipient of an Australia Council Project Music Fellowship & new work commissions \u2018Coral Bells\u2019 & \u201cInstincts and Episodes\u2019 also Artist in Resident at Marshall University USA with a Edwards Distinguished Professor Artist Residency, Indiana University USA and ADM NTU Singapore. Also most recently she has presented her works on the Big screen at Federation Square Melbourne, Lontano Festival Internaciol Musica Contemoranea em Goiania Brazil, SEAMUS USA, Tilde Festival Melb Australia, ABC Classic FM. ICMC International Computer Music Conference Perth Australia, Echofluxx 14 to19 Festivals Prague, Generative Arts Festivals in Ravena, Florence, Rome & Milan Italy, Asian Music Festivals in Tokyo, The Melbourne International Arts Festival, Futura Music Festival Paris France, Mona Foma Festival Hobart, The International Clarinet Festivals in Japan and Canada also Seoul and Australian International Computer Music Festivals. She has a PhD in Composition from UTAS and a Master of Music in Composition from The University of Melbourne.\nwww.brigid.com.au\n\n","image":"100","keywords":["music"],"pdf_url":"","recs":[],"session":["C4_Thursday"],"sessionpos":["3"],"soundcloud":"","title":"THESE WOULD BE OTHER","vimeo":"https://vimeo.com/397733269","website":"https://vimeo.com/397733269","youtube":""},"forum":"100","id":"100"},{"content":{"TLDR":"Thirteen years post its release in Japan in 1986 under Studio Ghibli, Hayao Miyazaki\u2019s Laputa: Castle in the Sky was re-released in America under the production of Disney. This paper discusses the differences between the scores of the film\u2019s two versions based on cue-by-cue qualitative analysis and examples from the films to illustrate the contrast. As there is a lack of unbiased investigation on the effects of different scores on the perception of the same film scenes, this case study aims to create a new resource providing an analytical comparison of these two compositions by Joe Hisaishi in order to interpret his compositional decisions. The four devices that played a role in reinventing the score in a Western compositional style to suit the audience\u2019s cultural background include orchestration changes, re-interpreting melodies, the use of Mickey-Mousing and the addition of new cues. Consequently, the paper highlights these factors\u2019 influence on the film\u2019s narrative and aims to widen the discourse on composition for animation film scores.","abstract":"Thirteen years post its release in Japan in 1986 under Studio Ghibli, Hayao Miyazaki\u2019s Laputa: Castle in the Sky was re-released in America under the production of Disney. This paper discusses the differences between the scores of the film\u2019s two versions based on cue-by-cue qualitative analysis and examples from the films to illustrate the contrast. As there is a lack of unbiased investigation on the effects of different scores on the perception of the same film scenes, this case study aims to create a new resource providing an analytical comparison of these two compositions by Joe Hisaishi in order to interpret his compositional decisions. The four devices that played a role in reinventing the score in a Western compositional style to suit the audience\u2019s cultural background include orchestration changes, re-interpreting melodies, the use of Mickey-Mousing and the addition of new cues. Consequently, the paper highlights these factors\u2019 influence on the film\u2019s narrative and aims to widen the discourse on composition for animation film scores.","authors":["Shally Sharin Pais"],"bandcamp":"","bio":"Shally Pais is an international student at AIM who completed her undergraduate degree in Visual Communication in India. She is currently pursuing her Master\u2019s Degree in Composition and Production, taking forward her Film Studies experience by developing her film scoring practice, specifically dealing with animation. She is working towards coalescing her knowledge and skills in creating audio and visual content through her intermedial projects.","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["musico-comp"],"sessionpos":["1"],"soundcloud":"","title":"Laputa: Castle in the Sky \u2013 A comparison of Joe Hisaishi\u2019s scores for the film\u2019s Japanese and English versions.","vimeo":"","website":"https://www.behance.net/shallyshar71c7","youtube":""},"forum":"15","id":"15"},{"content":{"TLDR":"This lecture/recital addresses key points and conclusions from a larger study around educational applications of Digital Audio Workstations (DAWs).\n\nExploration of liveness in computer-based music is far from new, and though there is now a strong culture of performance which integrates both DAWs and acoustic instruments, this study seeks to investigate the role DAWs can play in facilitating effective instrumental practise. This multi-perspective approach can provide a unique insight into the incorporation of compositional approaches into instrument practise, potentially resulting in a more enriching experience.\n\nThe lecture/performance discusses four distinct areas of drum set practise (polyrhythm, micro-subdivision, timekeeping and supporting song form), and whether each area can be uniquely developed through DAW associations. Parallels and relationships between practical performance tasks and session building in Ableton Live are discussed and demonstrated, and the application of unique DAW techniques (e.g. audio warping and analysis, rhythmic/melodic randomization, session view) are used to create specialized drumset exercises.\n\nSome learning strategies are explored, such as \u2018flexible meter\u2019 exercises; where a single melody and accompanying rhythm are programmed, allowing a drummer to accompany while making subtle changes in approach to micro-subdivision. The purpose of incorporating melody into this exercise is to give a second point of reference for the drummer, potentially providing a stronger link to real-world application.\n\nThe relationship between DAW-specific processes and drum practise are further explored, particularly concepts around symbiosis achieved through integration, potentially resulting in simultaneous proficiency development.\nThe lecture will conclude with a short solo drumset/Ableton Live performance which uses the points of discussion as a method of highlighting interplay between the two media.\n","abstract":"This lecture/recital addresses key points and conclusions from a larger study around educational applications of Digital Audio Workstations (DAWs).\n\nExploration of liveness in computer-based music is far from new, and though there is now a strong culture of performance which integrates both DAWs and acoustic instruments, this study seeks to investigate the role DAWs can play in facilitating effective instrumental practise. This multi-perspective approach can provide a unique insight into the incorporation of compositional approaches into instrument practise, potentially resulting in a more enriching experience.\n\nThe lecture/performance discusses four distinct areas of drum set practise (polyrhythm, micro-subdivision, timekeeping and supporting song form), and whether each area can be uniquely developed through DAW associations. Parallels and relationships between practical performance tasks and session building in Ableton Live are discussed and demonstrated, and the application of unique DAW techniques (e.g. audio warping and analysis, rhythmic/melodic randomization, session view) are used to create specialized drumset exercises.\n\nSome learning strategies are explored, such as \u2018flexible meter\u2019 exercises; where a single melody and accompanying rhythm are programmed, allowing a drummer to accompany while making subtle changes in approach to micro-subdivision. The purpose of incorporating melody into this exercise is to give a second point of reference for the drummer, potentially providing a stronger link to real-world application.\n\nThe relationship between DAW-specific processes and drum practise are further explored, particularly concepts around symbiosis achieved through integration, potentially resulting in simultaneous proficiency development.\nThe lecture will conclude with a short solo drumset/Ableton Live performance which uses the points of discussion as a method of highlighting interplay between the two media.\n","authors":["Tom Pierard"],"bandcamp":"","bio":"Tom Pierard is a performer, producer, commercial composer and educator currently living in Hawkes Bay, New Zealand.\n\nHe completed a degree in jazz performance (drumset) at Massey University Wellington in 2005, after which Tom secured a fulltime performance role with the Wellington-based ensemble Strike Percussion, which saw extensive national and international touring for the next five years with the group twice attending and performing at the Taiwanese International Percussion Convention.\n\nAfter the birth of his second child, Tom decided to step back from fulltime session performance and moved to Hawkes Bay in 2011 to take the role of Head of Music Studies at the Eastern Institute of Technology. This marked the beginning of his academic career, during which he led a successful faculty and developed NZQA-accredited programmes as well as numerous teaching resources, including \u2018The Modern Beat\u2019 \u2013 an online blended learning system which hosts a number of comprehensive video and written courses and practise tools for drumset players.\n\nAs a drumset player and percussionist Tom\u2019s playing can be heard on recordings such as Rhian Sheehan\u2019s \u2018Standing in Silence\u2019 (2009), Strike Percussion\u2019s \u2018Sketches\u2019 (2009), The Family Cactus\u2019 \u2018Spirit Lights\u2019 (2011) and many more.\n\nAs a solo artist Tom has produced four EPs and one full-length album to date (The Devil You Know, 2013) under the alias Kingfischer.\n\nTom completed a Master of Music in contemporary composition through the University of Auckland in 2016, his thesis and body of compositions focusing on atypical rhythmic stress and transfigured audio in contemporary popular music. He is presently completing his PhD in the field of  pedagogical applications of DAW use, and has recently produced research papers around new systems of graphic scoring, polyrhythmic function, music technology in education, and the influence of jazz drumming concepts in western popular music.\n","image":"https://drive.google.com/file/d/1qasOvGk_yzwG3YOWCV1D--3qyAADCdk8/view?usp=sharing","keywords":["paper"],"pdf_url":"","recs":[],"session":["daws-live-algo"],"sessionpos":["2"],"soundcloud":"","title":"Exploring integration of DAW Processes for Effective Instrumental Practise","vimeo":"https://vimeo.com/432341344 ","website":"https://www.facebook.com/kingfischerofficial/","youtube":""},"forum":"18","id":"18"},{"content":{"TLDR":"This paper compares acousmatic music and certain popular genres of electronic music such as electronic dance music (EDM) and the less dance floor-oriented styles of electronica. Specifically, this research explores how certain methods of structuring a composition in acousmatic composition can be adapted for the creative process of these popular electronic music genres.\n\nIt is less the artistic goal, and more the technical and theoretical skillsets of acousmatic composition that are being examined as valuable assets in the composition of popular styles of electronic music, namely the methods of accumulating and processing sonic material, planning and organising these temporally and creating a sense of compositional narrative. It explores this through a method of combining the top-down compositional strategy of more popular electronic music genres with the bottom-up strategy of acousmatic music work.\nThe techniques of organising a work through grouping sounds of similar timbral and/or morphological characteristics are also explored beyond the context of acousmatic composition, in my own work Dashboard Exam. Here, the macrostructure is derived from a popular music framework, yet the musical materials are treated in an acousmatic music convention, with all sounds (save for a short synthesizer chord sequence) created from a recording of buttons and switches on the inside of a car being manipulated. During the compositional process, the sounds in the recording were processed extensively before they were installed into the already-established popular music framework, resembling Curtis Roads\u2019 multiscale composition.\n\nDenis Smalley\u2019s analytical concept of Spectromorphology, provides an intuitive resource for articulating certain sonic processes in time, specifically those which occur in relation to a sound\u2019s frequency spectrum. This concept is used to analyse the sound materials, searching for relationships among the sounds and certain behaviours over time which can suggest their possible functions within a composition.\n\nWhile these concepts may be considered somewhat esoteric, the value in their ability to create new musical thought and compositions themselves, in all areas of electronic music, is too substantial to be restricted to academic zones of composition. This research invites a new group of composers (often existing outside of the field of academia) to explore elements of acousmatic music theory, within the more accessible context of EDM and electronica.","abstract":"This paper compares acousmatic music and certain popular genres of electronic music such as electronic dance music (EDM) and the less dance floor-oriented styles of electronica. Specifically, this research explores how certain methods of structuring a composition in acousmatic composition can be adapted for the creative process of these popular electronic music genres.\n\nIt is less the artistic goal, and more the technical and theoretical skillsets of acousmatic composition that are being examined as valuable assets in the composition of popular styles of electronic music, namely the methods of accumulating and processing sonic material, planning and organising these temporally and creating a sense of compositional narrative. It explores this through a method of combining the top-down compositional strategy of more popular electronic music genres with the bottom-up strategy of acousmatic music work.\nThe techniques of organising a work through grouping sounds of similar timbral and/or morphological characteristics are also explored beyond the context of acousmatic composition, in my own work Dashboard Exam. Here, the macrostructure is derived from a popular music framework, yet the musical materials are treated in an acousmatic music convention, with all sounds (save for a short synthesizer chord sequence) created from a recording of buttons and switches on the inside of a car being manipulated. During the compositional process, the sounds in the recording were processed extensively before they were installed into the already-established popular music framework, resembling Curtis Roads\u2019 multiscale composition.\n\nDenis Smalley\u2019s analytical concept of Spectromorphology, provides an intuitive resource for articulating certain sonic processes in time, specifically those which occur in relation to a sound\u2019s frequency spectrum. This concept is used to analyse the sound materials, searching for relationships among the sounds and certain behaviours over time which can suggest their possible functions within a composition.\n\nWhile these concepts may be considered somewhat esoteric, the value in their ability to create new musical thought and compositions themselves, in all areas of electronic music, is too substantial to be restricted to academic zones of composition. This research invites a new group of composers (often existing outside of the field of academia) to explore elements of acousmatic music theory, within the more accessible context of EDM and electronica.","authors":["Patrick Carroll"],"bandcamp":"","bio":"Patrick Carroll is a composer from Sydney, and a current PhD candidate at the Sydney Conservatorium of Music, studying the intersection of traditional acousmatic theory and popular genres of electronic music. While completing his Bachelor of Music (Composition) with Honours in 2015, Carroll began releasing and publishing dance music and electronica under the alias Piecey, amassing over a million plays on Spotify to date. In 2018, Carroll began releasing music under his own name (Pat Carroll), aiming the project at more of an experimental sound, and putting the concepts at the centre of his studies to practice. ","image":"https://drive.google.com/file/d/1q4TmlS2CwtG_G-4ElhbgpPZQCDtDQU1F/view?fbclid=IwAR23YXtLz8K7fvXTZjBoz699zO_8TjKCdr4BOsJqbopirPyAonT5aaQ-R9c","keywords":["paper"],"pdf_url":"","recs":[],"session":["musico-comp"],"sessionpos":["3"],"soundcloud":"","title":"Repurposing an Acousmatic Skill Set for the Composition of Popular Electronic Music","vimeo":"","website":"https://www.facebook.com/patcarrollmusic/","youtube":""},"forum":"59","id":"59"},{"content":{"TLDR":"This paper discusses the technologies and four notation approaches used to score a series of four  works (Barren, Ferns, Smother, and Chaos) using extended vocal and percussion techniques for Vowels in Retrograde (Rose, 2019). The challenges I was faced with were notating for extended vocal techniques, spatial music, and for players who could not read traditional Western notation. Scoring techniques were chosen according to perceived relevance to the piece. The technologies used were either low-cost or open source, such as Decibel ScorePlayer, iPad, Inkscape, AutoStitch, and assorted physical media and consequent digitising means. The result was a 55-minute suite of new works for extended vocal techniques, percussion, cello, koauau, and electronics. Scoring techniques were derived from a mixture of existing approaches, including text-based (Harlow, 2019; Oliveros, 2013), artwork or image-based (Steiner, 2004), ancient music scoring methods (Daves, 1952; Hickmann, 1956), shapes to indicate breath, sounds, or pitches (Schieve, 1984; Wishart, 1996, 2012), and mixtures of traditional and graphic notation (Christou, 1968; Crumb, 1971).\n\nKeywords: spatial music; extended vocal techniques; graphic notation; ","abstract":"This paper discusses the technologies and four notation approaches used to score a series of four  works (Barren, Ferns, Smother, and Chaos) using extended vocal and percussion techniques for Vowels in Retrograde (Rose, 2019). The challenges I was faced with were notating for extended vocal techniques, spatial music, and for players who could not read traditional Western notation. Scoring techniques were chosen according to perceived relevance to the piece. The technologies used were either low-cost or open source, such as Decibel ScorePlayer, iPad, Inkscape, AutoStitch, and assorted physical media and consequent digitising means. The result was a 55-minute suite of new works for extended vocal techniques, percussion, cello, koauau, and electronics. Scoring techniques were derived from a mixture of existing approaches, including text-based (Harlow, 2019; Oliveros, 2013), artwork or image-based (Steiner, 2004), ancient music scoring methods (Daves, 1952; Hickmann, 1956), shapes to indicate breath, sounds, or pitches (Schieve, 1984; Wishart, 1996, 2012), and mixtures of traditional and graphic notation (Christou, 1968; Crumb, 1971).\n\nKeywords: spatial music; extended vocal techniques; graphic notation; ","authors":["Sophie Rose"],"bandcamp":"","bio":"Sophie Rose is a doctoral student at the University of Melbourne and contemporary vocals lecturer at Australian Institute of Music. She is a singer, extended vocal technique enthusiast, composer, improviser, performer, and maker. She explores the relationship between creative practice, interactive technology, and embodiment in her work. She performs and collaborates regularly with Cloud Unknowing, Brigid Burke, and surrealist music collective Little Songs of the Mutilated. ","image":"https://www.dropbox.com/s/w2z1x18mcrexshe/Sophie%20Rose%20Bio%20Pic%201.jpg?dl=0","keywords":["paper"],"pdf_url":"","recs":[],"session":["synth-nota-inter"],"sessionpos":["3"],"soundcloud":"","title":"Four Approaches to Graphic Notation of Quadrophonic Electroacoustic Music and Extended Vocal Techniques","vimeo":"","website":"https://sophiemusicrose.com/","youtube":"https://youtu.be/QhPTvP4ZDJs"},"forum":"69","id":"69"},{"content":{"TLDR":"This paper will describe the compositional techniques used in the multichannel sound composition \u201cDarshan with a Pelican: Multiplicities\u201d which would have been premiered on May 22 at the GRM Paris 40 channel Acousmonium concert hall, but this performance was cancelled due to the corona virus. (It is tentatively rescheduled for October 10, 2020.)  A 2 channel version, prepared specially for this conference, will be presented here instead.  This piece will incorporate a number of compositional techniques, such as: image sonification using the Virtual ANS Synthesizer; Markov-chain driven melody composing routines; real-time use of the GRM Tools Spaces modules; multiple microtonal scale complexes; virtual analogue sound fragmentation routines \u2013 assembling thick microtonal sound textures with the use of the NYSTHI module toolkit for VCV Rack; and several others.  The aim will be to assemble a large scale (21 minute duration) multi-timbral and multi-textural composition where at any moment, several different sound ideas will be happening, each with their own discrete sound-path spatializations throughout the space.  Making an ever changing ear-dazzling sound complex is the name of the game here.  The paper will also include several sound excerpts from the piece.  ","abstract":"This paper will describe the compositional techniques used in the multichannel sound composition \u201cDarshan with a Pelican: Multiplicities\u201d which would have been premiered on May 22 at the GRM Paris 40 channel Acousmonium concert hall, but this performance was cancelled due to the corona virus. (It is tentatively rescheduled for October 10, 2020.)  A 2 channel version, prepared specially for this conference, will be presented here instead.  This piece will incorporate a number of compositional techniques, such as: image sonification using the Virtual ANS Synthesizer; Markov-chain driven melody composing routines; real-time use of the GRM Tools Spaces modules; multiple microtonal scale complexes; virtual analogue sound fragmentation routines \u2013 assembling thick microtonal sound textures with the use of the NYSTHI module toolkit for VCV Rack; and several others.  The aim will be to assemble a large scale (21 minute duration) multi-timbral and multi-textural composition where at any moment, several different sound ideas will be happening, each with their own discrete sound-path spatializations throughout the space.  Making an ever changing ear-dazzling sound complex is the name of the game here.  The paper will also include several sound excerpts from the piece.  ","authors":["Warren Burt"],"bandcamp":"","bio":"Warren Burt (b 1949): composer, performer, writer, instrument builder, sound poet.  Currently Coordinator of Post Graduate Studies in Music, Box Hill Institute, Melbourne.  Born in the US, moved to Australia in 1975.  Has been involved in music, video, community arts, community radio, education, etc. since arriving.  Currently living and working in Daylesford, Vic.","image":"https://spaces.hightail.com/receive/aVJf7W41c5","keywords":["paper"],"pdf_url":"","recs":[],"session":["synth-nota-inter"],"sessionpos":["4"],"soundcloud":"","title":"Darshan with a Pelican: Multiplicities (2020)","vimeo":"","website":"http://www.warrenburt.com","youtube":""},"forum":"82","id":"82"},{"content":{"TLDR":"This short (15 minute) performance submission accompanies the \u2018Abletweet: Harnessing Social Media APIs for Encoding, Co-Creating and Performing Improvised Generative Electronic Music.\u2019 research paper / artist talk submission for ACMC2020. This performance work seeks to explore the improvisational possibilities offered by the Abletweet device by restricting all melodic and percussive sequences to patterns generated by the device in real time, using digital and analogue hardware.","abstract":"This short (15 minute) performance submission accompanies the \u2018Abletweet: Harnessing Social Media APIs for Encoding, Co-Creating and Performing Improvised Generative Electronic Music.\u2019 research paper / artist talk submission for ACMC2020. This performance work seeks to explore the improvisational possibilities offered by the Abletweet device by restricting all melodic and percussive sequences to patterns generated by the device in real time, using digital and analogue hardware.","authors":["J. Curtis (lysdexic)"],"bandcamp":"","bio":"lysdexic began its existence as dark experiments in freeform sound design, audio manipulation and textural soundsculpting before moving into rhythmic based composition. Using frameworks such as MaxMSP, Arduino, C++ and Python to create custom DSP tools and sampling everything from scratched discs, hacked data files, circuit bent kids toys and his ever growing collection of modified drum machines, lysdexic continues to explore and destroy the audio spectrum.","image":"https://www.dropbox.com/sh/7l9q57kuokzvku5/AABQPNpyQB7VdFX-sMpxrKeua?dl=0","keywords":["music"],"pdf_url":"","recs":[],"session":["C2_Tuesday"],"sessionpos":["TBA"],"soundcloud":"","title":"Abletweet (Performance / live demonstration)","vimeo":"","website":"http://www.lysdexic.com","youtube":""},"forum":"95","id":"95"},{"content":{"TLDR":"This talk will demonstrate a new system for generating variations on segments of music that have been previously recorded or composed and saved as MIDI files.\n\nThe system builds on work presented at the ACMC 2018 Perth conference which was documented in the paper Hacking Music Notation with Bach and Cage (Hirst, 2018). That paper documented the \u201cComposer\u2019s Little Helper\u201d (CLH) which is a Max patcher that makes use of the \u201cBach\u201d and \u201cCage\u201d libraries to manipulate and mix musical notation that has been saved in a \u201cshelf\u201d as separate segments. CLH implements musical operations such as transposition, inversion, retrograde, plus a number of other \u201ctreatments\u201d that use the high-level \u201cCage\u201d library for real-time computer-aided composition.\n\nThe Composer\u2019s Little Helper was a non-realtime system to assist in the compositional process. In contrast, Vari-Gen is like the next generation development where a generative system is used to produce note information in realtime. Reading from a MIDI file, variations in pitch, onset time, durations and note velocity are produced and can be varied in realtime using a Markov methodology for each parameter independently.\n\nUsing the \u201cbach.roll\u201d representation of the original MIDI file, which is a musical staff with note-heads notated in proportional notation, the composer/performer can select a specific number of notes for treatment, vary the transposition, durations, onset times, velocities, and the order of the Markov chain for each parameter of the variation generator.\n\nRealtime output triggers sound using a VST instrument plugin, which can be varied to suit the musical style. The note output is recorded in another bach.roll object, which can be saved as a MIDI file at any time.\n\nIn essence, the system functions as an exotic sequencer/variation generator, and because it all happens in realtime, it could also function as a performance instrument.\n\nThe two part work Pierre and Frank was created using a combination of the Vari-Gen software and non-realtime editing (to be screened at ACMC 2020).","abstract":"This talk will demonstrate a new system for generating variations on segments of music that have been previously recorded or composed and saved as MIDI files.\n\nThe system builds on work presented at the ACMC 2018 Perth conference which was documented in the paper Hacking Music Notation with Bach and Cage (Hirst, 2018). That paper documented the \u201cComposer\u2019s Little Helper\u201d (CLH) which is a Max patcher that makes use of the \u201cBach\u201d and \u201cCage\u201d libraries to manipulate and mix musical notation that has been saved in a \u201cshelf\u201d as separate segments. CLH implements musical operations such as transposition, inversion, retrograde, plus a number of other \u201ctreatments\u201d that use the high-level \u201cCage\u201d library for real-time computer-aided composition.\n\nThe Composer\u2019s Little Helper was a non-realtime system to assist in the compositional process. In contrast, Vari-Gen is like the next generation development where a generative system is used to produce note information in realtime. Reading from a MIDI file, variations in pitch, onset time, durations and note velocity are produced and can be varied in realtime using a Markov methodology for each parameter independently.\n\nUsing the \u201cbach.roll\u201d representation of the original MIDI file, which is a musical staff with note-heads notated in proportional notation, the composer/performer can select a specific number of notes for treatment, vary the transposition, durations, onset times, velocities, and the order of the Markov chain for each parameter of the variation generator.\n\nRealtime output triggers sound using a VST instrument plugin, which can be varied to suit the musical style. The note output is recorded in another bach.roll object, which can be saved as a MIDI file at any time.\n\nIn essence, the system functions as an exotic sequencer/variation generator, and because it all happens in realtime, it could also function as a performance instrument.\n\nThe two part work Pierre and Frank was created using a combination of the Vari-Gen software and non-realtime editing (to be screened at ACMC 2020).","authors":["David Hirst"],"bandcamp":"","bio":"David Hirst\u2019s electroacoustic music compositions have been performed in the United States, Canada, the UK, the Netherlands, New Zealand, South Korea and nationally across Australia. He studied computer music at La Trobe University, composition with Jonty Harrison at the University of Birmingham, and completed a PhD in electroacoustic music composition and analysis at the University of Melbourne. Hirst has worked as an academic at the Tasmanian Conservatorium of Music, La Trobe University, and at the University of Melbourne. He is currently Honorary Principal Fellow at the Melbourne Conservatorium of Music, University of Melbourne. His most recent album, \u201cThe Shape of Water\u201d is available on iTunes and Spotify.","image":"9","keywords":["paper"],"pdf_url":"","recs":[],"session":["synth-nota-inter"],"sessionpos":["1"],"soundcloud":"","title":"Vari-Gen: A Generative System for Creating Musical Variations Using Max, Bach, and Cage in Real-Time","vimeo":"","website":"https://davidhirst.me","youtube":""},"forum":"9","id":"9"},{"content":{"TLDR":"This work is part of an ongoing interest in the intersection of acoustic ecology, novel synthesis techniques and generative composition. It imitates and deconstructs the timbral, structural and textural aspects of natural sound environments using a combination of processed field recordings and self-playing/generative synthesizer patches developed in the Pure Data visual programming environment. ","abstract":"This work is part of an ongoing interest in the intersection of acoustic ecology, novel synthesis techniques and generative composition. It imitates and deconstructs the timbral, structural and textural aspects of natural sound environments using a combination of processed field recordings and self-playing/generative synthesizer patches developed in the Pure Data visual programming environment. ","authors":["Benjamin Keough"],"bandcamp":"","bio":"Ben Keough is an experimental composer/improviser and amateur photographer/videographer currently living in Bellingen on the NSW Mid North Coast. A graduate of the ANU School of Music, his composition work is based around digital synthesis and generative sequencing techniques (using the Pure Data development environment), field recording and electroacoustic improvisation.","image":"https://drive.google.com/file/d/13Q2OG8j2DopYI_KJTLRVX4J_XxiCM8Vu/view?usp=sharing","keywords":[""],"pdf_url":"","recs":[],"session":["P1_Monday"],"sessionpos":["3"],"soundcloud":"","title":"Reflections from the outside","vimeo":"","website":"","youtube":""},"forum":"93","id":"93"},{"content":{"TLDR":"Torrents was an idea first formed in 2017 while working as an artist in residence in Sydney, Australia. The idea was to create an interactive performance presented through a quadrophonic speaker array. Torrents 1.5 is a development on this original foundation.\n\nOriginally, the work was presented as a live performance that then gave way to audience interaction. Torrents 1.5 takes this idea and presents a format for an interactive sound art installation. Torrents 1.5 is intended to be an installation that is interacted with through a midi controller. The installation is based around a PureData patch that allows an audience member to interact with the sounds that are coming out through each individual speaker. The audience member can interact with the music of the installation and create a soundscape that has the potential to be completely unique to that of the next person. Torrents 1.5 is a development of an idea but not it\u2019s final intended state. This installation is planned for a multi-speaker array, four or above. 1.5 is limited to the stereo field for the purpose of this presentation.\n\nThe soundscape that makes up Torrents 1.5 is a combination of originally composed music and field recordings that were captured during a one-month residency in Itoshima, Japan. The piece you hear as part of Torrents 1.5 is a section for the work Dual Location, which paired field recordings from the rural area of Itoshima with recordings of Fukuoka city. These recordings inform the aesthetic direction of the work while building the foundations of the soundscape. Presenting Dual Locations through the medium of Torrents 1.5 allows me to explore the composition in a new light, further developing the existing material.\n\nConceptually, the work is based on a loose interpretation of the way digital files are shared through peer-to-peer sharing. When a torrent is downloaded through a peer-to-peer service the file is created by taking information from multiple sources and combining them to create a complete whole. Following this idea, Torrents 1.5 is using multiple sounds that are then being dispersed through a speaker array (however large it may be) that then creates the sum of the work. Each speaker acts as a \u2018seed\u2019 within the piece and as the user interacts with the interface, they are able to develop the sum of the work \u2013 in a sense mimicking the role of the downloader.\n\nThe purpose of Torrents 1.5 is to develop new and novel ways of approaching sound-art installation and exploring the possibilities that programs such as PureData allow within the medium. Another aim of the project is to present new possibilities for audience participation with an installation. This could lead to further develop of the project or spawn ideas for new projects that follow the same or similar template; for example, combining the work with live performance.","abstract":"Torrents was an idea first formed in 2017 while working as an artist in residence in Sydney, Australia. The idea was to create an interactive performance presented through a quadrophonic speaker array. Torrents 1.5 is a development on this original foundation.\n\nOriginally, the work was presented as a live performance that then gave way to audience interaction. Torrents 1.5 takes this idea and presents a format for an interactive sound art installation. Torrents 1.5 is intended to be an installation that is interacted with through a midi controller. The installation is based around a PureData patch that allows an audience member to interact with the sounds that are coming out through each individual speaker. The audience member can interact with the music of the installation and create a soundscape that has the potential to be completely unique to that of the next person. Torrents 1.5 is a development of an idea but not it\u2019s final intended state. This installation is planned for a multi-speaker array, four or above. 1.5 is limited to the stereo field for the purpose of this presentation.\n\nThe soundscape that makes up Torrents 1.5 is a combination of originally composed music and field recordings that were captured during a one-month residency in Itoshima, Japan. The piece you hear as part of Torrents 1.5 is a section for the work Dual Location, which paired field recordings from the rural area of Itoshima with recordings of Fukuoka city. These recordings inform the aesthetic direction of the work while building the foundations of the soundscape. Presenting Dual Locations through the medium of Torrents 1.5 allows me to explore the composition in a new light, further developing the existing material.\n\nConceptually, the work is based on a loose interpretation of the way digital files are shared through peer-to-peer sharing. When a torrent is downloaded through a peer-to-peer service the file is created by taking information from multiple sources and combining them to create a complete whole. Following this idea, Torrents 1.5 is using multiple sounds that are then being dispersed through a speaker array (however large it may be) that then creates the sum of the work. Each speaker acts as a \u2018seed\u2019 within the piece and as the user interacts with the interface, they are able to develop the sum of the work \u2013 in a sense mimicking the role of the downloader.\n\nThe purpose of Torrents 1.5 is to develop new and novel ways of approaching sound-art installation and exploring the possibilities that programs such as PureData allow within the medium. Another aim of the project is to present new possibilities for audience participation with an installation. This could lead to further develop of the project or spawn ideas for new projects that follow the same or similar template; for example, combining the work with live performance.","authors":["Ben Harb"],"bandcamp":"","bio":"Ben Harb is a sound artist, composer and performer from Canberra, Australia. As a graduate of the Australian National University\u2019s School of Music Ben has worked with many members of the local and international experimental music and art communities including the ANU New Music Ensemble, the Experimental Music Studio, Opus and bands such as Ecruteak and Lost Coast. Ben\u2019s work focuses on the relationship between fixed media and live instrumentation with an emphasis on the performer\u2019s agency when playing a piece.\n\nBen\u2019s interests as a composer lie between contemporary Post-Rock and the experimental sounds of movements such as minimalism and free improvisation.","image":"44","keywords":[""],"pdf_url":"","recs":[],"session":["TBA"],"sessionpos":["TBA"],"soundcloud":"","title":"Torrents 1.5","vimeo":"","website":"https://www.benharbcomposition.com/","youtube":"https://youtu.be/DreBmTbhZKk"},"forum":"44","id":"44"},{"content":{"TLDR":"Totem is a spirit being, sacred object, or symbol that serves as an emblem to a group of people, such as a family, clan, lineage, or tribe. The sound & vision interprets peoples roles and responsibilities, and their relationships with each other and creation. The sound & visual encompasses topics and ideas around, electronic music, electronic art, social sustainability, sociology, justice, risk, safety, adaptation, energy consumption, right to the city.\n\nOur Common Future\n\nThe Land Owns Us\n\nTotem\n\nSwell\n\nCustodian\n\nMythic Landscape\n\nAncient Land\n\nElders\n\nSaltwater\n\nI acknowledge the Traditional Owners of the land, the\u00a0Bunurong people\u00a0on which I work. I pay my respects to their Elders, past,  present and emerging.\n\nWARNING: Aboriginal and Torres Strait Islander viewers are warned that this film may contain images and voices of deceased persons.","abstract":"Totem is a spirit being, sacred object, or symbol that serves as an emblem to a group of people, such as a family, clan, lineage, or tribe. The sound & vision interprets peoples roles and responsibilities, and their relationships with each other and creation. The sound & visual encompasses topics and ideas around, electronic music, electronic art, social sustainability, sociology, justice, risk, safety, adaptation, energy consumption, right to the city.\n\nOur Common Future\n\nThe Land Owns Us\n\nTotem\n\nSwell\n\nCustodian\n\nMythic Landscape\n\nAncient Land\n\nElders\n\nSaltwater\n\nI acknowledge the Traditional Owners of the land, the\u00a0Bunurong people\u00a0on which I work. I pay my respects to their Elders, past,  present and emerging.\n\nWARNING: Aboriginal and Torres Strait Islander viewers are warned that this film may contain images and voices of deceased persons.","authors":["Seafar (Steven Harran)"],"bandcamp":"","bio":"Seafar - singular, limits bursting rhythm & sound design transcendence. Wander the streets together, the noise of life muted by sheets of rain, brilliant colours dimmed in a pervading mist of grey. An instrumental amalgam of moody beats and synth drenched noises.","image":"https://drive.google.com/file/d/14kYIU9AeFhtUB332fr99_FwWmf77CdAp/view?usp=sharing","keywords":["music"],"pdf_url":"","recs":[],"session":["C3_Wednesday"],"sessionpos":["1"],"soundcloud":"","title":"Totem","vimeo":"","website":"http://www.seafar.co","youtube":""},"forum":"4","id":"4"},{"content":{"TLDR":"Unseen (2019) is a work for modular synthesiser and live visuals. The work was developed using images of Diffusion MRI scans of the human brain as source material, images that were kindly provided by Prof Fernando Calamante of the University of Sydney/Sydney Imaging. This work explores the concept of \u2018networks\u2019, both figuratively and literally, through a tight connection between a bespoke modular synthesis patch and dynamically processed MRI images. These source images show in great detail the connections between various parts of the brain. Such images allow researchers to understand the enormous complexity of the brain\u2019s interconnected structure. \n\nIn this work, these source images are placed in a 3-dimensional scene and subjected to various forms of image processing and manipulation. These processed images are used training data for a neural network model trained in the Wekinator software, whose resultant outputs are converted into control voltages in response to analyses of the real-time movement of these images during a performance. These voltages are then used to control aspects of a modular synthesis patch. The sounds created by the synthesist are derived both from internal generative processes, as well as via the neural network outputs themselves, and the sound material is in turn analysed and used to manipulate the 3D scene which is being analysed by the neural network. This creates a tightly woven feedback loop between image, sound, digital information and control voltage. This interconnected feedback process is presented as a performative and compositional allegory for the complexity of the networked structure of the brain, enabling the performer to both affect, and be affected by a complex audio-visual network. Unseen was developed in response to the theme \u2018Seeing the Unseen: From Brains to Black Holes\u2019, and was commissioned by the University of Sydney as part of the 2019 Innovation Week program.\n\n","abstract":"Unseen (2019) is a work for modular synthesiser and live visuals. The work was developed using images of Diffusion MRI scans of the human brain as source material, images that were kindly provided by Prof Fernando Calamante of the University of Sydney/Sydney Imaging. This work explores the concept of \u2018networks\u2019, both figuratively and literally, through a tight connection between a bespoke modular synthesis patch and dynamically processed MRI images. These source images show in great detail the connections between various parts of the brain. Such images allow researchers to understand the enormous complexity of the brain\u2019s interconnected structure. \n\nIn this work, these source images are placed in a 3-dimensional scene and subjected to various forms of image processing and manipulation. These processed images are used training data for a neural network model trained in the Wekinator software, whose resultant outputs are converted into control voltages in response to analyses of the real-time movement of these images during a performance. These voltages are then used to control aspects of a modular synthesis patch. The sounds created by the synthesist are derived both from internal generative processes, as well as via the neural network outputs themselves, and the sound material is in turn analysed and used to manipulate the 3D scene which is being analysed by the neural network. This creates a tightly woven feedback loop between image, sound, digital information and control voltage. This interconnected feedback process is presented as a performative and compositional allegory for the complexity of the networked structure of the brain, enabling the performer to both affect, and be affected by a complex audio-visual network. Unseen was developed in response to the theme \u2018Seeing the Unseen: From Brains to Black Holes\u2019, and was commissioned by the University of Sydney as part of the 2019 Innovation Week program.\n\n","authors":["Benjamin Carey"],"bandcamp":"","bio":"Benjamin Carey is a composer, performer and researcher. Ben\u2019s research and practice is concerned with musical interactivity, generativity and the delicate dance between human and machine agencies in composition and performance. He has released four albums, the latest of which is an LP of modular synthesis works entitled ANTIMATTER (2019, Hospital Hill Records). Ben\u2019s work has been performed and exhibited nationally and internationally at numerous festivals and academic conferences including the Huddersfield Festival of Contemporary Music (UK), IRCAM Live @ La Gait\u00e9 Lyrique (France), the International Computer Music Conference (Australia) and the International Conference on New Interfaces for Musical Expression (USA, UK and Australia). Ben is currently Lecturer in Composition and Music Technology at the University of Sydney, Conservatorium of Music.","image":"","keywords":["music"],"pdf_url":"","recs":[],"session":["TBA"],"sessionpos":["TBA"],"soundcloud":"https://soundcloud.com/ben_carey","title":"Unseen - for modular synthesiser and live visuals","vimeo":"","website":"https://bencarey.net","youtube":"https://youtu.be/AstlNI8VTRk"},"forum":"97","id":"97"},{"content":{"TLDR":"Volca is a 10-minute fixed, stereo electroacoustic work designed around the exploration of the small Korg analog drum machine, the \u2018Volca Beats\u2019. Entirely all sounds were created from the source material of the analog kick drum, snare drum, tom and hi-hat engines, as well as from the machine noise of the unit itself. The compositional process involved the creation of a set processing patches created in Ableton Live and Bitwig, that involved rhythmically controlled randomisation the parameters of various audio effects, of which the output was recorded. The idea of this process was to create a sound design situation with a high chance of stumbling on happy accidents that could then be used in the creation of micro sound units and longer structures. The piece also explores a reductive exploration of the sounds of the Volca Beats by avoiding the dance music rhythmic structures that the unit is designed to sequence and focussing on the more unexpected sonic possibilities of the machine. However, many sounds and sections do reference the unit\u2019s close association with rhythm and popular electronic music.","abstract":"Volca is a 10-minute fixed, stereo electroacoustic work designed around the exploration of the small Korg analog drum machine, the \u2018Volca Beats\u2019. Entirely all sounds were created from the source material of the analog kick drum, snare drum, tom and hi-hat engines, as well as from the machine noise of the unit itself. The compositional process involved the creation of a set processing patches created in Ableton Live and Bitwig, that involved rhythmically controlled randomisation the parameters of various audio effects, of which the output was recorded. The idea of this process was to create a sound design situation with a high chance of stumbling on happy accidents that could then be used in the creation of micro sound units and longer structures. The piece also explores a reductive exploration of the sounds of the Volca Beats by avoiding the dance music rhythmic structures that the unit is designed to sequence and focussing on the more unexpected sonic possibilities of the machine. However, many sounds and sections do reference the unit\u2019s close association with rhythm and popular electronic music.","authors":["Patrick Carroll"],"bandcamp":"","bio":"Patrick Carroll is a composer from Sydney, and a current PhD candidate at the Sydney Conservatorium of Music, studying the intersection of traditional acousmatic theory and popular genres of electronic music. While completing his Bachelor of Music (Composition) with Honours in 2015, Carroll began releasing and publishing dance music and electronica under the alias Piecey, amassing over a million plays on Spotify to date. In 2018, Carroll began releasing music under his own name (Pat Carroll), aiming the project at more of an experimental sound, and putting the concepts at the centre of his studies to practice. ","image":"https://drive.google.com/file/d/1q4TmlS2CwtG_G-4ElhbgpPZQCDtDQU1F/view?fbclid=IwAR23YXtLz8K7fvXTZjBoz699zO_8TjKCdr4BOsJqbopirPyAonT5aaQ-R9c","keywords":["music"],"pdf_url":"","recs":[],"session":["P4_Thursday"],"sessionpos":["3"],"soundcloud":"https://soundcloud.com/patrick-carroll-music/volca","title":"Volca","vimeo":"","website":"https://www.facebook.com/patcarrollmusic/","youtube":""},"forum":"60","id":"60"},{"content":{"TLDR":"We present output of research into the integration of human flight and musical performance via telemetry driven, spatial meta-compositions. The work explores the use of a sailplane flight path as a structure around which musical form can emerge. It serves as a point of reflection on the composition of such spatial meta-compositions and their potential for enhancing personal experiences and enabling new modes of artistic performance in mixed realities.\n\nThe system has three primary components - a body-worn sensing unit responsible for logging and broadcasting positional data, a log playback system, and a suite of composition modules built in Max for Live.\n\nThe positional sensing unit consists of an Ardupilot-based flight controller and Raspberry Pi computer. The Raspberry Pi communicates with the Ardupilot board via the Mavlink protocol allowing for logging and broadcasting of captured sense data as Open Sound Control (OSC). The system has been designed such that it can be used for logging data as well as communicating in real-time with the composition modules either locally over Wi-Fi or remotely via 433Mhz radio link. The telemetry playback module was built in Max allowing for the replaying of logged telemetry alongside synchronised video recordings. This enables the composer to create a spatial meta-composition \"offline\" for later use in a real-time performance context.\n\nThe piece Aileron One is the output of an iterative, creative process consisting of cycles of music composition and interactive system development. The work uses only a subset of the data points captured during the flight; altitude (position above the ground in metres), total velocity (in metres per second), and heading (orientation around the up vector in degrees).\n\nAltitude serves as the foundational element on which the piece is built. The piece can be considered as a vertical structure rising to approximately 950 metres above the ground. A circular chord progression (Fmaj9 Fmaj9/E Am9 G) repeats along the up vector with instrumentation becoming denser in six, discrete, evenly spaced layers. To add variation and movement, a set of instruments are panned to four evenly spaced compass headings. Total velocity is then used to control the speed of melodic and percussive elements throughout the piece.\n\nThe spatial approach leads to interesting temporal-harmonic consequences: as the glider ascends, the repeating chord progression is played in one direction, as the glider descends the progression is played in the other direction. The rate at which the progression moves is dictated by the vertical velocity of the glider. The harmonic progression ascends at a steady rate as the glider is towed into the air, leading to a point of contrast when the tow is released and the glider transitions to a slow, steady descent. The musical effect of these moments is an aesthetic mirroring of the dynamics of the aircraft.\n\nIt can also be seen that during periods of rapid ascent and descent such as during aerobatic manoeuvres, the musical result is less coherent as chords pass to quickly to establish effective tension and release. This highlights a challenge of composing spatial meta-compositions, that is how to spatially scale musical properties such that passing through them at a range of velocities produces satisfying musical results.\n\nThe work as it is presented is part of an active research effort and as such will continue to be refined. We foresee potential to create far richer work by creating coherent mappings between musical elements and data components resulting in complex, emergent musical results.\n\nWe are actively investigating gliding flight alongside other forms of physical activities such as kayaking and rock climbing to determine both the performative potential of these activities as well as the musical considerations when composing spatial meta-compositions of contrasting scales and physical contexts.","abstract":"We present output of research into the integration of human flight and musical performance via telemetry driven, spatial meta-compositions. The work explores the use of a sailplane flight path as a structure around which musical form can emerge. It serves as a point of reflection on the composition of such spatial meta-compositions and their potential for enhancing personal experiences and enabling new modes of artistic performance in mixed realities.\n\nThe system has three primary components - a body-worn sensing unit responsible for logging and broadcasting positional data, a log playback system, and a suite of composition modules built in Max for Live.\n\nThe positional sensing unit consists of an Ardupilot-based flight controller and Raspberry Pi computer. The Raspberry Pi communicates with the Ardupilot board via the Mavlink protocol allowing for logging and broadcasting of captured sense data as Open Sound Control (OSC). The system has been designed such that it can be used for logging data as well as communicating in real-time with the composition modules either locally over Wi-Fi or remotely via 433Mhz radio link. The telemetry playback module was built in Max allowing for the replaying of logged telemetry alongside synchronised video recordings. This enables the composer to create a spatial meta-composition \"offline\" for later use in a real-time performance context.\n\nThe piece Aileron One is the output of an iterative, creative process consisting of cycles of music composition and interactive system development. The work uses only a subset of the data points captured during the flight; altitude (position above the ground in metres), total velocity (in metres per second), and heading (orientation around the up vector in degrees).\n\nAltitude serves as the foundational element on which the piece is built. The piece can be considered as a vertical structure rising to approximately 950 metres above the ground. A circular chord progression (Fmaj9 Fmaj9/E Am9 G) repeats along the up vector with instrumentation becoming denser in six, discrete, evenly spaced layers. To add variation and movement, a set of instruments are panned to four evenly spaced compass headings. Total velocity is then used to control the speed of melodic and percussive elements throughout the piece.\n\nThe spatial approach leads to interesting temporal-harmonic consequences: as the glider ascends, the repeating chord progression is played in one direction, as the glider descends the progression is played in the other direction. The rate at which the progression moves is dictated by the vertical velocity of the glider. The harmonic progression ascends at a steady rate as the glider is towed into the air, leading to a point of contrast when the tow is released and the glider transitions to a slow, steady descent. The musical effect of these moments is an aesthetic mirroring of the dynamics of the aircraft.\n\nIt can also be seen that during periods of rapid ascent and descent such as during aerobatic manoeuvres, the musical result is less coherent as chords pass to quickly to establish effective tension and release. This highlights a challenge of composing spatial meta-compositions, that is how to spatially scale musical properties such that passing through them at a range of velocities produces satisfying musical results.\n\nThe work as it is presented is part of an active research effort and as such will continue to be refined. We foresee potential to create far richer work by creating coherent mappings between musical elements and data components resulting in complex, emergent musical results.\n\nWe are actively investigating gliding flight alongside other forms of physical activities such as kayaking and rock climbing to determine both the performative potential of these activities as well as the musical considerations when composing spatial meta-compositions of contrasting scales and physical contexts.","authors":["Robert Jarvis"],"bandcamp":"","bio":"Robert Jarvis is an accomplished audio-visual artist based in Melbourne, Australia. He works across live video performance, music, animation and software development, with a focus on the development of tools for live audio-visual performance. He is currently a PhD candidate at RMIT University where he is exploring the intersection of gliding flight and musical performance.","image":"https://bobzeal.sg3.quickconnect.to/d/s/560821039641178301/6tLKnO5ckqEvfcPCDNZcpuBj0xlA_ocF--7AACtVvyAc_","keywords":["paper"],"pdf_url":"","recs":[],"session":["daws-live-algo"],"sessionpos":["3"],"soundcloud":"","title":"Vertical Harmony: Flight-path as Musical Form","vimeo":"","website":"https://zeal.co","youtube":"https://youtu.be/Ct-Sb-zGlG0"},"forum":"64","id":"64"},{"content":{"TLDR":"With the development of free accessible Artificial Intelligence (AI) musical compositional tools such as Google\u2019s Magenta Studio it is now possible for musicians with little or no computer programming experience to utilize AI to assist in their compositions of musical works. This paper discusses and evaluates the techniques used in creating electro acoustics works of the acid genre for performance utilizing these tools and the performance of this created work. This research suggests that musicians that can utilize AI as part of their compositional process without the need for in-depth programming knowledge alongside their traditional compositional methods. The tools reported on in this research do not replace compositional techniques rather they can augment the compositional process. ","abstract":"With the development of free accessible Artificial Intelligence (AI) musical compositional tools such as Google\u2019s Magenta Studio it is now possible for musicians with little or no computer programming experience to utilize AI to assist in their compositions of musical works. This paper discusses and evaluates the techniques used in creating electro acoustics works of the acid genre for performance utilizing these tools and the performance of this created work. This research suggests that musicians that can utilize AI as part of their compositional process without the need for in-depth programming knowledge alongside their traditional compositional methods. The tools reported on in this research do not replace compositional techniques rather they can augment the compositional process. ","authors":["Dylan Davis"],"bandcamp":"","bio":"Dr Dylan Davis is a design researcher and lecturer at Swinburne University, Melbourne, Australia. He has extensive experience in researching design process, community engagement, digital storytelling, interaction design, audio production and audio composition. Dylan is also a composer and a musician whose practice covers performance, and production of electronic music. His Non-traditional Research Outputs include works for festivals such as Melbourne Music Week, recordings for a range of international record labels and community based music projects. This research utilizes reflective practice to further understand and explore the compositional and performance methods and practices for electronic and electroacoustic music of the techno and acid house genre. Dylan is an extremely prolific musician with over 100 listed works on APPRA/AMCOS. Dylan\u2019s musical works have been released on various record labels in USA, Europe, Asia and Australia.  ","image":"","keywords":["paper"],"pdf_url":"","recs":[],"session":["daws-live-algo"],"sessionpos":["1"],"soundcloud":"","title":"Compute and Resonate. Creating electro acoustic music of the acid genre for presentation within a club environment utilizing accessible Artificial Intelligence and computer based generative tools.","vimeo":"","website":"https://soundcloud.com/dylabs","youtube":""},"forum":"8","id":"8"},{"content":{"TLDR":"Within the field of spatial audio exists a variety of barriers to participation and engagement. These issues range from financial accessibility issues (where equipment can cost thousands of dollars) to class issues (where many are excluded because of the cultures surrounding the music and the spaces in which it occurs). There are also barriers to engagement erected through the types of spatial systems constructed, the listening environment that is encouraged, and the type of music that people are encouraged to compose. The fundamentality of the \u2018sweet spot\u2019 within much spatial music automatically excludes those outside of the sweet spot from most likely experiencing the composer\u2019s intentions.\n\nThis paper looks at the creation of a new compositional framework that is intended for use in facilitating non-sweet spot oriented multichannel works. It discusses the construction and utilisation of the framework, listener and composer responses from a test case of framework, while also touching on future work that has been inspired by these investigations.\n","abstract":"Within the field of spatial audio exists a variety of barriers to participation and engagement. These issues range from financial accessibility issues (where equipment can cost thousands of dollars) to class issues (where many are excluded because of the cultures surrounding the music and the spaces in which it occurs). There are also barriers to engagement erected through the types of spatial systems constructed, the listening environment that is encouraged, and the type of music that people are encouraged to compose. The fundamentality of the \u2018sweet spot\u2019 within much spatial music automatically excludes those outside of the sweet spot from most likely experiencing the composer\u2019s intentions.\n\nThis paper looks at the creation of a new compositional framework that is intended for use in facilitating non-sweet spot oriented multichannel works. It discusses the construction and utilisation of the framework, listener and composer responses from a test case of framework, while also touching on future work that has been inspired by these investigations.\n","authors":["Jesse Austin-Stewart","Bridget Johnson"],"bandcamp":"","bio":"Jesse Austin-Stewart is a Te Whanganui-a-Tara-based sonic artist with a focus on spatial sound. He is currently working on his PhD at Massey University researching barriers of engagement within spatial audio.\n\nBridget Johnson creates immersive sound installations and performances that heighten the audiences experience with spatial audio. Her work focuses on exploring the way sound can move through space and developing new interfaces to allow composers and performers to further explore expressivity through real time spatialisation in their work. Her installations explore these themes in combination with site-specificity and abstraction of time.\n","image":"46","keywords":["paper"],"pdf_url":"","recs":[],"session":["spatial-perf"],"sessionpos":["2"],"soundcloud":"","title":"Multiple Monophony and the Multichannel Monophonic Compositional Framework","vimeo":"","website":"https://www.jesseaustinstewart.com/","youtube":"https://youtu.be/op0PMnH6SHc"},"forum":"46","id":"46"},{"content":{"TLDR":"\u5983\u5b50\u7b11 (Smile of Yang) a real-time interactive performance for Wacom and Kyma.\n\n\u5983\u5b50\u7b11 (Smile of Yang) is inspired by the Qing Palace Quatrain by Du Mu of the Tang Dynasty and Chinese traditional Cantonese Opera Lychee Ode. One phrase used in the composition is \u201c\u4e00\u9a91\u7ea2\u5c18\u5983\u5b50\u7b11 (Yi Ji Hong Chen Fei Zi Xiao)\u201d. \u5983\u5b50 refers to Imperial Noble Consort Yang; she is the love of Emperor Tang Xuanzong of the Tang Dynasty. The backstory of this phrase is that Yang loves the lychee of Lingnan, but Lingnan is very far from the Palace, and the lychee is challenging to keep fresh during transportation. The Emperor to let Yang eat the fresh lychee, ordered officers and soldiers to use horses to travel overnight to transport lychee to the palace. Therefore, many horses died in transport. When Yang saw the fresh lychee, she smiled. She did not think of the officers, soldiers, and horses. Because of Yang, Emperor lost the loyalty of officials and the army. Finally, under the pressure of the military, he had to hang Yang. \u201cYi Ji Hong Chen Fei Zi Xiao\u201d is also the lyrics of the Lychee Ode in Cantonese Opera. Most of the sounds in this composition are human voices: I use Cantonese to say, \u201cYi Ji Hong Chen Fei Zi Xiao.\u201d","abstract":"\u5983\u5b50\u7b11 (Smile of Yang) a real-time interactive performance for Wacom and Kyma.\n\n\u5983\u5b50\u7b11 (Smile of Yang) is inspired by the Qing Palace Quatrain by Du Mu of the Tang Dynasty and Chinese traditional Cantonese Opera Lychee Ode. One phrase used in the composition is \u201c\u4e00\u9a91\u7ea2\u5c18\u5983\u5b50\u7b11 (Yi Ji Hong Chen Fei Zi Xiao)\u201d. \u5983\u5b50 refers to Imperial Noble Consort Yang; she is the love of Emperor Tang Xuanzong of the Tang Dynasty. The backstory of this phrase is that Yang loves the lychee of Lingnan, but Lingnan is very far from the Palace, and the lychee is challenging to keep fresh during transportation. The Emperor to let Yang eat the fresh lychee, ordered officers and soldiers to use horses to travel overnight to transport lychee to the palace. Therefore, many horses died in transport. When Yang saw the fresh lychee, she smiled. She did not think of the officers, soldiers, and horses. Because of Yang, Emperor lost the loyalty of officials and the army. Finally, under the pressure of the military, he had to hang Yang. \u201cYi Ji Hong Chen Fei Zi Xiao\u201d is also the lyrics of the Lychee Ode in Cantonese Opera. Most of the sounds in this composition are human voices: I use Cantonese to say, \u201cYi Ji Hong Chen Fei Zi Xiao.\u201d","authors":["Yue Pan"],"bandcamp":"","bio":"Yue Pan was born in China. She is an intermedia composer and performer, and she enjoys sound design. Her compositions have been performed internationally, including National Student Electronic Music Event (2020), Turn Up multimedia Festival (2020), and New York City Electroacoustic Music Festival (2020). She is currently pursuing her MM in Intermedia Music Technology from the University of Oregon.","image":"","keywords":["music"],"pdf_url":"","recs":[],"session":["C4_Thursday"],"sessionpos":["5"],"soundcloud":"","title":"\u5983\u5b50\u7b11 Smile of Yang","vimeo":"","website":"","youtube":"https://youtu.be/PQORilh9RSE"},"forum":"50","id":"50"}]
