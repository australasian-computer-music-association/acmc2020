UID,title,authors,download_url,abstract,bio,image_url,online_url,website,keywords,session_name,session_position,soundcloud,youtube,vimeo,bandcamp
1,"""City circles"": sound collage on urban canvas",Subespai,https://u.pcloud.link/publink/show?code=XZmt9UkZNb4u0YkwYiYaEqV7FcJX0Ltv0zLX,"Scene. A nondescript hum smothers the echoes of a distant playground. The eeriness intensifies as the afternoon loses the daily battle against the evening soundscape. ""Where is this?"". The sonic familiarity puts the name of a place on the tip of the tongue right as the movement changes, or does it? The hum left unannounced, that's a fact; now there's something else filling the space, something deeper, more spiritual even. Repetition gives way to flow; quietness prevails, for a while at least. Feedback turns into flares as the night falls. ""Is this still the West?"". Continental lines may or may not have been crossed, the only way to find out is to wander further, facing novelty until everything becomes known again. A faint sadness lingers in the air. End of scene.
Composed & performed by Subespai (Mauri Edo)

Visuals by Josh Paton

A/V support by Matthew Syres","Subespai is Mauri Edo, a Sydney-based sound artist working with found sound, repetition and volume to explore and express complex thoughts and ideas on existence, society and the individual.",https://u.pcloud.link/publink/show?code=XZ5O9UkZDitquhSmSf0zp1z7g8yO9j74qFvX,https://youtu.be/XdphlWifu5Q,http://subespai.net/,music,C1_Monday,TBA,,https://youtu.be/XdphlWifu5Q,,
2,Corrupted Vinyl,Adam Melzer,,"Corrupted Vinyl is an acousmatic, fixed media composition. It is noise based, and programmatic in that it musicalises the sonic deconstruction of a vinyl record player. Spectromorphological processes are central to the sonic structuring of the work, with most manipulations pertaining to sonic artifacts often heard in vinyl reproduction. There are two overarching sections: A (0:00 - 3:33) and B (3:34 - 5:10); with an additional coda (5:11 - 6:23).

","Adam Melzer is a young, Sydney-based composer. His compositional work is varied, but more recently has focussed in on both acousmatic and experimental music domains. He graduated from the Sydney Conservatorium of Music in 2019 with the result of a First Class Bachelor of Composition (Honours). In addition, he has put on his own concerts as a part of Audioshape; was a featured artist in the November 2019 playlist ""Evolving Waves"", curated by Making Waves New Music; and was a featured artist throughout the 2020 Hibernation Festival with the contribution of three livestreams and three audiovisual works.",2,,https://www.facebook.com/AdamMelzerMusic/?ref=bookmarks,music,P2_Tuesday,TBA,,,,
4,Totem,Seafar (Steven Harran),https://drive.google.com/file/d/1B5V8-73NvAofw3B0qgSICgr8pApkCkJx/view?usp=sharing,"Totem is a spirit being, sacred object, or symbol that serves as an emblem to a group of people, such as a family, clan, lineage, or tribe. The sound & vision interprets peoples roles and responsibilities, and their relationships with each other and creation. The sound & visual encompasses topics and ideas around, electronic music, electronic art, social sustainability, sociology, justice, risk, safety, adaptation, energy consumption, right to the city.

Our Common Future

The Land Owns Us

Totem

Swell

Custodian

Mythic Landscape

Ancient Land

Elders

Saltwater

I acknowledge the Traditional Owners of the land, the Bunurong people on which I work. I pay my respects to their Elders, past,  present and emerging. 

WARNING: Aboriginal and Torres Strait Islander viewers are warned that this film may contain images and voices of deceased persons.","Seafar - singular, limits bursting rhythm & sound design transcendence. Wander the streets together, the noise of life muted by sheets of rain, brilliant colours dimmed in a pervading mist of grey. An instrumental amalgam of moody beats and synth drenched noises.",https://drive.google.com/file/d/14kYIU9AeFhtUB332fr99_FwWmf77CdAp/view?usp=sharing,,http://www.seafar.co,music,C3_Wednesday,TBA,,,,
5,Illuminations I: Calibration,Matthew Barnard,,"Sound as excitation of space

a flash through the unlit quiet

earshot illuminations

as shimmering series of spark

The first of a series of works comprising multiple personal binaural room impulse responses (BRIRs) - samples of space in the human spatial register - this piece explores the modulation and dynamic of the spatial image via predominantly abstract materials.

Spaceless, abstract synthesis and feedback is given architectural animation through convolution with the BRIRs, imparting the composer’s peculiar, cumulative directional filtering: an echo of occupied space now intaglio, a nebulous anatomical topography, a spatial mould through which the sonics are extruded and tamed.","
Matt Barnard is a composer primarily interested in the spatial parameter of sound in both binaural and ambisonic domains. He previously studied under Joseph Anderson, and is now a lecturer and researcher at the University of Hull and member of the Hull ElectroAcoustic Resonance Orchestra (HEARO).",5,,https://soundcloud.com/mattt,music,P1_Monday,TBA,https://soundcloud.com/mattt/illuminations-i-calibration,,,
6,"torbuammpa [2019] for flute, clarinet, electric guitar, harp, piano, percussion and recording.","Kirsten Smith - flute, Lindsay Vickery - bass clarinet, Jameson Feakes - electric guitar, Catherine Ashley - harp, Erik Griswold - prepared piano and Vanessa Tomlinson - percussion",https://www.dropbox.com/s/c84q4f8knxt54s0/torbuammpaV1.aif?dl=0,"For this piece, the inauguration speeches of US presidents. Obama (2009) and Trump (2017) were edited into single words (over 4000 of them). and then placed alphabetically. It is ""constraint music"", more  overtly processual than political. Still, it is impossible not to compare the delivery and content of the two speeches. Perhaps most surprising are the number of words used by both presidents: not just ""and"" (82 occurrences) and ""the"" (75) but also ""blood"", ""God"" and ""winter"".  Given that the duration of the two speeches is over 35 minutes, a lot of space was closed up between the words and in a number of places the audio is accelerated to varying degrees.  For the most part the instrumental parts are derived directly from the audio without any attempt at extra-musical commentary. ",This recording was made,https://www.dropbox.com/s/n3m3ru3lej1rv5v/410219616a7877bb080ce42897c99b00.jpg?dl=0,http://lindsayvickery.bandcamp.com/track/t-o-r-b-u-a-m-m-p-a-2019-for-flute-bass-clarinet-electric-guitar-harp-piano-percussion-and-tape,https://www.lindsayvickery.com,music,TBA,TBA,,,,
7,No-Input Mixing Board,Reuben Ingall,https://www.dropbox.com/s/bmlkrb2oskqbqbr/Reuben%20Ingall%20-%20NIMB%20ACMC%202020.mp4?dl=0,"A No-Input Mixing Board (NIMB) is an instrument created by plugging an audio mixer's outputs to its inputs. The internal feedback creates a variety of tones and noises, and the dynamic audio-electrical interactions create potential for an exploratory sound practice.

The sound can be visualised with an oscilloscope (or in my case, oscilloscope software), and by panning multiple channels of mixer feedback in stereo, the sound can be visualised with a phase scope / lissajous figure.

This presentation is a mix of demonstration, how-to, and discussion. ","Reuben Ingall grew up in Canberra, and studied computer music and interactive digital media at the Centre for New Media Arts at The Australian National University. He makes a range of music with puredata patches, no-input mixer, guitar and voice, found sound, field recordings, etc.
His other projects include the monthly experimental music night Soundscapes, the radio program Subsequence, mixing and mastering local bands, and a mashup-DJ act. He has worked on music for installations, theatre, e-publications, film, and dance.",https://www.dropbox.com/s/ndjrl4zh7yywcci/vid%20image.png?dl=0,https://www.youtube.com/watch?v=v7e_s1le5co,http://reubeningall.com/,paper,TBA,TBA,,https://www.youtube.com/watch?v=v7e_s1le5co,,
8,Compute and Resonate. Creating electro acoustic music of the acid genre for presentation within a club environment utilizing accessible Artificial Intelligence and computer based generative tools.,Dylan Davis,https://www.dropbox.com/s/9hfsang9vbj7o9y/Compute_and_Resonate_Dylan_Davis_ACM_presentation.mp4?dl=0,With the development of free accessible Artificial Intelligence (AI) musical compositional tools such as Google’s Magenta Studio it is now possible for musicians with little or no computer programming experience to utilize AI to assist in their compositions of musical works. This paper discusses and evaluates the techniques used in creating electro acoustics works of the acid genre for performance utilizing these tools and the performance of this created work. This research suggests that musicians that can utilize AI as part of their compositional process without the need for in-depth programming knowledge alongside their traditional compositional methods. The tools reported on in this research do not replace compositional techniques rather they can augment the compositional process. ,"Dr Dylan Davis is a design researcher and lecturer at Swinburne University, Melbourne, Australia. He has extensive experience in researching design process, community engagement, digital storytelling, interaction design, audio production and audio composition. Dylan is also a composer and a musician whose practice covers performance, and production of electronic music. His Non-traditional Research Outputs include works for festivals such as Melbourne Music Week, recordings for a range of international record labels and community based music projects. This research utilizes reflective practice to further understand and explore the compositional and performance methods and practices for electronic and electroacoustic music of the techno and acid house genre. Dylan is an extremely prolific musician with over 100 listed works on APPRA/AMCOS. Dylan’s musical works have been released on various record labels in USA, Europe, Asia and Australia.  ",,,https://soundcloud.com/dylabs,paper,daws-live-algo,1,,,,
9,"Vari-Gen: A Generative System for Creating Musical Variations Using Max, Bach, and Cage in Real-Time",David Hirst,,"This talk will demonstrate a new system for generating variations on segments of music that have been previously recorded or composed and saved as MIDI files.

The system builds on work presented at the ACMC 2018 Perth conference which was documented in the paper Hacking Music Notation with Bach and Cage (Hirst, 2018). That paper documented the “Composer’s Little Helper” (CLH) which is a Max patcher that makes use of the “Bach” and “Cage” libraries to manipulate and mix musical notation that has been saved in a “shelf” as separate segments. CLH implements musical operations such as transposition, inversion, retrograde, plus a number of other “treatments” that use the high-level “Cage” library for real-time computer-aided composition.

The Composer’s Little Helper was a non-realtime system to assist in the compositional process. In contrast, Vari-Gen is like the next generation development where a generative system is used to produce note information in realtime. Reading from a MIDI file, variations in pitch, onset time, durations and note velocity are produced and can be varied in realtime using a Markov methodology for each parameter independently.

Using the “bach.roll” representation of the original MIDI file, which is a musical staff with note-heads notated in proportional notation, the composer/performer can select a specific number of notes for treatment, vary the transposition, durations, onset times, velocities, and the order of the Markov chain for each parameter of the variation generator.

Realtime output triggers sound using a VST instrument plugin, which can be varied to suit the musical style. The note output is recorded in another bach.roll object, which can be saved as a MIDI file at any time.

In essence, the system functions as an exotic sequencer/variation generator, and because it all happens in realtime, it could also function as a performance instrument.

The two part work Pierre and Frank was created using a combination of the Vari-Gen software and non-realtime editing (to be screened at ACMC 2020).","David Hirst’s electroacoustic music compositions have been performed in the United States, Canada, the UK, the Netherlands, New Zealand, South Korea and nationally across Australia. He studied computer music at La Trobe University, composition with Jonty Harrison at the University of Birmingham, and completed a PhD in electroacoustic music composition and analysis at the University of Melbourne. Hirst has worked as an academic at the Tasmanian Conservatorium of Music, La Trobe University, and at the University of Melbourne. He is currently Honorary Principal Fellow at the Melbourne Conservatorium of Music, University of Melbourne. His most recent album, “The Shape of Water” is available on iTunes and Spotify.",9,,https://davidhirst.me,paper,synth-nota-inter,1,,,,
10,Pierre and Frank (Parts 1&2) – a Video Music work in two parts,David Hirst,,"Program Notes: Presented in two distinct parts, this work is dedicated to two of the most original composers of the twentieth century: Pierre Boulez and Frank Zappa. Although their life trajectories were quite different, one an orchestral conductor and the other a rock musician, their paths crossed through their compositional activities. Pierre Boulez founded the French research institute IRCAM in 1977 and conducted several tracks on the Zappa album The Perfect Stranger in 1984. It was performed by IRCAM’s Ensemble InterContemporain, with the title track commissioned by Boulez. Zappa was also very active in his use of the Synclavier in his custom-built studio. The Synclavier was an early digital synthesizer, polyphonic digital sampling system, and music workstation manufactured by New England Digital Corporation of Norwich, Vermont. The original design and development of the Synclavier prototype occurred at Dartmouth College with the collaboration of Jon Appleton, Professor of Digital Electronics, Sydney A. Alonso, and Cameron Jones, a software programmer and student. Thus Boulez and Zappa were both highly active in the use of technology in their music.

The work is organized in two separate parts. The first part is dedicated to Pierre Boulez and is subtitled “Pierre”. The second part is dedicated to Frank Zappa and is subtitled “Frank”. Both parts were created using the composer’s variation generation Max patch called Vari-Gen (see separate presentation talk). The technique, in both cases, was to record a keyboard improvisation, edit it and dissect its constituents, and create variations on the segments using the variation generator software. Then each piece was assembled to create the final composition.

Parts 1 & 2 are quite different from each other in style. Part 1 (Pierre) is a more homogenous electronic style work, which is not really an emulation of a Boulez piece, but is a nod to his use of electronics. Part 2 (Frank) uses drums, bass, and a lead instrument in a form that begins with a solid drum back beat, then morphs into a duet between drums (percussion) and lead instruments, followed by a duet between lead and bass, before a final return to solid drum beat, bass line and lead synths. The bass instrument in Part 2 uses a software emulation of a Synclavier by the Arturia company. This is a nod to Zappa’s use of the Synclavier later in his life. Part 2 is more in the phrenetic, relentless Zappa style. The production style is very retro!
","David Hirst’s electroacoustic music compositions have been performed in the United States, Canada, the UK, the Netherlands, New Zealand, South Korea and nationally across Australia. He studied computer music at La Trobe University, composition with Jonty Harrison at the University of Birmingham, and completed a PhD in electroacoustic music composition and analysis at the University of Melbourne. Hirst has worked lectured at the Tasmanian Conservatorium of Music, La Trobe University, and at the University of Melbourne. He is currently Honorary Principal Fellow at the Melbourne Conservatorium of Music, University of Melbourne. His most recent album, “The Shape of Water” is available on iTunes and Spotify.",10,,https://davidhirst.me,music,C2_Tuesday,TBA,https://soundcloud.com/david-hirst-1,,,
14,Qualia,Panayiotis Kokoras,,"Qualia was composed at CEMI studios – Center for Experimental Music and Intermedia at the University of North Texas in 2017. The composition explores the experience of music from perception to sensation; the physical process during which our sensory organs – those involved with sound, tactility, and vision in particular – respond to musically organized sound stimuli. Through this deep connection, sound, space, and audience are all engaged in a multidimensional experience. The motion and the meaning inherited in the sounds are not disconnected from the sounds and are not the reason for the sounds but are, in fact, the sound altogether. Energy, movement, and timbre become one; sound source identification, cause guessing, sound energies, gesture decoding, and extra-musical connotations are not independent of the sound but vital internal components of it. Qualia are claimed to be individual instances of subjective, conscious experience. The way it feels to have mental states such as hearing frequencies at the lower threshold of human hearing or a piercing sound, hearing a Bb note from a ship horn, as well as the granularity of a recorded sound. It is an exploration of time and space, internal and universal. In Qualia, I do not experience musical memory as a sequence of instances but as a sensory wholeness that lasts the entire duration of the piece. The experience of sound itself is not sequential; it bypasses past or future; time becomes a single omnipresent unity. In this state of consciousness, time dissolves. The vibrating air molecules from the speakers, the reflections in the physical space, and the audience are the sound.","Kokoras is an internationally award-winning composer and computer music innovator, and currently an Associate Professor of composition and CEMI director (Center for Experimental Music and Intermedia) at the University of North Texas. Born in Greece, he studied classical guitar and composition in Athens, Greece and York, England; he taught for many years at Aristotle University in Thessaloniki. Kokoras's sound compositions use sound as the only structural unit. His concept of ""holophonic musical texture"" describes his goal that each independent sound (phonos), contributes equally into the synthesis of the total (holos). In both instrumental and electroacoustic writing, his music calls upon a ""virtuosity of sound,"" a hyper-idiomatic writing which emphasizes on the precise production of variable sound possibilities and the correct distinction between one timbre and another to convey the musical ideas and structure of the piece. His compositional output is also informed by musical research in Music Information Retrieval compositional strategies, Extended techniques, Tactile sound, Hyperidiomaticity, Robotics, Sound and Consciousness.",14,,http://www.panayiotiskokoras.com,music,P4_Thursday,TBA,https://soundcloud.com/pkokoras/qualia-tape,,,
15,Laputa: Castle in the Sky – A comparison of Joe Hisaishi’s scores for the film’s Japanese and English versions.,Shally Sharin Pais,https://www.dropbox.com/s/6s858yct0w5ch5v/Submission%2015.mp4?dl=0,"Thirteen years post its release in Japan in 1986 under Studio Ghibli, Hayao Miyazaki’s Laputa: Castle in the Sky was re-released in America under the production of Disney. This paper discusses the differences between the scores of the film’s two versions based on cue-by-cue qualitative analysis and examples from the films to illustrate the contrast. As there is a lack of unbiased investigation on the effects of different scores on the perception of the same film scenes, this case study aims to create a new resource providing an analytical comparison of these two compositions by Joe Hisaishi in order to interpret his compositional decisions. The four devices that played a role in reinventing the score in a Western compositional style to suit the audience’s cultural background include orchestration changes, re-interpreting melodies, the use of Mickey-Mousing and the addition of new cues. Consequently, the paper highlights these factors’ influence on the film’s narrative and aims to widen the discourse on composition for animation film scores.","Shally Pais is an international student at AIM who completed her undergraduate degree in Visual Communication in India. She is currently pursuing her Master’s Degree in Composition and Production, taking forward her Film Studies experience by developing her film scoring practice, specifically dealing with animation. She is working towards coalescing her knowledge and skills in creating audio and visual content through her intermedial projects.",,,https://www.behance.net/shallyshar71c7,paper,musico-comp,1,,,,
16,The Disquiet Of Melting,C. Tsang,,"The Disquiet Of Melting is an audio-visual piece combining field recording with crowd-sourced vocal contributions from the Disquiet Junto, an online community of musicians. The piece is a sonic representation of the anxiety and unease invoked by climate change, particularly in light of the extreme weather events in Australia. The Disquiet Junto is a group created by author and sound artist Marc Weidenbaum as a weekly project where musicians respond to fast-turnaround assignments to compose and share compositions. This particular project (#419 - Dischoir) asked participants to create music from 113 vocal samples of held syllables shared by members of the Disquiet Junto. The field recording was made by freezing a hydrophone into a glass of water, then slowly pouring hot water over the frozen block. The vocal contributions were then layered on top of the field recording in Ableton Live 9. The resulting composition was turned into an audio visualisation using Trapcode Form in Adobe After Effects. Vocal contributions [in order of appearance]: Jet Jaguar Patricia Wolf Cray Samarobryn Atomboyd Precht Zoundsabari Ejkelly BellyFullOfStars Sevenism tja Zero Meaning KRSeward Vonna Wolf","Born in 1982 in Hong Kong, C. Tsang (pronouns: they/them) is a nonbinary audio-visual artist living in Perth, Australia. Their work explores the emotional nature of landscape, and the main focus of their practice has been on their response to the natural landscape as a composer and performer, incorporating audio and visual elements of place into compositions, and using the landscape as a narrative device. C. has performed and exhibited their works in Australia, Asia, UK, Ireland and the USA as samarobryn, and has been nominated multiple times in the WAM Song Of The Year Awards in the experimental category. They were also nominated in the 2019 WAM Awards for Best Experimental Artist. They are currently a PhD candidate at the Western Australian Academy Of Performing Arts (Edith Cowan University).",16,,https://www.fb.me/samarobrynAU,,C2_Tuesday,,,,https://vimeo.com/390380677,
18,Exploring Liveness in Instrumental Practise through DAW Processes,Tom Pierard,asked for extension,TBA,,,,,paper,daws-live-algo,2,,,,
27,The Sky Is The Score #1,Michael Spicer,https://drive.google.com/file/d/1kKrBokuuK5-_MGFSI1SKBkW-pr11QVCh/view?usp=sharing,"The Sky Is The Score #1 is the first in a series of pieces that use photographs of the sky as a graphic score. This stereo fixed media  piece is conceived to be played in a reverberant space, such as a church. It was initially mixed with no artificial reverb, but reverb has been added for playing online. The piece was created by recording several generative patches on a modular synth, and then shaping the resulting layers in a DAW so as to reflect my interpretations of various parts of the sky image. ","Michael Spicer has a PhD Music and a M.Sc in Computer
Science, and is constantly looking for ways to combine these two areas. He
has been performing professionally as a keyboard/synthesizer/flute player
since the late 1970’s. He was a member of the popular Australian folk/rock
group “Redgum” in the 1980’s. He is currently teaching at Singapore Polytechnic and performing in Singapore with the improvisation group “Sonic Escapade”.",https://drive.google.com/file/d/1b3hq_lnk_aLVEXf6IcE_cGJeBVZvne0c/view?usp=sharing,,https://sites.google.com/view/michaelspicerweblinks/home,music,P2_Tuesday,TBA,,,,
28,Atlas of Uncertainty,Massimo Vito Avantaggiato,,"The Audiovisual  ""Atlas of Uncertainty"" is based on the representation of 4 Classical elements, that typically refer to the concepts in Ancient Greece of earth, water, fire, earth and aether, which were proposed to explain the nature and complexity of all matter in terms of simpler substances.

The music that accompany the video is a sonic continuum ranging from unaltered natural sounds to entirely new sounds - or, more poetically -- from the real world to the realm of the imagination.

In “Atlas of Uncertainty” a microcosm of sounds, explored through some and max msp interfaces, becomes the hyletic universe of the work.

Heterogeneous sound materials are explored through various techniques (granular, subtractive). The sounds and the images are here combined in well identifiable gestures.
","His work revolves around research processes and combination of experimental video and experimental electronic music. He took a master degree in Electroacoustic Composition, Composition, Sound Engineering. He has won several prizes for his works in international composition competitions with concerts and academic presentations in over 90 countries.",28,,https://vimeo.com/user22709645,music,C1_Monday,TBA,,,https://vimeo.com/264567646,
29,Chromaticity,Vicki Hallett|Jem Savage|Ferne Millen,https://drive.google.com/file/d/1AaZQys0rkXcxhoJJj_dgz6YJEQii9bLE/view?usp=sharing,"Chromaticity is an installation and performance featuring photographic images, live clarinet improvisation and a generative soundscape incorporating instrumental samples and environmental field recordings from the You Yangs, Victoria, Australia. The images, live improvisation and recordings form a visual and aural installation to create an emotional and environmental response to the landscape and a sense of connection with the essence of the You Yangs.

Field recordings of the You Yang environment inform the “sonic postcard” nature of Chromaticity’s generative soundscape. Features such as the drone of a major highway, trains, a working quarry, children playing, bush-walking and mountain-bike riding area and other anthropogenic sounds are retained to reveal the current day sonic space.

The three images of the You Yangs, taken at dawn, meridian and dusk, reveal a dynamic and vivid spectrum of landscape colours. Each image informs a 5-minute movement of the work. A fourth image is projected directly onto the performer - in a sense, immersing them in the landscape.

For each image, parameters were assigned for sample and note duration, mode, tempo, sound density and pitch. The electromagnetic frequencies of the light spectrum were related to frequencies of sound waves and the saturation of colour determined the intensity of the sounds. Images were mapped and assigned notes with sounds of interest within the field recordings inspiring small motifs and cells to be sampled - performed on a range of instruments (Bass Clarinet, Taegum, Flute and small percussion instruments).

A Max/MSP patch controls playback of field recordings associated with the time of day of each movement and provides a mechanism by which the sampled motifs and cells are triggered to form a harmonic backdrop for the clarinet improvisation. The contours of the landscape, as captured by Millen, are mapped to derive data input for control of parameters within the Max patch, and to suggest target notes for the clarinetist's improvisation in a small window within the patch. Thus, the patch containing this window, along with the images and mapping of their contours becomes a supplementary score for the performer to follow during live performance (along with the composer’s notes describing tonality and motifs of each section).

Premiere:

The installation was premiered at the 4-hour Geelong After Dark pop-up arts festival held on Friday the 5th of May 2017 between 6 pm and 10 pm by the City of Greater Geelong. Approximately 250 people engaged with the installation during the 4-hour event. There were numerous comments from the public wishing to reconnect and visit a place they hadn't visited in many years or decades, even though they see it on their daily horizon.

The event was free, and participation and interaction with the artists was encouraged. The audience - young, elderly, dancing in the space, pretending to be trees, absorbed in conversation with artists and other participants, asking questions about music composition, the You Yangs, noise, “ear-cleaning”, noise pollution, generative computer programming, anthropogenic sounds and more - helped the implications of the Chromaticity installation crystallise on the night: increased public awareness of You Yangs, environment and the effects of noise pollution; public interaction with artists and their art, public conversations about the creative process be it visual art, photography, music or composition; sparking new interest in contemporary art music, computer programming, improvisation and live performance.","**Vicki Hallett** is a composer, musician and sound artist who graduated from the Victorian College of the Arts and the University of Melbourne. She has composed, produced and performed in live concerts, solo recordings ranging from chamber music to sound art and acoustic ecology. Through a unique approach combining acoustic ecology, scientific analysis and innovative performance practices, Hallett reshapes the role of interdisciplinary research. This exploration has led her to develop a collaborative concept with Cornell University's Elephant Listening Project.  In 2017, Hallett attended the international residency, Sonic Mmabolela, where she performed on Mabolel Rock with a pod of Hippopotami. 
Website: <https://www.vickihallett.com> 

**Ferne Millen** attended The Victorian College of Arts (Melbourne University) completing a Bachelor in Visual & Performing arts(major. photography & theatre). Millen was a finalist in the 2015 National Photographic Portrait Prize for her photograph of “Who’s that lady?”, has twice been a finalist in the National Photographic Portrait Prize as well as a dual finalist in the 2014 Moran Prize in Sydney. Her clients including Deakin University, Porsche, Citi Power, City of Melbourne, Tennis Australia, Geelong Arts Centre, Universal Music and many individuals, musicians and artists.

**Jem Savage** is a musician and sound artist for improvised and new music. His performances effortlessly blend live processing, looping and interactive visuals with highly developed instrumental techniques-often leveraging proprietary software and hardware devises including iPSi, the Isomorphic Pitch-Shifting Interface. Savage has performed or collaborated with a unique cross-section of improvisers, experimental musicians and composers including the AAO, Gian Slater’s Invenio and Barney McAll. Savage is currently a PhD candidate within the Faculty of VCA and MCM at the University of Melbourne.",29,,http://www.vickihallett.com/,music,C3_Wednesday,TBA,,https://youtu.be/b4SJwhJBb4c  ,,
30,Making the Enabled System,Michael Spicer,https://drive.google.com/file/d/1Hd_4e9Xr283ItNWQUsMLmPChcg8PnOvc/view?usp=sharing,"The Enabled System is designed to provide a way for small groups of untrained people, or people with disabilities, to engage in group musical improvisations through a computer-mediated performance environment. The system consists of a variety of physical controllers that send performance information to some computer software which transforms the inputs to produce control information for a collection of synthesisers. ","Michael Spicer has a PhD Music  and a M.Sc in ComputerScience, and is constantly looking for ways to combine these two areas. He has been performing professionally as a keyboard/synthesizer/flute player since the late 1970’s. He was a member of the popular Australian folk/rock group “Redgum” in the 1980’s. He is currently teaching at Singapore Polytechnic and performing in Singapore with the improvisation group “Sonic Escapade”.",https://drive.google.com/file/d/1JzbR9M11phzct0IXSRamwxNAb2i8at79/view?usp=sharing,https://youtu.be/Fd0yvJpb7QY,https://sites.google.com/view/michaelspicerweblinks/home,paper,synth-nota-inter,2,,https://youtu.be/Fd0yvJpb7QY,,
32,Scrapes and Sighs,Alexis Weaver,https://cloudstor.aarnet.edu.au/plus/s/R05gG1XabT2Catw,"Scrapes and Sighs (2019) makes use of common kitchen items such as metal fruit bowls, oven  trays  and  porcelain  cups.  Struck  in  the  manner  of  a  singing  bowl,  these everyday vessels produce pure harmonics which are teased out into strings of sound. The work progresses from its initial meditative section into a much busier world of gaudy, processed sound. These shards pivot and ricochet around the sound space in broken envelopes before settling back into a calmer mood.

Scrapes and Sighs was first composed for Multiple Monophonies, an installation featuring a multichannel monophonic spatial system. As such, the work was initially composed for twelve discrete monophonic channels, but here exists as a stereo work optimised for headphones.","Alexis Weaver is an electroacoustic composer based in Sydney, Australia. Alexis draws on field recordings of animals, insects and everyday objects to create whimsical, adventurous radiophonic and acousmatic works. While her principal interest lies in composing fixed-media acousmatic music, she has also composed and collaborated on soundtracks for  animation, short film, radio, theatre, and dance. Alexis’ work has been broadcast in Australia, France and Scotland, as well as featured on New Weird Australia’s Solitary Wave (In) (2019) and RMN Classical’s Electroacoustic and Beyond II (2017). While studying a Bachelor of Composition at the Sydney Conservatorium of Music, Alexis was awarded People’s Choice Award and First Place in the 2015 and 2016 University of Sydney Verge Awards respectively for her acousmatic works. In January 2018, Alexis was awarded the National Council of Women’s Australia Day Prize for her research undertaken during her Honours year on the visibility and practice of female electroacoustic composers. She is currently a Master of Music candidate at the Sydney Conservatorium, where she also teaches composition. Her research has focused on the transferral of high-quality acousmatic music to everyday, portable diffusion systems – naming this new, inclusive audio movement Small Diffusion. Alexis is co-founder of composer collective lost+sound, who in 2018 launched a concert series celebrating emerging experimental artists.",https://drive.google.com/file/d/12eBf146ml4cZ6heTW30xoIKNSW0F1VOd/view?usp=sharing,https://alexismarieweaver.bandcamp.com/album/scrapes-and-sighs,http://www.alexismarieweaver.com,music,P1_Monday,TBA,,,,https://alexismarieweaver.bandcamp.com/album/scrapes-and-sighs
33,The Shimmering Haze,Alexis Weaver,https://cloudstor.aarnet.edu.au/plus/s/fiQZIHWT5vBGR71,"The Shimmering Haze (2019) explores dichotomies of aural perspective, texture and sound source. The piece alludes to our human-induced plastic crisis through the exploration of small,  highly  active  sound  objects (which  I  name 'micro' sounds) embedded  within  vast, heavily  textured  sonic  backgrounds (which  I  have  termed 'macro' sounds). Comprised of a mix of synthesised sound and close recordings of sinks, kettles and coffee machines, The Shimmering Haze alludes to our man-made plastic crisis and conjures a sonic metaphor for shiny plastic glinting through hazy water. At times menacing, cloying, The Shimmering Haze also exhibits brief moments of linear simplicity – providing a welcome textural respite. 

This work was composed as part of a larger portfolio for my Master of Music degree. With its highly detailed spatialisation and sound object placement, the piece is best listened to with quality headphones. While there also exists a stereo speaker-optimised mix, I have provided the headphone-optimised mix for the ACMC. ","Alexis Weaver is an electroacoustic composer based in Sydney, Australia. Alexis draws on field recordings of animals, insects and everyday objects to create whimsical, adventurous radiophonic and acousmatic works. While her principal interest lies in composing fixed-media acousmatic music, she has also composed and collaborated on soundtracks for  animation, short film, radio, theatre, and dance. Alexis’ work has been broadcast in Australia, France and Scotland, as well as featured on New Weird Australia’s Solitary Wave (In) (2019) and RMN Classical’s Electroacoustic and Beyond II (2017). While studying a Bachelor of Composition at the Sydney Conservatorium of Music, Alexis was awarded People’s Choice Award and First Place in the 2015 and 2016 University of Sydney Verge Awards respectively for her acousmatic works. In January 2018, Alexis was awarded the National Council of Women’s Australia Day Prize for her research undertaken during her Honours year on the visibility and practice of female electroacoustic composers. She is currently a Master of Music candidate at the Sydney Conservatorium, where she also teaches composition. Her research has focused on the transferral of high-quality acousmatic music to everyday, portable diffusion systems – naming this new, inclusive audio movement Small Diffusion. Alexis is co-founder of composer collective lost+sound, who in 2018 launched a concert series celebrating emerging experimental artists.",https://drive.google.com/file/d/12eBf146ml4cZ6heTW30xoIKNSW0F1VOd/view?usp=sharing,,http://www.alexismarieweaver.com,music,P3_Wednesday,TBA,,,,https://alexismarieweaver.bandcamp.com/album/the-shimmering-haze
34,Encroaching,Elsa Jaeyoung Park,https://soundcloud.com/elsa-park/encroaching,"“What gets us into trouble is not what we don’t know. It’s what we know for sure that just ain’t so”-Mark Twain. ‘Encroaching’ is a piece that deals with an experience of unpleasant truth - human’s attitude to nature, consisting of sonic environment and marine data of four seasons of Haeundae Beach, Busan in 2019. Inspired by the beauty of the sea, the piece is firstly created from the recording of Haeundae beach last year. Such as sounds of waves, people amused and the motorbikes on the road were captured. After then, a film about climate change encouraged me to look back on the moment I simply enjoyed the sea while not knowing the actual condition of the environment. Thus, I used oceanographic observation data of the beach from April to December in 2019 in the piece, then such numerical data were translated into musical data through Supercollider. During the process, I was in the position of experiencer and interpreter as to the collage of what computer-generated marine data and raw sound of the sea deliver. Also, the varying gradations of swaying gestures in the piece manifest composer’s changing experiences towards nature from a feeling of awe to the awareness of problems and become distant from the issues at some point. Therefore, ‘Encroaching’ is to invite listeners to explore and share demeanours to nature, acknowledging people’s connections and gaps to the environment. The piece was presented on Telematic Festival Earth Day Art Model 2020.","Elsa Jaeyoung Park is a composer based in South Korea. She studied electroacoustic music at the University of Birmingham and Jazz Piano at Seoul Institute of the Arts. Her main interests focus on exploring uncomfortable or ignored emotions primarily in the composition of electroacoustic music and data sonification. Park’s music has been heard in various locations including the UK, Germany, Australia, Korea, and USA. Her music has also been featured in international events such as International Computer Music Conference (ICMC), Seoul International Computer Music Festival (SICMF) and film festivals. Park won the best piece of ICMA 2018 regional award Asia-Oceanic at ICMC.",https://drive.google.com/file/d/1zLnS8_WiOFnVa7EQbCf4z57kE_BaKy4z/view?usp=sharing,https://soundcloud.com/elsa-park/encroaching,http://www.elsapjy.com,,TBA,TBA,https://soundcloud.com/elsa-park/encroaching,,,
35,Unawakened Routine of a Salaryman,Minchang Han,"somehow didn't know how to submit, in contact now.",TBA,,,,,music,P3_Wednesday,TBA,,,,
38,Dorfl sings,"Composer and conductor: Abigail Thomas; Performers: Lynden Bassett, Weitong Huang, Jaime Langner, Miles Mclaughlin",https://www.dropbox.com/s/i33264a2puhkc66/Dorflsings.mp4?dl=0,"In Terry Pratchett’s Feet of Clay, Dorfl is a golem: a living machine who, since his mouth is sealed shut, cannot speak. Towards the end of the book, though, he is bought, freed, and given a voice. 

Interested in combining computer music with my experience as a chorister, I’ve created an instrument which aims to do the same. It listens to the sound of someone singing, and deconstructs their voice into a series of harmonics. By randomly piecing these harmonics back together, Dorfl mimics the voice it heard, but adds an eerie quality of its own. To maintain the similarities between Dorfl and a human voice, the instrument slides randomly between notes, which are tuned to a Pythagorean scale.

As well as demonstrating the sound of a digital voice, Dorfl sings is an exploration of freedom in improvised music. The performers improvise throughout, constrained only by my whims as conductor. By conveying instructions with a small set of gestures, I can restrict the mode from which they can play, the settings of their instruments, and how melodically or atmospherically they should play. Although I never have complete control over how the choristers play, these broad instructions are enough to influence the overall sound and trajectory of the piece. This system was inspired by the political landscape Dorfl encounters: the local benevolent dictator rules in much the same way as I conduct.","Abigail is a computer science student at ANU, where she participated in the Laptop Ensemble (LENS) program last year. She has returned to the ensemble as a tutor in 2020. ",,,,music,C2_Tuesday,TBA,,,,
39,Acid Dub - Composing Real-Time Electronic Dance Music: How studio and performance based- practices combine to create Acid,David Haberfeld,https://drive.google.com/file/d/1xtJT3_oVme8bI6SpQYiWQkr2XyEoKEl3/view?usp=sharing,"The live performance explores the iconic TB-303 Bassline Synthesizer through time and space with an all hardware live electronic dance music (EDM) performance. The performance will be a live improvisation with the TB-303 as the focused sound. Presenting real time sonic examples born from the unique TB-303 sound and demonstrate its significant contribution to the sonic development of EDM since Acid House. 

Roland’s TB-303 Bassline synthesizer was manufactured as an electronic bass accompaniment machine. The TB-303 was conceived from the same idea as the drum machine. Although proving more difficult to program than a drum machine, the 303 did not resemble the bass guitar sound that guitarists were seeking at that time, ultimately leading to the initial in the early 1980s.

From then DJs picked up the inexpensive and discontinued 303. The short repeating sequences of the 303 and its peculiar sound became appealing to the burgeoning producer. Further manipulating the limited real-time controls they were able to produce a squelching bubbling bass timbre that engaged dance floors and the sonic meaning of Acid House was born. Music technology companies have since continued to produce versions that emulate or clone the 303 in various forms. As much as Rock ’n Roll owes its existence to the electric guitar, EDM found its electric guitar in the form of the TB-303. 

This particular performance extends the stylistic parameters of the genre of Acid through a Dub Music approach by slowing the tempo down and making use of spatial audio effects such as delays and reverbs to create further density . The performance makes use of 3x 303, consisting of an original TB-303, a modified TB-303 known as the Devilfish and a recent clone the TB-03. The iconic TR-808 drum machine provides the minimal drum pattern. This performance was recorded in a single take with no predetermined arrangement or form, all composed in real-time with no post production.","David Haberfeld is an electronic dance music artist, producer, composer, performer, DJ, academic and educator, since the early 1990s. Best known for his productions and live performances under the artist moniker Honeysmack. In 1999, he was an Australian Record Industry Association (ARIA) finalist nominee for Best Dance Music Release for ""Walk on Acid""—which sampled Burt Bacharach's ""Walk on By"" earning David a co-writing credit with the Grammy and Academy awarded songwriter. His work as an energetic and colourful live electronic act has earned him a rare respect on the Australian live rock circuit, performing live electronica at festivals nationally and abroad.

His dynamic performances and productions are purely hardware based and centre on Roland’s iconic machines of the 1980s including the TB-303, TR-909 and TR-808. These machines were pivotal in the development of electronic dance music, and David continues to push stylistic parameters and explore new contexts with these vintage machines through his research in composing real-time electronic dance music. His work strongly features modular synthesis and enjoys the creative possibilities each new configuration and performance can sonically provide. As an accomplished artist and academic he embodies a diverse mix of experiences and continues to explore and challenge new thinking around music, sound, media arts and creativity more broadly. David is in the final stages of his PhD in music composition at Monash University. ",https://drive.google.com/file/d/1f820fVEy9q1FrBcIcWG4q2rMJH_UGAlA/view?usp=sharing,https://youtu.be/D6fVwILV8aw ,https://www.davidhaberfeld.com/,music,C3_Wednesday,TBA,,https://youtu.be/D6fVwILV8aw ,,
40,The Decibel Scoreplayer as a portable medium for spatial music performance.,Lindsay Vickery and Stuart James,?,TBA,,,,,paper,spatial-perf,1,,,,
41,Diantara,Fahmi Mursyid,https://www.dropbox.com/s/lgxyq6nnd9lu7ft/Fahmi%20Mursyid%20-%20Diantara%20%28conference%20podcast%20version%2C%202020%29.wav?dl=0,"Between of composition and improvisation (computer generative sounds and algorithmic synthesizer). Blending of ambient, drone, everyday sound / soundscape, and West Java music (gamelan, bonang, saron, and sundanese flute).

Computer-based digital signal processing for manipulation the sound materials in micro sampling, granular synthesis, change the pitch/octave, reverse, play at different speeds, and create droning sounds (less effect / minimalist approach). Inspired by nature, organic structure, climate change, and landscape.","Fahmi Mursyid is a contemporary musician, composer, sound designer and producer based in Bandung, Indonesia. He began releasing recordings under various monikers and labels since 2011 until now. Fahmi uses found objects and computers to create the glitches, sampling, granular synthesis and electronic sound characterising in his work.",41,,https://www.patreon.com/ideologikal,music,P2_Tuesday,TBA,,,,https://ideologikal.bandcamp.com
42,Women of Music Production Perth,Elise Reitze-Swensen,extension,TBA, ,,,,paper,cult-comm-eco,4,,,,
43,Musica ex machina: integrating the sonic pallet of machines with acoustic instruments.,Lindsay Vickery,switch in with older paper (fine with me),TBA,,,,,paper,musico-comp,2,,,,
44,Torrents 1.5,Ben Harb,https://drive.google.com/drive/folders/191C3y0RvT6tzPkHs2dJDk4jMaWCSnym-?usp=sharing,"Torrents was an idea first formed in 2017 while working as an artist in residence in Sydney, Australia. The idea was to create an interactive performance presented through a quadrophonic speaker array. Torrents 1.5 is a development on this original foundation.

Originally, the work was presented as a live performance that then gave way to audience interaction. Torrents 1.5 takes this idea and presents a format for an interactive sound art installation. Torrents 1.5 is intended to be an installation that is interacted with through a midi controller. The installation is based around a PureData patch that allows an audience member to interact with the sounds that are coming out through each individual speaker. The audience member can interact with the music of the installation and create a soundscape that has the potential to be completely unique to that of the next person. Torrents 1.5 is a development of an idea but not it’s final intended state. This installation is planned for a multi-speaker array, four or above. 1.5 is limited to the stereo field for the purpose of this presentation.

The soundscape that makes up Torrents 1.5 is a combination of originally composed music and field recordings that were captured during a one-month residency in Itoshima, Japan. The piece you hear as part of Torrents 1.5 is a section for the work Dual Location, which paired field recordings from the rural area of Itoshima with recordings of Fukuoka city. These recordings inform the aesthetic direction of the work while building the foundations of the soundscape. Presenting Dual Locations through the medium of Torrents 1.5 allows me to explore the composition in a new light, further developing the existing material.

Conceptually, the work is based on a loose interpretation of the way digital files are shared through peer-to-peer sharing. When a torrent is downloaded through a peer-to-peer service the file is created by taking information from multiple sources and combining them to create a complete whole. Following this idea, Torrents 1.5 is using multiple sounds that are then being dispersed through a speaker array (however large it may be) that then creates the sum of the work. Each speaker acts as a ‘seed’ within the piece and as the user interacts with the interface, they are able to develop the sum of the work – in a sense mimicking the role of the downloader.

The purpose of Torrents 1.5 is to develop new and novel ways of approaching sound-art installation and exploring the possibilities that programs such as PureData allow within the medium. Another aim of the project is to present new possibilities for audience participation with an installation. This could lead to further develop of the project or spawn ideas for new projects that follow the same or similar template; for example, combining the work with live performance.",,,,https://www.benharbcomposition.com/,,TBA,TBA,,,,
46,Multiple Monophony and the Multichannel Monophonic Compositional Framework,Jesse Austin-Stewart|Bridget Johnson,https://drive.google.com/file/d/1s82qpxXfyvSsn8nFdMNW96TdaPYlrVn8/view?usp=sharing,"Within the field of spatial audio exists a variety of barriers to participation and engagement. These issues range from financial accessibility issues (where equipment can cost thousands of dollars) to class issues (where many are excluded because of the cultures surrounding the music and the spaces in which it occurs). There are also barriers to engagement erected through the types of spatial systems constructed, the listening environment that is encouraged, and the type of music that people are encouraged to compose. The fundamentality of the ‘sweet spot’ within much spatial music automatically excludes those outside of the sweet spot from most likely experiencing the composer’s intentions.

This paper looks at the creation of a new compositional framework that is intended for use in facilitating non-sweet spot oriented multichannel works. It discusses the construction and utilisation of the framework, listener and composer responses from a test case of framework, while also touching on future work that has been inspired by these investigations.
","Jesse Austin-Stewart is a Te Whanganui-a-Tara-based sonic artist with a focus on spatial sound. He is currently working on his PhD at Massey University researching barriers of engagement within spatial audio. 

Bridget Johnson creates immersive sound installations and performances that heighten the audiences experience with spatial audio. Her work focuses on exploring the way sound can move through space and developing new interfaces to allow composers and performers to further explore expressivity through real time spatialisation in their work. Her installations explore these themes in combination with site-specificity and abstraction of time.
",46,,https://www.jesseaustinstewart.com/,paper,spatial-perf,2,,https://youtu.be/op0PMnH6SHc,,
48,鬼哭 Wailing Ghosts,Li Tao ,https://www.dropbox.com/s/6mcohv31yo8l7kc/Li%20Tao%20-%20Wailing%20Ghosts%20video.mp4?dl=0,"Taken from a story by the same title in 聊斋志异 Strange Tales from a Chinese Studio (1740) published in Qing dynasty China by 蒲松龄 Pu Songling (1640 ~ 1715), 鬼哭 Wailing Ghosts explores different live processing aspects of voice and non-pitched instruments in addition to acoustic and theatric performance. The piece employs text-painting through live and synthetic timbres to portray a narrative heard in Mandarin and Sanskrit such that even those unfamiliar with these natural languages can comprehend. The text used is mostly from the above tale with an added Buddhist mantra of Ksitigarbha at the end.

To briefly summarize the tale: at the time of the Xie Qian troubles, the residences of the nobility were all commandeered by the rebels including the residence of Commissioner Wang. When the government eventually retook the town, every porch was strewn with corpses and blood flowed from every doorway. Since then, Wang frequently saw ghosts in the day and night, hearing the ghosts wailing in various corners of the house. He eventually ordered a lengthy ritual performed to depart the wandering souls. Ever since, the hauntings ceased.

I took a new approach in this piece when dealing with the text. Rather than setting the text in a traditional way, instead I act as narrator to present the text directly. Since Chinese is a melodic language, through the dramatic reading, the nuance of the melodic contour of the text can be easily perceived. The live-processed electronic sounds further amplify the character of the language. The Chinese opera gong used in this piece achieves multiple layers of functionality: from the timbral perspective, striking with mallet and palm brings out different levels of complex sounds; when used with singing, the gong acts as a filter giving the sound a special effect; from the theatric perspective, the gong acts as a mask, separating the narrator and the characters of the story.","Li Tao is a composer and pianist from China. While Chinese traditional culture profoundly influences her, years of living in the U.S., culminating in the receipt of her Ph.D. in Music Composition from the University of Oregon in 2020, have formed her distinct multicultural musical language. Her primary interests include acoustic and electroacoustic composition, performance, and theoretical analysis of compositional techniques and aesthetics. She maintains a deep interest in the inner connections between composer, performer, and instrument. As an interdisciplinary performer, Tao is actively performing both classical and contemporary, acoustic and electroacoustic music in concerts and music festivals. Tao’s music has been performed by numerous musicians and ensembles across Asia, Europe, North America, and Australia. ",https://www.dropbox.com/s/5qimla2gky1t6vs/LI%20WG%20image.png?dl=0,https://youtu.be/5hLfEN0BA9o,https://taolimusic.com,music,C1_Monday,TBA,,https://youtu.be/5hLfEN0BA9o,,
50,妃子笑 Smile of Yang,Yue Pan,emailed,"妃子笑 (Smile of Yang) a real-time interactive performance for Wacom and Kyma. 

妃子笑 (Smile of Yang) is inspired by the Qing Palace Quatrain by Du Mu of the Tang Dynasty and Chinese traditional Cantonese Opera Lychee Ode. One phrase used in the composition is “一骑红尘妃子笑 (Yi Ji Hong Chen Fei Zi Xiao)”. 妃子 refers to Imperial Noble Consort Yang; she is the love of Emperor Tang Xuanzong of the Tang Dynasty. The backstory of this phrase is that Yang loves the lychee of Lingnan, but Lingnan is very far from the Palace, and the lychee is challenging to keep fresh during transportation. The Emperor to let Yang eat the fresh lychee, ordered officers and soldiers to use horses to travel overnight to transport lychee to the palace. Therefore, many horses died in transport. When Yang saw the fresh lychee, she smiled. She did not think of the officers, soldiers, and horses. Because of Yang, Emperor lost the loyalty of officials and the army. Finally, under the pressure of the military, he had to hang Yang. “Yi Ji Hong Chen Fei Zi Xiao” is also the lyrics of the Lychee Ode in Cantonese Opera. Most of the sounds in this composition are human voices: I use Cantonese to say, “Yi Ji Hong Chen Fei Zi Xiao.”","Yue Pan was born in China. She is an intermedia composer and performer, and she enjoys sound design. Her compositions have been performed internationally, including National Student Electronic Music Event (2020), Turn Up multimedia Festival (2020), and New York City Electroacoustic Music Festival (2020). She is currently pursuing her MM in Intermedia Music Technology from the University of Oregon.",,https://youtu.be/PQORilh9RSE,,music,C4_Thursday,TBA,,https://youtu.be/PQORilh9RSE,,
52,Composing Real-Time Electronic Dance Music: an improvisation approach to composing Acid Techno,David Haberfeld,"emailed, try to get him to convert to 10 min artist talk",TBA,,,,,workshop,TBA,TBA,,,,
53,Polvere nera,Nicola Fumo Frattegiani,https://drive.google.com/file/d/1O6Co-klqDncBIT-EjTYS0rUat74OiQY-/view?usp=sharing,"The totality of the acoustic material.

Excavated, mutilated. Sublimated and deposited. The One that is fragmented and reduced to dust. Chalk blocks engraved and carved through the space and elasticity of time. Polvere nera is divided into four sections, bounded by sudden stops and static poses, in which there is an incessant dialogue between two opposing formal poles: bands and points. In the end the dialogue becomes union through a process of massification of the material that does not however cancel the intrinsic differences of the models employed. Polvere nera was constructed using noise, synthesis sounds and percussive sounds.
","Born in Perugia, Nicola Fumo Frattegiani graduated with highest honours from D.A.M.S. (Academy of Arts, Music and Show) at the University of Bologna, with a thesis on Luigi Nono’s work “Intolleranza 1960”.
Later he has advanced post-graduate degree on “The musical cultures of 1900’s” at the University of Rome “Tor Vergata”, a bachelor’s degree cum laude on “Electronic Music and New Technologies” (course electroacoustic composition) at the “Francesco Morlacchi” Conservatory of Music of Perugia and a Master’s degree cum laude and special mention for artistic merit on “Electronic Music and New Technologies” at the “Licinio Refice” Conservatory of Music of Frosinone (course digital audiovisual composition).
His works have been presented at various national and international festivals including ICMC (South Korea), NYCEMF, ICMC-NYCEMF, New Music Miami Festival ISCM, Electroacoustic Barn Dance, WSU ElectroAcousticMiniFest (USA), SMC (Cyprus), Atemporánea Festival, Foundation Destellos (Argentine), Festival Futura, Finale Prix Russolo (France), Synchresis Festival (Spain), Evimus (Germany), MUSLAB (Brazil), Echofluxx (Czech Republic), Audio Mostly, BFE/RMA Research Students' Conference, Convergence, Noisefloor Festival, SOUND/IMAGE Exploring Sonic and Audio-Visual Practice (United Kingdom), WOCMAT (Taiwan), Matera Intermedia Festival, Diffrazioni Firenze Multimedia Festival, XXII CIM Colloquium of Musical Informatics, Venice Biennale of Architecture, Soundscape of Work and of Play 9th International FKL symposium on soundscape, Moon in June, Macro Asilo Museum of contemporary art, Corsie Festival, Segnali Audio-visual arts and performance, Premio Nazionale delle Arti, Elettronicamente Beyond the Borders (Italy).
Author and performer, his research deals with electroacoustic music, sound for images, video, art exhibition and in particular electroacoustic compositions for contemporary theatrical performance. Nicola collaborates with many artists and performers in several productions of live electroacoustic music, with whom he experimented many types of generation and manipulation of sound dimension. He also collaborates regularly with various recording studios and video production studios as a sound designer, sound engineer and re-recording mixer.
",53,,https://www.nicolafumofrattegiani.com/,music,P1_Monday,TBA,https://soundcloud.com/nicola-fumo-frattegiani/polvere-nera,,,
56,Magnetic Stripes,Matthew Davis (Nervous Plaything),https://www.dropbox.com/s/wdnxukrfa673enx/MagneticStripes_Final.mp4?dl=0,"Magnetic Stripes is an audio-visual performance that highlights the potential of algorithmic generation, finding a middle ground between human performance and machine creation. This is achieved through multiple stochastic algorithms controlling various aspects of a performance, with curated specific states created by the artist. By giving control over some aspects of the performance to algorithmic generation, space is made for the artist to have clarity and focus on other musical elements. Magnetic Stripes simultaneously shows the subtlety of human performance and its effect on the experience, and the raw efficiency of machine generation. The generative elements of Magnetic Stripes are made obvious through musical and visual choices, yet are subtle in their control of the performance. All sound, visuals and controls are made with Cycling74’s Max 8 software.

Three probability-based algorithms have been designed for their distinctive effect on the respective musical elements they control. The algorithms are stochastic, but have been created with a rules-based mentality. They are a set of rules that manipulate percentage chances of upcoming musical choices that are inspired by the compositional work of John Cage and Brian Eno. They ensure no certainty in the performance, but possible controlled curves for the artist to react to.

The performance is a powerful, generative drone exploration. Eight channels of audio fuse together to create subtle ambience and moving sonic sweeps, in this contemplative 10-minute experience. They are six channels of simply affected wave table oscillators, and two noise generators. The resulting serene sound draws inspiration from musicians such as Todd Anderson-Kunert, Hobo Cubes and Ryoji Ikeda. The tones wash over the audience through their simplicity and medium movement. The changes in texture create moving sections and crescendos. Four of the oscillator drones (two triangle waves, one sawtooth and a square wave), feature algorithms that control their pitch, timbre and rhythm changes.

A permutation probability algorithm generates pitch through curated percentages of likelihood. The proceeding notes are based on the preceding choices. For the square and sawtooth oscillators, this is a list of eight potential frequencies each, that are generated by the preceding three choices (starting with a random selection). The pitch choices are across a wide register for depth and variety. The two triangle oscillators have four potential choices, based on the preceding four selections (starting with a random selection). These frequencies are at the lower middle range to create a sense of warm
undercurrents. The probability percentage lists are individually curated by the artist. The movement between them and the choices are generated by the machine. All of these drones have a second set multiplier that can be individually volume controlled by the artist, creating a possible thicker texture and harmony when necessary.

The timbre of each generative oscillator is manipulated by an algorithm
that prioritises selections based on how often they were made prior. The more a choice is made, the more likely it comes up in the future. Each drone has four potential shifts in the partials of the sound, that all start with equal chance of occurring, but the probabilities move and change, as the performance progresses (as with anything chance-based). This produces a sense of consistency over smaller blocks of time in the work, that the performer reacts to. The result is a feedback loop between performer and machine, that is different every time.

The rhythms of the drones are each controlled by curated, individual percentage-based probability algorithms. One for the length of each of the drones (four lists of percentages with four choices) and one for the space between each drone (four lists of percentages with four choices). These are set selections of percentages because Magnetic Stripes is meant to be deliberate, ambient and thought provoking.

Two additional sine wave channels have a constant pitch and no pulse. They create unity to the variety of constantly moving other drones. The sines have a second multiplier that can be controlled by the artist to add further texture. A white noise generator and a clicking-machine like texture (that is a blend of three separate beds); accompany the drones, to add body, where needed in the performance.

The aspects of Magnetic Stripes that the artist controls in real time are volume of each channel, the mix of each drone’s harmonies, each drone’s ADSR envelope, texture control (if a drone is on or off), and the panning between stereo outputs for each channel. These are musical elements that the artist determined should not be left up to chance, as they can create tension, build and shape to the performance.

The visuals are constantly changing, brightly coloured shapes, contrasted against a black background and space. They are generative and connected to the four algorithmic channels of audio. The sawtooth channel controls circular, radiating waves; the square drone controls grid-like rectangles with circular corners; the first triangle wave pushes and pulls multiple squares; and the second triangle channel controls haunting white bars and a simple red waveform. The audio of these channels dictates whether it’s on screen and its volume controls their size. The artist’s control of channel panning, determines the x axis starting point of each visual. The timbre changes determine if the visuals have a pixilation effect applied. This creates a sense of unity and synchronicity in the performance between the audio and the visuals.

The simplicity of the audio channels and the clear visual cues are used to highlight the algorithmic aspects to Magnetic Stripes and the performative controls of the artist. Gestural performative cues are created through audio-visual interaction. The piece sits somewhere between a curated experience, generative work and live performance. Algorithmic art usually either sits in an area of certain control of outcome or created with random at its core. However, in Magnetic Stripes, organisation and prioritisation of certain choices is balanced with chance to create a meaningful experience.","Nervous Plaything is an experimental composer, focusing on a textural experience using heavily effected guitars, synthesizers and artist developed technologies. These self-developed technologies blur the line between artist and machine; analog and digital, and allow for a more timbral compositional focus. Since 2011, he has performed at a selection of Brisbane venues, as well as releasing recordings on Bandcamp. More recently, he has been working on Live Takes: a weekly, improvised live audio-visual collection released on Vimeo. This has been performed in the Precursor experimental art series, and envisioned as Now-What at Backbone Youth Arts’ Future30 festival. ",56,,https://www.facebook.com/nervousplaything/,music,C2_Tuesday,TBA,,,https://vimeo.com/429792651,
58,Accessibility of Music Theory in Ableton Live,Elise Reitze-Swensen,extension,TBA,,,,,paper,daws-live-algo,6,,,,
59,Repurposing an Acousmatic Skill Set for the Composition of Popular Electronic Music,Patrick Carroll,https://drive.google.com/file/d/1EBG86heLeWh6NQg8U8tWOBxih74mQS_2/view?usp=sharing,"This paper compares acousmatic music and certain popular genres of electronic music such as electronic dance music (EDM) and the less dance floor-oriented styles of electronica. Specifically, this research explores how certain methods of structuring a composition in acousmatic composition can be adapted for the creative process of these popular electronic music genres. 

It is less the artistic goal, and more the technical and theoretical skillsets of acousmatic composition that are being examined as valuable assets in the composition of popular styles of electronic music, namely the methods of accumulating and processing sonic material, planning and organising these temporally and creating a sense of compositional narrative. It explores this through a method of combining the top-down compositional strategy of more popular electronic music genres with the bottom-up strategy of acousmatic music work.
The techniques of organising a work through grouping sounds of similar timbral and/or morphological characteristics are also explored beyond the context of acousmatic composition, in my own work Dashboard Exam. Here, the macrostructure is derived from a popular music framework, yet the musical materials are treated in an acousmatic music convention, with all sounds (save for a short synthesizer chord sequence) created from a recording of buttons and switches on the inside of a car being manipulated. During the compositional process, the sounds in the recording were processed extensively before they were installed into the already-established popular music framework, resembling Curtis Roads’ multiscale composition.

Denis Smalley’s analytical concept of Spectromorphology, provides an intuitive resource for articulating certain sonic processes in time, specifically those which occur in relation to a sound’s frequency spectrum. This concept is used to analyse the sound materials, searching for relationships among the sounds and certain behaviours over time which can suggest their possible functions within a composition.

While these concepts may be considered somewhat esoteric, the value in their ability to create new musical thought and compositions themselves, in all areas of electronic music, is too substantial to be restricted to academic zones of composition. This research invites a new group of composers (often existing outside of the field of academia) to explore elements of acousmatic music theory, within the more accessible context of EDM and electronica.","Patrick Carroll is a composer from Sydney, and a current PhD candidate at the Sydney Conservatorium of Music, studying the intersection of traditional acousmatic theory and popular genres of electronic music. While completing his Bachelor of Music (Composition) with Honours in 2015, Carroll began releasing and publishing dance music and electronica under the alias Piecey, amassing over a million plays on Spotify to date. In 2018, Carroll began releasing music under his own name (Pat Carroll), aiming the project at more of an experimental sound, and putting the concepts at the centre of his studies to practice. ",https://drive.google.com/file/d/1q4TmlS2CwtG_G-4ElhbgpPZQCDtDQU1F/view?fbclid=IwAR23YXtLz8K7fvXTZjBoz699zO_8TjKCdr4BOsJqbopirPyAonT5aaQ-R9c,,https://www.facebook.com/patcarrollmusic/,paper,musico-comp,3,,,,
60,Volca,Patrick Carroll,https://drive.google.com/open?id=1gWq4kSgSOPYvKjygF00GPyyrXkM64RoE,"Volca is a 10-minute fixed, stereo electroacoustic work designed around the exploration of the small Korg analog drum machine, the ‘Volca Beats’. Entirely all sounds were created from the source material of the analog kick drum, snare drum, tom and hi-hat engines, as well as from the machine noise of the unit itself. The compositional process involved the creation of a set processing patches created in Ableton Live and Bitwig, that involved rhythmically controlled randomisation the parameters of various audio effects, of which the output was recorded. The idea of this process was to create a sound design situation with a high chance of stumbling on happy accidents that could then be used in the creation of micro sound units and longer structures. The piece also explores a reductive exploration of the sounds of the Volca Beats by avoiding the dance music rhythmic structures that the unit is designed to sequence and focussing on the more unexpected sonic possibilities of the machine. However, many sounds and sections do reference the unit’s close association with rhythm and popular electronic music.","Patrick Carroll is a composer from Sydney, and a current PhD candidate at the Sydney Conservatorium of Music, studying the intersection of traditional acousmatic theory and popular genres of electronic music. While completing his Bachelor of Music (Composition) with Honours in 2015, Carroll began releasing and publishing dance music and electronica under the alias Piecey, amassing over a million plays on Spotify to date. In 2018, Carroll began releasing music under his own name (Pat Carroll), aiming the project at more of an experimental sound, and putting the concepts at the centre of his studies to practice. ",https://drive.google.com/file/d/1q4TmlS2CwtG_G-4ElhbgpPZQCDtDQU1F/view?fbclid=IwAR23YXtLz8K7fvXTZjBoz699zO_8TjKCdr4BOsJqbopirPyAonT5aaQ-R9c,https://soundcloud.com/patrick-carroll-music/volca,https://www.facebook.com/patcarrollmusic/,music,P4_Thursday,TBA,https://soundcloud.com/patrick-carroll-music/volca,,,
62,Black Summer,Mark Oliveiro,emailed,TBA,,,,,music,TBA,TBA,,,,
63,community development through online radio,Eugenia Stuart|Nicholas Trivett|Laurence Hughes,emailed,TBA,,,,,paper,cult-comm-eco,1,,,,
64,Vertical Harmony: Flight-path as Musical Form,Robert Jarvis,https://bobzeal.sg3.quickconnect.to/d/s/560821039641178301/6tLKnO5ckqEvfcPCDNZcpuBj0xlA_ocF--7AACtVvyAc_,"We present output of research into the integration of human flight and musical performance via telemetry driven, spatial meta-compositions. The work explores the use of a sailplane flight path as a structure around which musical form can emerge. It serves as a point of reflection on the composition of such spatial meta-compositions and their potential for enhancing personal experiences and enabling new modes of artistic performance in mixed realities.

The system has three primary components - a body-worn sensing unit responsible for logging and broadcasting positional data, a log playback system, and a suite of composition modules built in Max for Live.

The positional sensing unit consists of an Ardupilot-based flight controller and Raspberry Pi computer. The Raspberry Pi communicates with the Ardupilot board via the Mavlink protocol allowing for logging and broadcasting of captured sense data as Open Sound Control (OSC). The system has been designed such that it can be used for logging data as well as communicating in real-time with the composition modules either locally over Wi-Fi or remotely via 433Mhz radio link. The telemetry playback module was built in Max allowing for the replaying of logged telemetry alongside synchronised video recordings. This enables the composer to create a spatial meta-composition ""offline"" for later use in a real-time performance context.

The piece Aileron One is the output of an iterative, creative process consisting of cycles of music composition and interactive system development. The work uses only a subset of the data points captured during the flight; altitude (position above the ground in metres), total velocity (in metres per second), and heading (orientation around the up vector in degrees).

Altitude serves as the foundational element on which the piece is built. The piece can be considered as a vertical structure rising to approximately 950 metres above the ground. A circular chord progression (Fmaj9 Fmaj9/E Am9 G) repeats along the up vector with instrumentation becoming denser in six, discrete, evenly spaced layers. To add variation and movement, a set of instruments are panned to four evenly spaced compass headings. Total velocity is then used to control the speed of melodic and percussive elements throughout the piece.

The spatial approach leads to interesting temporal-harmonic consequences: as the glider ascends, the repeating chord progression is played in one direction, as the glider descends the progression is played in the other direction. The rate at which the progression moves is dictated by the vertical velocity of the glider. The harmonic progression ascends at a steady rate as the glider is towed into the air, leading to a point of contrast when the tow is released and the glider transitions to a slow, steady descent. The musical effect of these moments is an aesthetic mirroring of the dynamics of the aircraft.

It can also be seen that during periods of rapid ascent and descent such as during aerobatic manoeuvres, the musical result is less coherent as chords pass to quickly to establish effective tension and release. This highlights a challenge of composing spatial meta-compositions, that is how to spatially scale musical properties such that passing through them at a range of velocities produces satisfying musical results.

The work as it is presented is part of an active research effort and as such will continue to be refined. We foresee potential to create far richer work by creating coherent mappings between musical elements and data components resulting in complex, emergent musical results.

We are actively investigating gliding flight alongside other forms of physical activities such as kayaking and rock climbing to determine both the performative potential of these activities as well as the musical considerations when composing spatial meta-compositions of contrasting scales and physical contexts.","Robert Jarvis is an accomplished audio-visual artist based in Melbourne, Australia. He works across live video performance, music, animation and software development, with a focus on the development of tools for live audio-visual performance. He is currently a PhD candidate at RMIT University where he is exploring the intersection of gliding flight and musical performance.",https://bobzeal.sg3.quickconnect.to/d/s/560821039641178301/6tLKnO5ckqEvfcPCDNZcpuBj0xlA_ocF--7AACtVvyAc_,https://youtu.be/Ct-Sb-zGlG0,https://zeal.co,paper,daws-live-algo,3,,https://youtu.be/Ct-Sb-zGlG0,,
65,AILERON ONE,Robert Jarvis,https://bobzeal.sg3.quickconnect.to/d/s/560821687040389482/YoQoliO5HDhsWYKjgOymVvcZ8HCks6le-GLaA49Z0yAc_,"Aileron One is the output of a process consisting of cycles of music composition and software development, looking into the integration of human flight and artistic performance.

Altitude serves as the foundational element on which the piece is built. The piece can be considered as a vertical structure rising to approximately 950 metres above the ground. A circular chord progression (Fmaj9 Fmaj9/E Am9 G) repeats along the up vector with instrumentation becoming denser in six, discrete, evenly spaced layers. To add variation and movement, a set of instruments are panned to four evenly spaced compass headings. Total velocity is then used to control the speed of melodic and percussive elements throughout the piece.

This spatial arrangement leads to interesting temporal-harmonic consequences: as the glider ascends, the repeating chord progression is played in one direction, as the glider descends the progression is played in the other direction. The rate at which the progression moves is dictated by the vertical velocity of the glider. The harmonic progression ascends at a steady rate as the glider is towed into the air, leading to a point of contrast when the tow is released and the glider transitions to a slow, steady descent. The musical effect of these moments is an aesthetic mirroring the dynamics of the aircraft.

The work as it is presented is part of an active research effort and as such will continue to be refined. We foresee potential to create richer work by maximising mappings between musical elements and input data to result in complex, emergent musical results.","Robert Jarvis is an accomplished audio-visual artist based in Melbourne, Australia. He works across live video performance, music, animation and software development, with a focus on the development of tools for live audio-visual performance. He is currently a PhD candidate at RMIT University where he is exploring the intersection of gliding flight and musical performance.",https://bobzeal.sg3.quickconnect.to/d/s/560821687040389482/YoQoliO5HDhsWYKjgOymVvcZ8HCks6le-GLaA49Z0yAc_,https://youtu.be/ywen9LQ6Wn4,https://zeal.co,music,C4_Thursday,TBA,,https://youtu.be/ywen9LQ6Wn4,,
67,"‘Critical Feedback - The evolution of the drums, feedback and the computer’",Nicholas Meredith,https://www.dropbox.com/s/oc641or31xokbgb/ACMC2020%20Nicholas%20Meredith%20%3A%20Kcin.mov?dl=0,"My motivation for this work is to evolve the drum kit into something that operates in a way far removed from its origin. More of a timbral drone maker than a traditional drum. Using the computer as a sound processor, I take incoming audio from contact microphones connected to multiple drums (played by me) and feed it though a network of effects and resonating algorithms. I create feedback loops through amplifiers and acoustic drums and use this as live source material routed through the computer via various digital and hardware processors. This process creates a series of constantly evolving rhythmic and timbral landscapes all driven by the acoustic drums.

I create an undulating ecosystem of sound that completely envelopes the listener. Many of the algorithmic processes I use are random and as such, every performance is different. I am constantly interacting with the computer; playing with it and playing against it in order to sculpt the arc of the performance. Despite being improvised, this particular work can also be tweaked to operate autonomously as an installation.

Performing on an instrument as resonant and physical as the drums, I am fascinated with how they ‘feel’ to play. The way a timbre of a particular note feels under my hands or feet is just as important as how it ‘sounds’. The issue of physicality and feel is an important one when it comes to music made on the computer and one which I think is a constant problem for performers in that arena. I am working on bridging that gap for myself by using acoustic drums but by also having amps next to me while I perform. I have been a drummer all my life I and have spent a large proportion of my time on stage next to amplifiers. Not only do they serve a utilitarian function in the music in terms of creating feedback loops to use as audio input for the computer but they help give me a very real and physical sense of what is going on in the music.

As someone who has never truely felt like they ‘fit in’ with any musical genre or scene, making solo work has really been the only way to truly find my voice as an artist. This particular solo work is just one part of what I do. It is the honest voice of someone trying to evolve the percussive art form through the use of computers whilst maintaining a very strong emotional and physical connection to the practice of making music.

[Video](https://www.nicholasmeredith.com/watch)

[Audio](https://www.nicholasmeredith.com/listen)

","Nicholas Meredith’s sound-world, although wet through with electronic and digital elements, draws significantly from the natural world: monolithic structures, human biology, water. Working with themes of uncertainty and environmental change.
His music is deeply personal and highly visceral. Moving from moments of stillness to complete sonic destruction. Meredith's history as a jazz drummer and improviser
  
combined with his embrace of the infinite opportunities offered by technology infuse his compositions with a freedom of rhythm and structure that is energising.",,,https://www.nicholasmeredith.com/,music,C3_Wednesday,TBA,,,,
68,The Evolution of Music in Pokémon Console Games: How does musical texture and aesthetic of the Pokémon videogame theme evolve from 1996- 2017?,Madhuri Suresh,https://drive.google.com/file/d/1liVVJsAjiSXvLTeAcuAiuK1Rf9751cO_/view?usp=sharing,"Music for videogame play is a deeply connected notion of the development of both the visual action and the evolution of human interaction. The narrative action and human interaction evident in the Pokemon franchise is supported by the musical design. The pokemon videogame series saw humble beginnings in February 1996 as a pair of videogames for Nintendos Game Boy console. Despite the various differences and developments in the existing generations of the video games, there is a perceptual evolution in the musical design, of particular note is the contrast and development in the theme tune, as well as a consistency in the motivic materials, throughout the expansive series. This research adopts a case-study methodology for testing its theory against a real-world application to music for video game design. This theory is that of consolidated unity and contrast within parallel notions of visual and musical design, as applied to interaction and narrative. The characters within this franchise, the Pokémon, evolve through the interaction with the human player, just as the theme tune evolves in the macro structure of this multi-generation video game franchise. The theory proposed concerning this video game is two-fold: (1) the central concept in the Pokémon video game narrative is: character evolution; (2) the salient consideration for the design of music in this Pokémon game is: evolution or thematic development. Out of all the music in Pokémon, the main theme of the title track of each game is arguably the most iconic and well known. The leady melody created by Junichi Masuda makes an appearance in every generation’s title theme till date. In this paper, I will be discussing about the evolution of theme music in the Pokémon video games focusing mainly on the musical texture and aesthetic. ",,,,,paper,musico-comp,4,,,,
69,Four Approaches to Graphic Notation of Quadrophonic Electroacoustic Music and Extended Vocal Techniques,Sophie Rose,https://www.dropbox.com/sh/15quxwm5ndljhjs/AACzQgBTlADDamWNku2dVbWPa?dl=0,"This paper discusses the technologies and four notation approaches used to score a series of four  works (Barren, Ferns, Smother, and Chaos) using extended vocal and percussion techniques for Vowels in Retrograde (Rose, 2019). The challenges I was faced with were notating for extended vocal techniques, spatial music, and for players who could not read traditional Western notation. Scoring techniques were chosen according to perceived relevance to the piece. The technologies used were either low-cost or open source, such as Decibel ScorePlayer, iPad, Inkscape, AutoStitch, and assorted physical media and consequent digitising means. The result was a 55-minute suite of new works for extended vocal techniques, percussion, cello, koauau, and electronics. Scoring techniques were derived from a mixture of existing approaches, including text-based (Harlow, 2019; Oliveros, 2013), artwork or image-based (Steiner, 2004), ancient music scoring methods (Daves, 1952; Hickmann, 1956), shapes to indicate breath, sounds, or pitches (Schieve, 1984; Wishart, 1996, 2012), and mixtures of traditional and graphic notation (Christou, 1968; Crumb, 1971).

Keywords: spatial music; extended vocal techniques; graphic notation; ","Sophie Rose is a doctoral student at the University of Melbourne and contemporary vocals lecturer at Australian Institute of Music. She is a singer, extended vocal technique enthusiast, composer, improviser, performer, and maker. She explores the relationship between creative practice, interactive technology, and embodiment in her work. She performs and collaborates regularly with Cloud Unknowing, Brigid Burke, and surrealist music collective Little Songs of the Mutilated. ",https://www.dropbox.com/s/w2z1x18mcrexshe/Sophie%20Rose%20Bio%20Pic%201.jpg?dl=0,https://youtu.be/QhPTvP4ZDJs,https://sophiemusicrose.com/,paper,synth-nota-inter,3,,https://youtu.be/QhPTvP4ZDJs,,
72,P-Bow: a profane electric violin and bow for use in Ambisonic audio environments,Cloud Unknowing|Sophie Rose,https://www.dropbox.com/sh/ja3smxz2d51bnh2/AAD1uaTgdrfVmIZUS2yxwybZa?dl=0,"In 1977 Laurie Anderson created the tape-bow violin which used magnetic tape in place of horsehair on the violin’s bow (Anderson, 1977). We ask about how this may be implemented with modern technologies, such as Arduino, motion sensors, Ambisonic sound-fields, and nichrome wire. Similar to Anderson, we take a postmodern punk approach to this design to create the P-Bow, or Profanity-Bow. This design allows the user to have two independent sources of control, including linear controllers, binary controllers, direction, and speed. The P-Bow has three parts: the bow, a wrist-mounted enclosure, and the violin. We used 3D-printed enclosures, three Nano-style microcontrollers, two BlueSMiRFs, nichrome wire, buttons, 6- and 9-Degrees-of-Freedom sensors (accelerometer, gyrometer, and compass), aluminium flashing strip, aluminium plate, knurled steel, and an LM317 as a Kelvin sensor. It was programmed in C++ for Arduino (arduino.cc, 2019) and connects via system serial ports to Max/MSP, and Max-for-Live and Ableton (2018). We discuss two preliminary works to chart the potential of the P-Bow, Cockwomble (Rose, 2020) and Tubular (Unknowing, 2020). This paper discusses the design, construction and development of the P-Bow and an evaluation of the current design. 

Keywords: Ambisonic; electroacoustic music; punk; interactive music; interface design. 
","Cloud Unknowing is a sound artist, sound engineer, maker, musician, and electronics enthusiast. He is studying a Bachelor of Sound Production at Box Hill Institute. Unknowing has worked in theatre and immersive theatre, installation, instrument design and building, and film. He often works with subversive themes with technical challenges, science fiction, and the retrofuture. In his spare time, he builds valve amplifiers, effects pedals, and collaborates with Sophie Rose. 

Sophie Rose is a doctoral student at the University of Melbourne and contemporary vocals lecturer at Australian Institute of Music. She is a singer, extended vocal technique enthusiast, composer, improviser, performer, and maker. She explores the relationship between creative practice, interactive technology, and embodied cognition in her work. She performs and collaborates regularly with Cloud Unknowing and surrealist music collective Little Songs of the Mutilated. 
",https://www.dropbox.com/s/zkdf3joh62xzdl4/2019%20Performance%20-%20Nothinge%20Gig%20March%2013%2030mins.jpg?dl=0,https://youtu.be/PR0LPOKeQDI,http://ampoule.audio/blog/,paper,spatial-perf,3,,https://youtu.be/PR0LPOKeQDI,,
74,The Creation of the Distance Mixer,Nic McConaghy,https://www.dropbox.com/s/a9c3iq1nz7wdbgc/ACMA2020_McConaghy_DistanceMixer.m4v?dl=0,"The expressiveness, complexity and detail of the sounds of the natural world hold particular appeal for composers working at the nexus of music, technology and the environment. The innate spectral and spatial characteristics of these sounds offer a wealth of creative potential, but such qualities often derive their meaning from the contextual setting in which they originate. The tradition of soundscape composition promotes the tight integration between this contextual information and musical structure and denounces abstraction.  Its interpretation, therefore, requires explicable knowledge of environmental associations, and its meaning is inseparable from this context. Respecting and balancing these concerns with the traditions and techniques of acousmatic music poses a considerable challenge. This paper outlines a personal approach to composing with environmental sound that integrates reductive and contextual aesthetics into single compositional language. It discusses the strategies that led to the creation of a new software artefact and the musical value of the sounds produced through this technology.

The Distance Mixer is a custom SuperCollider class designed to assist in the composition of fixed-media works that seek to integrate environmental structures into electroacoustic composition. Its ongoing development reflects an attempt at maintaining the integrity of natural sound environments via acoustic partitioning while still permitting a degree of abstraction and transformation of sound materials. The Distance Mixer allows natural sound ecologies to inform the deployment of materials and encourages the practitioner to explore in real-time, the relationship between perspective, movement and distance and the influence of these spatial attributes on spectral space and the designation of acoustic niches.

The Distance Mixer applies the powerful spatial filtering tools of the Ambisonic Toolkit in novel ways to spatialise a variable number of sound sources and produces convincing spatial illusions and cogent aural landscapes. At its core is a distance variable that controls low-pass filter coefficients, amplitude scaling, soundfield quality, and the level of a simple convolution reverb for first-order ambisonic signals.

This paper will describe how the Distance Mixer emerged from an aesthetic discourse that consolidates the concepts of biomimicry, acoustic ecology and spectromorphology to explain the function of sound in the environment and its parallels with musical structure. The relationship between acoustic niches, perspectival space and spectromorphological qualities were of particular importance to its development. Through the immateriality of this theoretical framework, new approaches to composition emerged that had a productive and transformative impact on the creative process. 

","Nic McConaghy is an audio technician and composer of acousmatic music. He is in the final stages of a PhD in composition at the Conservatorium of Music, University of Sydney, where he actively engages in researching numerous elements of music technology, composition, recording, electroacoustics and ecoacoustic composition. In a research capacity, his oeuvre encompasses an array of innovations based on field recordings and environmental sound composition across a broad range of activities involving the use of extant and emergent technologies.",,,,paper,spatial-perf,4,,,,
75,Braided Stream: Intertwining Soundscape Strata and Ecological Data in South Australia's Limestone Coast,Jesse Budel,https://drive.google.com/file/d/1HFMFC5VqNjDT6NM,"Live streaming of soundscapes has in recent years become increasingly popular in acoustic ecology and ecoacoustics as a means of monitoring ecosystem activity and behaviour. This is particularly true of creative projects, where such livestreams is used in lieu of or in tandem with prerecorded content, often coupled with other ecological data sets that contextually inform audio processing approaches. However, where such projects employ multiple simultaneous livestreams, the soundscapes used are often from geographically separate locations, which overlooks the possibilities offered by a multiperspectival consideration of a singular site.

One such example of this latter approach is this author’s installation, Strata, for the Sir Robert Helpmann Theatre in Mt Gambier, South Australia. Comprising two systems—a three-tier microphone array at the Naracoorte Caves National Park (at subterranean, ground and canopy levels) and a counterpart speaker array in the Theatre’s courtyard (at ground, mezzanine and roof levels)—the installation draws together distinct soundscape layers of the Naracoorte Caves system. This paper discusses the employment and manipulation of multiple soundscape livestreams in the installation in Max 8, drawing particular attention to the inherent creative acts in streaming (with reference to Baradian agential realism) and the employment of complementary ecological data streams (such as BOM weather data) as a means of driving live audio processing. Potential future pathways for creative applications of soundscape live-streams, particularly those related to Arts-Science collaboration and community engagement, are summarily considered.","Jesse Budel is a composer and sound artist based in South Australia. He recently graduated with a PhD (with Dean’s Commendation for Doctoral Thesis Excellence) at the Elder Conservatorium of Music, The University Of Adelaide, where his research focussed on ecological sound art. Developing works for diverse media and spaces, Jesse’s concert works have performed by the Elder Conservatorium Wind Orchestra (AUS), the Australian String Quartet (AUS) and Corvus Ensemble (AK, USA). A previous Carclew Fellowship and Helpmann Academy Grant recipient, he remains highly active in the South Australian arts scene, curating both the Featherstone Sound Space and Murray Bridge Piano Sanctuary. Jesse currently serves as the Secretary for Australian Forum for Acoustic Ecology and World Forum for Acoustic Ecology.",https://drive.google.com/file/d/1emFoT5DRZw61xJXOAW8hpZ6l4HH6LptY/view?usp=sharing,https://www.youtube.com/watch?v=ysOwzb2uiCI,http://www.jesse-budel.com,paper,cult-comm-eco,2,,https://www.youtube.com/watch?v=ysOwzb2uiCI,,
77,Towards a vision for a virtual DAW collaboration studio for professional post-production music projects,Scott Stickland|Nathan Scott|Rukshan Athauda,https://uoneduau-my.sharepoint.com/:v:/g/personal/c3068031_uon_edu_au/EbZAvzdHOrVEjPURwTwO8eoBIfr-CHbML2AeGP1vVYIVZg?e=sIAR5d,"The global coronavirus (COVID-19) pandemic has brought into sharp relief not only the need for effective and inclusive online collaboration platforms but those that provide naturalistic and contextual extensions to the day-to-day “offline” milieu [1-3]. In this presentation, we explore “What would an ‘ideal’ online collaboration platform provide for professional post-production in music?”

We interviewed a group of music/sound practitioners who regularly work with complex multitrack digital audio projects and, in many cases, have limited or no facility for remote real-time collaboration directly through their music software. We collated the data produced from professionals working in Australian recording and post-production contexts through a series of user-focused interviews. This process ascertains their existing methods of collaboration/production and garners their perspectives of an “ideal” environment for collaborative remote music post-production. In this talk, we present an analysis of their information and the subsequent outcomes. 
Overwhelmingly, the collaborative post-production practices described by the interviewees are asynchronous. That is, real-time collaboration does not occur even though it is highly desirable. Although some music production platforms now feature integrated collaboration methods, such as Avid’s Cloud Collaboration [4] and Steinberg’s VST Transit [5], audio and session/project file sharing via third-party cloud storage remains a popular means for studio engineers to disseminate work to their clients for feedback and approval. Using services such as Dropbox [6] and Google Drive [7], studio mixers/producers upload stems and a software-specific project file, or just an audio mix, for the client to download and audition. Any changes require further communication between the client and mixer/producer before any alterations can proceed. Upon completion, the uploading/downloading, auditioning, and feedback process begins again, until eventually reaching a mutually-satisfactory result, or, in some instances, the client depletes the budget. When integrating video into a project, and depending on a music production software’s import/export capabilities, some practitioners prefer to share advanced authoring format (AAF) [8] or open media framework (OMF) [9] files instead, primarily to ensure video-audio synchronisation, but also to include basic automation, such as volume and panning changes. The interviewees also articulated a “wish-list” of features and capabilities given the opportunity to collaborate remotely with clients, producers, or other studios in real-time, highlighting the expected outcomes of such an environment.

A conclusion to be drawn from the interviews is that, outside of remote one-to-one recording of an instrumentalist or vocalist, studio engineers do not engage in real-time remote collaboration simply because synchronous post-production collaboration methods do not currently exist. That is not to say that studio professionals have not contemplated what would constitute a practical remote post-production real-time collaborative environment. All the interviewees were forthcoming when asked to provide a wish-list of capabilities and features they would consider essential when working with a synchronous platform. Some common themes emerged from the various responses, particularly:

- The ability to see and speak with the client during the session;
- The ability to edit, mix and produce, and operate the music production software in-studio;
- The ability to audition remote changes on-the-spot; and
- The integrity and high fidelity of the project’s audio files are maintained and protected.

One interviewee stated his expectation quite succinctly, saying he wanted the experience “to be like I was sitting there in the studio” with his project collaborators in a “virtual studio”. One could well adopt this sentiment as an overarching vision for developing any remote collaboration platform, particularly one that operates in a real-time environment.

Our research work has the potential to make this vision a reality. In 2018, we proposed a framework that had the potential to realise real-time collaboration on music production projects over the Internet [10]. Further refinement to the framework, mainly focusing on online group establishment and the creation of bilateral control data channels, led to our prototype implementation paper, presented at the 2019 Web Audio Conference [11]. Presently, we are working on a fully-functional online collaboration platform for professional post-production in music. In future, we expect to evaluate this platform in real-world “virtual studio” environments with industry professionals/sound engineers and clients. 

Supporting Material

1. Organisation for Economic Co-operation and Development. 2020. Education responses to COVID-19: Embracing digital learning and online collaboration.  (23 March 2020) Retrieved 5 May 2020 from https://www.oecd.org/coronavirus/policy-responses/education-responses-to-covid-19-embracing-digital-learning-and-online-collaboration-/
2. Burris, P. 2020. COVID-19 Era Will Tell Us Much About Future of Collaboration Tools.  eWeek. Retrieved 5 May 2020 from https://www.eweek.com/enterprise-apps/covid-19-era-will-tell-us-much-about-future-of-collaboration-tools
3. Wong, K. (2020, June). COVID-19 Pushes PLM/PDM to the Cloud: From bill of materials and file sharing to collaboration, many functions move to the cloud during lockdown. Digital Engineering 247, 26, 5, 23-25. Retrieved 15 June 2020, from http://search.ebscohost.com/login.aspx?direct=true&db=aps&AN=143695425&site=eds-live.
4. Avid Technology Inc. 2020. Producing Software for Music - Cloud Collaboration - Pro Tools.  Retrieved 7 March 2020 from https://www.avid.com/pro-tools/cloud-collaboration
5. Steinberg Media Technologies GmbH. 2020. VST Transit | Steinberg.  Retrieved 7 March 2020 from https://www.steinberg.net/en/products/vst/vst_transit.html?et_cid=15&et_lid=22&et_sub=VST%20Transit
6. Dropbox Inc. n.d. Dropbox Professional.  Retrieved 7 March 2020 from https://www.dropbox.com/pro
7. Google LLC. 2020. Google Drive: Free Cloud Storage for Personal Use.  Retrieved 7 March 2020 from https://www.google.com/drive/
8. McLeish, D. and Tudor, P. 2004. The Advanced Authoring Format and its Relevance to the Exchange of Audio Editing Decisions. In Proceedings of the 25th International Conference of the Audio Engineering Society (London, United Kingdom, 17-19 June 2004). Audio Engineering Society Inc., New York, United States. Retrieved 9 January 2019 from http://www.aes.org/tmpFiles/elib/20190108/12824.pdf
9. Lamaa, F. 1993. Open Media Framework Interchange. In Proceedings of the 8th Audio Engineering Society Conference UK (London, United Kingdom, 18-19 May 1993). Audio Engineering Society Inc., London, United Kingdom. Retrieved 9 January 2019 from http://www.aes.org/tmpFiles/elib/20190108/6134.pdf
10. Stickland, S., Scott, N. and Athauda, R. 2018. A Framework for Real-Time Online Collaboration in Music Production. In Proceedings of the ACMC2018: Conference of the Australasian Computer Music Association (Perth, Australia, 6-9). Retrieved 20 March 2019 from https://computermusic.org.au/conferences/acmc-2018/
11. Stickland, S., Athauda, R. and Scott, N. 2019. Design of a real-time multiparty DAW Collaboration Application using Web MIDI and WebRTC APIs. In Proceedings of the Web Audio Conference (WAC 2019) Diversity in Web Audio (NTNU, Trondheim, Norway, 4-6). Trondheim, Norway. Retrieved 1 May 2020 from https://www.ntnu.edu/documents/1282113268/1292502725/WAC_2019_proceedings.pdf

","Bios
Scott Stickland:
Scott Stickland is a third-year PhD (Music) candidate in the School of Creative Industries at The University of Newcastle (UoN), Australia. He has previously completed a Master of Music Technology (UoN) and a Bachelor of Education (Sec) – Music (Melbourne), and taught and coordinated music programs in Victorian secondary schools for 16 years. Scott has presented papers at ACMC2018 and WAC 2019 since commencing his PhD. He currently teaches audio and music production through his business, Monty Sound Production, and plays keyboards in the national touring band, Cool Change – The Ultimate Tribute.
Nathan Scott:
Nathan Scott is a lecturer in the School of Creative Industries at the University of Newcastle, Australia. He has interdisciplinary research interests spanning creative arts, technology, science, health and education. Nathan has presented and performed internationally, and has published in the areas of music, technology, education, gaming and the human voice. He has presented workshops in regional NSW (2003) and developed an online international postgraduate program supporting the use of technology in music contexts. He participated in the CHASS Expanding Horizons forum in Canberra (2006) and undertook a sub-project as part of an ALTC National Teaching Fellowship (2010).
Rukshan Athauda:
Dr Rukshan Athauda is a Senior Lecturer at the School of Electrical Engineering and Computing at The University of Newcastle (UoN), Australia. Dr Athauda’s research interests span Database Systems, Technology-enhanced Learning, Cloud Computing and IT Security. Dr Athauda has published over 60 peer-reviewed research articles internationally. He has supervised 4 PhD completions at UoN and also undertaken a number of admin roles including Head of Discipline and Program Convenor. Prior to joining UoN, Dr Athauda has worked at Microsoft Corporation, USA, High-Performance Database Research Centre at Florida International University, USA and Sri Lanka Institute of Information Technology, Sri Lanka.
",https://uoneduau-my.sharepoint.com/:i:/g/personal/c3068031_uon_edu_au/ERIEZo-gNsZNqJyy7hiynjMBGOHJnm73zbgM6Peuib7tEg?e=X9nS4a,,https://www.montysp.com.au,paper,daws-live-algo,4,,,,
78,Piece No. 3,Lynden Bassett,https://www.dropbox.com/s/hdrh07uh7w7hk7m/19062020_mixed%20lens%20ems_Piece%20No%203%20v2_21062020.avi?dl=0,"Piece No. 3 explores communication and miscommunication. The texture is formed by a web of improvisatory communicative links between performers. Soft-synths are directed to respond to each other and to physical instruments, which in turn interact with each other. These links are built flawed however, as each part’s sonic content is only vaguely related to all the others’, the synthesiser controls are alien, and the nature of the interactions is combative. Recordings of a human voice underscore this, attempting to express frustration, disillusionment, and panic, but being able to only in fragments. 

Hopefully, the specific rules of interaction in the score are not central to fruitfully listening to the piece, and the (mis)communications between parts are audible to some degree. Further, while these connections create form and texture, and hold much meaning for me, the surface, the impression of the work is intended to reflect the messaging in the underpinning structures. 

Performed by members of the ANU Laptop Ensemble and Canberra Experimental Music Studio: Abigail Thomas, Weitong Huang, Lynden Bassett (laptops), Jaime Langer (keyboard, laptop), and Miles McLaughlin (banjo, laptop) at the Lonsdale St Studio.
","Lynden Bassett is an Indonesian-Australian composition student, currently studying an undergraduate degree at the ANU School of Music. His musical background is in punk and hardcore, and he plays in Canberra band, HYMMNN.
The Canberra Experimental Music Studio (Canberra EMS) is a group of performers, composers, and improvisers from the Canberra region, loosely based at the ANU School of Music. 
The ANU Laptop Ensemble is operates out of the ANU Research School of Computer Science and the ANU School of Music. Members explore different ways of using the laptop in group performance.",https://www.dropbox.com/s/2pkutr6l38p1icb/Screenshot%20%28295%29.png?dl=0,https://lyndenbassett.bandcamp.com/track/piece-no-3,,music,C4_Thursday,TBA,,,,https://lyndenbassett.bandcamp.com/track/piece-no-3
79,Croake,Robert Croft & Stephen Oakes,https://forms.gle/vd6BHM9HCk3RMF9Y7,"Robert and Stephen create work made possible by software. 

There is an immediacy to the process. Roberts vocals are captured on the fly into sound clips that are then spat back out complete with digital processing.",Croake is a collaborative project between Stephen Oakes and Robert Croft. The pair have been producing their unique compositions for over 10 years. ,https://drive.google.com/file/d/1V5tlTQHrU4wlJnUtw8wxSWt1KMwlwaqM/view?usp=sharing,https://soundcloud.com/stephen-oakes-175115657,,music,C3_Wednesday,TBA,https://soundcloud.com/stephen-oakes-175115657,,,
81,Algorithmic music generation: solving the right problem,Matthew Whitley / Skueue,https://drive.google.com/file/d/1yMj9q7h2ieKocZbocRCF0O6XYKNxcpEI/view?usp=sharing,"I've spent the last 7 years working on an algorithmic music generator within the PureData programming environment

By the end of this talk,  I want you to know: how to come up with a musical idea, and get a computer to create music that follows along to your musical intent

My music software allows me to create a completed musical work within 20 minutes, and has enabled the creation of hundreds of musical pieces 

I want to share some of the insights into how I get I'm able to use music theory terminology, as a way to compose generative music

The talk centres around using the 'Clave' or 'Rhythmic Key' as a structural device, to provide a framework to place musical ideas

Some of the other concepts I explore are: harmonic contours, scale quantizers, pre-mediated scale modulation, functional harmony, melodic repetition, chord progressions, rhythmic embellishments, and deterministic-algorithmic music generation","My name is Matt, and I compose and perform under the artist name Skueue

I'm a musician that makes generative, microtonal electronic dance music using the PureData programming environment",https://drive.google.com/file/d/1k5GDnBgueRC3Nlr718zQxsu-9WRmMP1k/view?usp=sharing,https://www.youtube.com/watch?v=7zo_iFMyydE,https://skueue.bandcamp.com,paper,daws-live-algo,5,,https://www.youtube.com/watch?v=7zo_iFMyydE,,
82,Darshan with a Pelican: Multiplicities (2020),Warren Burt,https://spaces.hightail.com/receive/aVJf7W41c5,"This paper will describe the compositional techniques used in the multichannel sound composition “Darshan with a Pelican: Multiplicities” which would have been premiered on May 22 at the GRM Paris 40 channel Acousmonium concert hall, but this performance was cancelled due to the corona virus. (It is tentatively rescheduled for October 10, 2020.)  A 2 channel version, prepared specially for this conference, will be presented here instead.  This piece will incorporate a number of compositional techniques, such as: image sonification using the Virtual ANS Synthesizer; Markov-chain driven melody composing routines; real-time use of the GRM Tools Spaces modules; multiple microtonal scale complexes; virtual analogue sound fragmentation routines – assembling thick microtonal sound textures with the use of the NYSTHI module toolkit for VCV Rack; and several others.  The aim will be to assemble a large scale (21 minute duration) multi-timbral and multi-textural composition where at any moment, several different sound ideas will be happening, each with their own discrete sound-path spatializations throughout the space.  Making an ever changing ear-dazzling sound complex is the name of the game here.  The paper will also include several sound excerpts from the piece.  ","Warren Burt (b 1949): composer, performer, writer, instrument builder, sound poet.  Currently Coordinator of Post Graduate Studies in Music, Box Hill Institute, Melbourne.  Born in the US, moved to Australia in 1975.  Has been involved in music, video, community arts, community radio, education, etc. since arriving.  Currently living and working in Daylesford, Vic.",https://spaces.hightail.com/receive/aVJf7W41c5,,http://www.warrenburt.com,paper,synth-nota-inter,4,,,,
83,Darshan with a Pelican: Multiplicities (2020),Warren Burt,https://spaces.hightail.com/receive/aVJf7W41c5,"
Recorded sounds routed in space.

This began on July 18, 2019.  We were driving on the Bellarine Peninsula, southwest of Melbourne, when, on a whim, I said, “Let’s go down to Swan Bay.”  So we did, and in the car park was a pelican.  It came right up to me, and we had a good eye-to-eye session.  In Hinduism, having eye contact with a statue of a god is called Darshan, and the pelican and I seemed to have that kind of a relationship.  Maybe it wanted food, but it stayed well beyond the time when it was clear that I didn’t have any food to give.  So we just sat there, in silent communion, for about 20 minutes.  It struck me that there was a complete juxtaposition of non-intersecting consciousnesses there.  We were both comfortable in each other’s presence, but as for the details of our communication, I would be at a loss to say.  

I have frequently written pieces in the past where various layers of sound are juxtaposed, which layers frequently could be viewed as having nothing to do with each other.  And I’m happy to let those layers of sound co-exist.  Maybe like the pelican and I, they eventually begin to make their own kind of sense.  So here we have five different layers, each of which is made in a different way, and each of which has its own kind of sound motion through space.  The five layers are: One: “Freehand,” which are freehand drawings made in Procreate and then transferred into Virtual ANS3.  That’s a program, based on the ANS synthesizer in Moscow, which allows drawings to be read as spectrograms for sound.  Two: “miRacks,” which are textures made from multiple microtonal transpositions of electronic melodies made in 2018-9 with Dhalang MG (microtonal synthesis and composing program).  These melodies are placed in Antonio Tuzzi’s “Sussudio” multiple sample module in the iOS version of VCVRack, called “miRack.”  Up to six different versions of each melody can be heard at once – and these melodies are transposed to pitch levels determined by the original microtonal scale the melody was in.  Three: “Piano Sequences” – a sampled piano in Thumbjam is tuned to a Harry Partch 29-note-to-the-octave scale – the one he tuned his Diamond Marimba to.  Two textures are made – first, a set of chords (mostly stacked fifths), played on a Diamond Marimba style keyboard (made in Lemur); and second, a monophonic melody controlled by the Markov chain generation module in Dhalang MG.  If the first texture seems to contain more than a passing reference to some pieces by Howard Skempton, the second may blushingly nod in the direction of John Cage’s “Cheap Imitation,” which is itself a faintly embarrassed cousin of Erik Satie’s “Socrate.”  The two textures are juxtaposed for the final sequence in the piece.  Four: “Virtual ANS Streetscapes.”  For many months now, I’ve been wandering around Google Maps, seeing streetscapes, some familiar, some never seen before.  Occasionally, I’ll take a screen-capture of a particular street scene.  Then, I’ll load those photos into Virtual ANS, and treat the photos, usually with a 3-step process – edge detect, posterize, and contrast – sometimes in that order, sometimes in another order.  The result will be to reduce the street photography to a series of outlines, which outlines can be heard as spectrographs – quite active ones, with a lot of variation in them (depending on the complexity of the photographs).  The three photos used here are Mohawk St in Cohoes, New York; a landscape photo of Johnstown, Pennsylvania, and the corner of 59th and 3rd in Manhattan.  Each produces a differently articulated sound-scape which form structural bases on which to pile the other textures.  Finally, Five: “Musique Concrete Demos,” which are textures made with various real-world sounds, recorded with my iPhone, that I used to demonstrate various sound modification techniques to my classes.  A back-door screen spring, a Navajo Burden Basket, with many small, high-pitched aluminum bells, a malfunctioning Yogurt fridge in a local supermarket, the sound of corn popping, and the sound of ice-cubes being stirred while making iced-tea.  Each one is time stretched, and most of them are equalized quite high to contrast with the other textures (the exception is the Yogurt fridge, which has an insane amount of bass boost added to its already bass heavy texture).  Originally, each of these five layers was then individually processed with the GRM Tools Spaces 3D plugin – each has its own path around an 8-channel space.  This was intended for a performance on May 22 at the Acousmonium at the INA-GRM in Paris, which features 40 loudspeakers on separate channels.  So with five layers having different independent 8-channel sound routings, the 40 channels of the hall would be totally occupied with moving sounds.  This performance did not happen because the concert was cancelled due to the corona virus.  It is anticipated that the piece will happen at the rescheduled concert on Saturday October 10, circumstances permitting.  However, I also fed each of the five channels of sound independently through the GRM Tools Spaces 3D plugin, this time with 2-channel output, instead of 8-channels.  These stereo processings used different settings and performance techniques than the 8-channel processings did. These five stereo moving-sound channels are then mixed into a stereo mix to make a 2-channel version of the final work, and that’s what is played at this conference.

The five layers don’t all occur at once – there are lots of silences within the individual tracks – but the sound is continuous from beginning to end.  Some of the sounds are abstract, and some veer uncomfortably close to  narrative, but the overall effect is one of formerly unrelated sounds now regarding each other across space and time much like a composer and a pelican in a car park next to Swan Bay in the middle of winter, making their own kind of sense as they sit regarding each other.

20 June 2020, Daylesford, VIC, Warren Burt

","Warren Burt (b 1949): composer, performer, writer, instrument builder, sound poet.  Currently Coordinator of Post Graduate Studies in Music, Box Hill Institute, Melbourne.  Born in the US, moved to Australia in 1975.  Has been involved in music, video, community arts, community radio, education, etc. since arriving.  Currently living and working in Daylesford, Vic.",https://spaces.hightail.com/receive/aVJf7W41c5,,http://www.warrenburt.com,music,P4_Thursday,TBA,,,,
84,Physical Movement in Computer-Based Live Performance,Becki Whitton,https://www.dropbox.com/s/xyei2tmuowku9lz/ACMC%20talk%20-%20physical%20movement%20in%20computer%20based%20live%20performance.mov?dl=0,"Striking a balance between interacting with gear on stage and performing with strong physical energy can be a major hurdle for many electronic musicians. In this talk, Becki reflects on the developments of her own live show to suggest some strategies to overcome this obstacle, with a specific focus on four key questions that any artist can use to help develop their on-stage performance. 

The emphasis of this talk is on the ability of every artist to find a means of physical expression live that will enable them to preserve both the intricacy and detail of their arrangements as well as an on-stage electricity and present engagement with the audience. 

Becki will also examine the ways that different stage settings and strategies in stage lighting (from no-budget DIY ideas to higher-budget #goals) can support different movement styles for electronic sets. This talk is intended for artists who are already making music and want to translate it into a live show, artists who haven’t made electronic music yet but who are keen to delve into that side of composition and performance, and artists who are already performing computer-based music live but would like a new lens through which to approach their physicality and stagecraft.","Becki Whitton is an engineer, songwriter and producer working out of Melbourne’s Rolling Stock Recording Rooms. She works in a range of styles from pop and hip hop (G Flip, Allday) to ambient and experimental music (Brambles, Rainbow Chan). For the last four years she has managed sound for Girls Rock! Camp ACT. In her solo electronic-choral project Aphir, Becki has been hailed by Triple J's Tim Shiel as part of ‘Australia’s new wave of female electronic innovators’ and has performed around Australia and internationally including performances at Music Tech Fest (Berlin), Denmark Arts (WA), and Falls Festival. 

",https://www.dropbox.com/s/tgisehs5tv8wjkf/image%20by%20Isabella%20Connelley.jpg?dl=0,,http://beckiwhitton.com  ,paper,synth-nota-inter,5,,,,
85,Instrument of Failure - Gelido,Andrea Gelido,extension 30/6,TBA,,,,,music,C1_Monday,TBA,,,,
90,i-c-u-curve2020,sharyn brand,https://tobeheard.github.io/i-c-u-curve2020/,"i-c-u-curve2020 is browser-based piece of generative audio-visual art. Navigating to https://tobeheard.github.io/i-c-u-curve2020/ you, the audience will be asked permission to access your camera as the composition begins and you slowly become part of the curve.
Building on from both Unresolved I... and Movement (Wellington, 2019), this new iteration explores humanity and inclusion through the lens of a pandemic. As COVID-19 moves rapidly across the globe, the advice to “flatten the curve” and slow the spread is through social isolation. In this isolation we are asked to consider each other, as within this virus no one is exempt, all humans are at risk of infection. 

i-c-u-curve2020 is a generative multimedia installation exploring isolation, inclusion and humanity using real-time data. Through live webcam, the audience is submersed and morphs into the data, creating an evolving soundscape with an evolving visual landscape. 
In its inaugural iterations, static pieces of text were used to create the sound and visual compositions, a webcam, responded to the viewer by submersing their image within the text. The effect, made the audience complicit in the information held within the work, creating a narrative of inclusion. i-c-u-curve2020 builds on these earlier iterations by using dynamic data, the constantly changing information pulled from the COVID19 API webpage. 

Artist Laurie Anderson said, “My job is to make images and leave the decision- making and conclusion-drawing to other people”. This underscores my motivation to use data as a medium or parameter. Through creative abstraction and methods of visualisation and sonification, the result is a work of generative art that seeks to uncover meaning not necessarily implicit in the raw data. As a multimodal artist a driver for my work is to make the unheard heard and the unseen seen. i-c-u-curve2020 uses COVID19 data as both composition and brush and looks to highlight the ultimate inclusive humanity through the isolationist individual. 
Accessing website API data from COVID19 cases and status for both the sound and vision creates a dynamic real-time generative installation. The piece becomes site responsive as the data is mapped to display the audience. The sound element of the piece is also derived from the data. Case numbers set the parameters for the synthesised tones built within the browsers with Tone.js

These elements of shifting data give the composition a rhythm and generative evolution through time. The results are driving and rhythmic, as the complexity of the image increases revealing the audience image onscreen and within the composition. 
Picasso once said “There is no abstract art. You must always start with something. Afterward you can remove all traces of reality.” While i-c-u-curve2020 definitely starts with something, it uses the method of abstraction not to remove reality, but to highlight it. 

Access Notes: https://tobeheard.github.io/i-c-u-curve2020/ This is a live webpage link. The piece can be accessed at anytime by anyone with the link. Viewers will be asked for access to their cameras so as to display the generative visual. The soundscape continues for 12min. The image will continue to draw as long as the page is open. All browsers with the exception of Internet Explorer have been tested and to work (internet explorer is not supported). The sound is best heard on headphones or through a pair of HiFi/studio stereo speakers (laptop and bluetooth not recommended!). Due to the efficiency of different browsers and the general speed of individual internet access and computer processors speed, each audience members experience may be different. It is recommended not to have multiple browsers or windows/tabs open for the fullest experience of this work. Thank you, enjoy.
","Sharyn Brand is an installation artist and collaborator focusing on sound-based urban ethnography, navigating the complexity of the world by uncovering the hidden and forgotten. In 2019 Sharyn received a Graduate Diploma of Music in Sonic Art from Te Kōkī, New Zealand School of Music Victoria University, Wellington.
Career highlights: Movement Extract (2019), Te Pātaka Toi, Adam Art Gallery Wellington NZ; Colour Labyrinth (2018/19), ArtPlay’s New Ideas Lab; Reflection (2017), Phantasmagoria, Bogong Sound Village; Maybe_Together’s Small Voices Louder Perth International Festival (2017); Renae Shadler’s Can You See What We See? Junction Arts Festival (2016); Blood.Sex.Tears. Women Of the World (2017).",https://drive.google.com/file/d/1cg_JstMvB8lRihxy7HQwrgKb1V4vYqrd/view,https://tobeheard.github.io/i-c-u-curve2020/ ,https://tobeheard.github.io/i-c-u-curve2020/ ,music,C1_Monday,TBA,,,,
91,"Abletweet: Harnessing Social Media APIs  for Encoding, Co-Creating and Performing Improvised Generative Electronic Music",James Curtis,https://www.dropbox.com/sh/7l9q57kuokzvku5/AABQPNpyQB7VdFX-sMpxrKeua?dl=0,"Abletweet is a novel Max For Live device which uses the Twitter API and Node.JS to facilitate improvised generative electronic music performances inside Ableton Live. The generation of MIDI sequence data from tweet text content is shaped parametrically by the performer using musically appropriate functions such as scale and rhythmic quantisation. Keywords derived from Sentiment Analysis using the AFINN-165 wordlist and Emoji Sentiment Ranking are used to accent the generated sequences using minor and major triad chords in accordance with their word ranking. Designed with consideration to bi-directionality, existing MIDI clips created by the performer can also be encoded as tweets and sent over twitter to other users and imported into Ableton in real time - harnessing social media as communication platform for creativity, co-creation and performance. Abletweet seeks to both encourage artists to harness social media APIs as a performable bidirectional networking protocol and consider openly accessible data as an available medium with which to create musical works.","James Curtis is a graduate researcher and sessional lecturer at RMIT University, Melbourne. Having completed a double degree in Fine Art (Sound and Spatial Practice) and Design (Industrial - First Class Honours) James received the Vice Chancellor's Award for Academic Excellence for his research in sound and interaction design in 2017. James is currently undertaking PhD in the RMIT School of Design on a full-time scholarship award, his research focus is developing Artificial Intelligence-mediated creative design tools for musicians.",https://www.dropbox.com/sh/7l9q57kuokzvku5/AABQPNpyQB7VdFX-sMpxrKeua?dl=0,,http://www.jcurtis.cc,paper,cult-comm-eco,3,,,,
92,Dim Unit,Damian Mason,https://www.dropbox.com/s/8t3hpk20gm30trv/Dim%20Unit%20ACMC2020%20submission%2092.wav?dl=0,"An improvisation where ambient/drone guitar meets glitchy electronica. Minimalist electric guitar, micro-synths and Axoloti microcontroller are processed using analog and digital stompboxes and further mangled and augmented using Plogue Bidule modular audio software.

The performer “begin(s) anywhere” and relies on spontaneity and serendipity, pushing to to the edge of discomfort and vulnerability. Layers of asynchronous loops build to create complex and beautiful textures.

Performed in one take, no edits. Recorded on 19th June 2020.","Damian Mason has been creating and performing music since the early 1990s. He has played in pop/shoegaze, industrial and ambient/electronic bands, and composed for film, TV and theatre. He currently works in community services and makes music on his days off while his kids are at school. His current project Dim Unit has been his main focus for the past 5 years.",https://www.dropbox.com/s/37dztu5qcln4kpe/Dim%20Unit%20ACMC2020%20image.jpg?dl=0,https://dimunit.bandcamp.com/track/acmc2020,http://www.damianmason.com.au,music,P1_Monday,TBA,,,,https://dimunit.bandcamp.com/track/acmc2020
93,Reflections from the outside,Benjamin Keough,https://drive.google.com/file/d/1SI5goa29yUkGI89fIVFQzuhOyNIHe14z/view?usp=sharing,"This work is part of an ongoing interest in the intersection of acoustic ecology, novel synthesis techniques and generative composition. It imitates and deconstructs the timbral, structural and textural aspects of natural sound environments using a combination of processed field recordings and self-playing/generative synthesizer patches developed in the Pure Data visual programming environment. ","Ben Keough is an experimental composer/improviser and amateur photographer/videographer currently living in Bellingen on the NSW Mid North Coast. A graduate of the ANU School of Music, his composition work is based around digital synthesis and generative sequencing techniques (using the Pure Data development environment), field recording and electroacoustic improvisation.",https://drive.google.com/file/d/13Q2OG8j2DopYI_KJTLRVX4J_XxiCM8Vu/view?usp=sharing,,,,TBA,TBA,,,,
94,Solo : Confabulate,Carl Polke,https://drive.google.com/file/d/1lfCfbLk8UJTcoWoTm0xrwuc_61QROEgi/view?usp=sharing,"Confabulation:

1. Engage in conversation; talk.
2. A memory error defined as the production of fabricated, distorted, or misinterpreted memories about oneself or the world, without the conscious intention to deceive.
Input sensitivity monitoring coupled to envelope generators and modifiers mapped to parameters within effects units and blocks allows for interplay with the effects themselves.

A conversation can be had with the machine.

Though the saxophone is the primary sound-source and performative control element, the acoustic sound of the instrument itself is not necessarily featured. Choice as to when or if the acoustic sound of the saxophone is involved is made on an improvisational basis in response to the sonic territory of the moment, and forms part of the performative outcome. 

Conceptually, the work speaks to the tensions inherent in the digital/real world intersection with which we constantly contend.

Access to the digital world is via familiar objects, phone, tablet, computer, but what lies beyond is ever changing, leading to a form of cognitive dissonance and engaging the mind as it comes to terms with whatever “reality” is presented.

A familiar object/sight, the saxophone/player, creates a sound so far removed from expectation that a form of cognitive dissonance is created, the observer is engaged as the mind comes to terms with this novel form of “reality”.

Solo confabulation.

Presented are four short improvisational pieces representative of the practice as outlined:

1. Smooth Times in Wowville: Amplitude at input (i.e. sax breath control) tied to several parameters, mostly pitch related. The acoustic sound converses with the machine pixies as they constantly try escape…. This effect is very delicately balanced (and a LOT of fun to play with) small variations in amplitude mapped to big changes allowing the possibility to play almost silently and still create much sound. 
2. Ahoy: Basically a random LFO tied to filter/s creates a harsh rhythm to battle against and play with. Based on Zappa’s guitar sound from the “Ship Ahoy” guitar solo.
3. Diamond Flower: The speed of the “stutter” effect is reliant on amplitude at input, the graph overlay is a real-time capture of the envelope parameter reacting to amplitude of sax through the pickup. A damping factor is applied to the envelope response to allow for more precise control. 
4. Harnk: Synthesis and randomness..due to the nature of the pitch follower and how it reads the complex noise from the sax, the synth doesn’t always go where you’d think it should….


","Carl Polke: A musician/sound-designer/performer living in Melbourne, Australia, whose current performative practise involves free improvisation utilising digital audio effects with alto saxophone as the primary sound source. At a conceptual level it interrogates the relationship between society/the individual and the digital realm. The work is perceived as evolving sound-design, and is presented as both solo and as part of improvisational duo The Confabulations. His project The Endless Guitar Solo received funding from The City of Melbourne Quick Response Arts grant and is currently in development, and he is collaborative/co-artist in the Audible Lockdown project awarded a 2020 Peggy Glanville-Hicks Commission. With over 30 years’ experience in the physical theatre/circus and live theatre industries, he has worked with many of Australia’s leading companies including Circus Oz, Legs on the Wall, Phunktional, The Flying Fruit Fly Circus many more. His work is known for its animated style and theatrical sensibility, and has achieved both national and international recognition, winning a Canadian Jesse award and received Helpmann nominations.",https://drive.google.com/file/d/1vbeogYcREo1Qf9K3epjfNcleRxZ49wNR/view?usp=sharing,https://youtu.be/gZsaFtYompo,https://www.carlpolke.com,music,C4_Thursday,TBA,,https://youtu.be/gZsaFtYompo,,
95,Abletweet (Performance / live demonstration),J. Curtis (lysdexic),https://www.dropbox.com/sh/7l9q57kuokzvku5/AABQPNpyQB7VdFX-sMpxrKeua?dl=0,"This short (15 minute) performance submission accompanies the ‘Abletweet: Harnessing Social Media APIs for Encoding, Co-Creating and Performing Improvised Generative Electronic Music.’ research paper / artist talk submission for ACMC2020. This performance work seeks to explore the improvisational possibilities offered by the Abletweet device by restricting all melodic and percussive sequences to patterns generated by the device in real time, using digital and analogue hardware.","lysdexic began its existence as dark experiments in freeform sound design, audio manipulation and textural soundsculpting before moving into rhythmic based composition. Using frameworks such as MaxMSP, Arduino, C++ and Python to create custom DSP tools and sampling everything from scratched discs, hacked data files, circuit bent kids toys and his ever growing collection of modified drum machines, lysdexic continues to explore and destroy the audio spectrum.",https://www.dropbox.com/sh/7l9q57kuokzvku5/AABQPNpyQB7VdFX-sMpxrKeua?dl=0,,http://www.lysdexic.com,music,C2_Tuesday,TBA,,,,
96,Peace Wall Belfast: Spatial Audio Representation of Divided Spaces and Soundwalks,Georgios Varoutsos,https://drive.google.com/file/d/1diAaAn31PSQdMAlkrbfXzi0iOiAkteHQ/view?usp=sharing,"In West Belfast lays the Peace Wall Belfast, a manifestation of multifaced messages on political, religious, and communal ideals represented by physical properties of cement, metal, fences, gates, and artwork. There have been discussions on initiatives to take down the walls, however, this remains a fragile state. When thinking about the connectivity of the surrounding spaces and communities, the placing of the Peace Wall(s) blocks any opportunity of cross-communication and produces disorienting effects. However, through alternative artistic approaches focusing on sound, there can be innovative capabilities of sharing these stories and spaces with spatial audio techniques. To use spatial audio to change the perception of these spaces brings forth alternative periods of reflections from stimulating another sensory tool other than sight. Forming two unique listening experiences that focus on the virtual abilities to combine auditory spaces into an immersive installation environment and binaural soundwalks to design site-specific augmentation of the sonic properties of the Peace Wall’s surrounding spaces. These projects aim at using spatial audio and artistic practice to plan new approaches for conflict transformation in Northern Ireland.","Georgios Varoutsos (b.1991) is a sonic artist from Montreal, Canada. He is currently completing his Ph.D. studies in Music at the Sonic Arts Research Centre (SARC) at Queen’s University Belfast, Northern Ireland. He has graduated with a Master’s in Research, Pass with Distinction, in Arts & Humanities – Focus in Sonic Arts at Queen’s University Belfast. He has also completed a BFA with Distinction in Electroacoustic Studies and a BA in Anthropology, both from Concordia University in Montreal, Canada.

He explores the field of sound through an extensive range of projects and performances. His audio creations derive from different inspirations such as field recordings, digital recordings, amplified sound materials, audio processing, synthesis, and experimental techniques. Georgios is merging his various backgrounds of study into research projects comprising immersive audio, sonification, urban arts, sonic arts, and socially engaged arts. This has been presented by using sound as a platform for cultural storytelling.
", https://drive.google.com/file/d/1qWfipMiKttzBcdSP-5MlyMN5INbdQUEA/view?usp=sharing,https://youtu.be/88Q91XPFF_A,https://www.georgiosvaroutsos.com,paper,spatial-perf,5,,https://youtu.be/88Q91XPFF_A,,
97,Unseen - for modular synthesiser and live visuals,Benjamin Carey,need to organise time for live concert,TBA,,,,,music,TBA,TBA,,,,
98,Divided Spaces,Georgios Varoutsos,https://drive.google.com/drive/folders/1GqIN9nP5aqCSoQYAJzy5juXE7eHh5GK3?usp=sharing,"In West Belfast lays the Peace Wall Belfast, a manifestation of multifaced messages on political, religious, and communal ideals represented by physical properties of cement, metal, fences, gates, and artwork. There have been discussions on initiatives to take down the walls, however, this remains a fragile state. When thinking about the connectivity of the surrounding spaces and communities, the placing of the Peace Wall(s) blocks any opportunity of cross-communication and produces disorienting effects. However, through alternative artistic approaches focusing on sound, there can be innovative capabilities of sharing these stories and spaces with spatial audio techniques. To use spatial audio to change the perception of these spaces brings forth alternative periods of reflections from stimulating another sensory tool other than sight. Forming two unique listening experiences that focus on the virtual abilities to combine auditory spaces into an immersive installation environment and binaural soundwalks to design site-specific augmentation of the sonic properties of the Peace Wall’s surrounding spaces. These projects aim at using spatial audio and artistic practice to plan new approaches for conflict transformation in Northern Ireland.

Divided Spaces by Georgios Varoutsos is an audio immersive piece focusing on the Peace Wall between the Falls and Shankill road in Belfast, Northern Ireland. Representing connectivity of spaces and experiences through sound to highlight Past, Present, and Future relations with the Peace Wall(s). It hosts an immersive sonic round-table discussion on controversial issues concerned with the history of the ‘Troubles’ and the status of Peace Wall(s) around the country.

The piece compresses the large surface areas surrounding the Peace Wall into a room listening experience. Demonstrating the separation and isolation from either side’s community spaces.
","Georgios Varoutsos (b.1991) is a sonic artist from Montreal, Canada. He is currently completing his Ph.D. studies in Music at the Sonic Arts Research Centre (SARC) at Queen’s University Belfast, Northern Ireland. He has graduated with a Master’s in Research, Pass with Distinction, in Arts & Humanities – Focus in Sonic Arts at Queen’s University Belfast. He has also completed a BFA with Distinction in Electroacoustic Studies and a BA in Anthropology, both from Concordia University in Montreal, Canada.

He explores the field of sound through an extensive range of projects and performances. His audio creations derive from different inspirations such as field recordings, digital recordings, amplified sound materials, audio processing, synthesis, and experimental techniques. Georgios is merging his various backgrounds of study into research projects comprising immersive audio, sonification, urban arts, sonic arts, and socially engaged arts. This has been presented by using sound as a platform for cultural storytelling.
",https://drive.google.com/file/d/1kn2P5RpzshmAKOTNYiHj7OKBCPMR7dbt/view?usp=sharing,https://soundcloud.com/georgiosvaroutsos/sets/divided-spaces,https://www.georgiosvaroutsos.com,music,P3_Wednesday,TBA,https://soundcloud.com/georgiosvaroutsos/sets/divided-spaces,,,
99,Dark Path #4,Anna Terzaroli,https://www.dropbox.com/s/5ehts3gi1i7w14x/DarkPath%234.mp3?dl=0,"""Dark Path #4"" is an acousmatic work of electroacoustic music. The sounds used in the piece were recorded in a soundscape dear to the author located in the Italian region of Marche. They were processed then composed together to create the musical work. ""Dark Path #4"" can be defned as a journey through light, shadow, shape, color, drifts and landings.

","Anna Terzaroli studied Electronic Music and Composition at the Santa Cecilia Conservatory of Music in Rome, as a composer she is dedicated to contemporary acoustic and electroacoustic music.

Her original works are selected and performed in concerts and festivals in Italy and abroad. Anna Terzaroli currently holds a Professor position in Electronic Music at Italian Conservatories. She is a member of the AIMI (Italian Computer Music Association) board.",,,,music,P2_Tuesday,TBA,,,,
100,THESE WOULD BE OTHER,Brigid Burke|Sophie Rose,https://www.dropbox.com/preview/Art%20work%20by%20Brigid%20Burke%20for%20Ruark%20Lewis/These%20would%20be%20Other%20Nov%202020.mov?role=personal,"These Would Be Other is a collaborative work spearheaded by Brigid Burke and Sophie Rose. Composed over a year, the piece uses video, live and pre-recorded audio, and electronics. Burke layers visuals and audio to further distort Chris Mann’s text. The work was initially conceived as a tribute to Chris Mann’s life and oeuvre and published in Open Space Magazine as Two for Chris.

These Would Be Other morphs the ideas of tribute and present-day considerations. The text was delivered by Sophie Rose at different points of exposure to the poem, from initial reading to experienced live delivery. This supplies an increasing complexity in delivery, pitch, and tonality in the pre-recorded. This complexity is further amplified by Burke’s echolalic audio distortions. Burke superimposes multiple layers of sound and video to create a fractured image of the fractal and recursive nature of the text and hues.
","Dr. Brigid Burke 
Biography

Brigid is an Australian composer, performance artist, clarinet soloist, visual artist, video artist and educator whose creative practice explores the use of acoustic sound and technology to enable media performances and installations that are rich in aural and visual nuances. Her work is widely presented in concerts, festivals, and radio broadcasts throughout Australia, Asia, Europe and the USA. 
Recently she has been a recipient of an Australia Council Project Music Fellowship & new work commissions ‘Coral Bells’ & “Instincts and Episodes’ also Artist in Resident at Marshall University USA with a Edwards Distinguished Professor Artist Residency, Indiana University USA and ADM NTU Singapore. Also most recently she has presented her works on the Big screen at Federation Square Melbourne, Lontano Festival Internaciol Musica Contemoranea em Goiania Brazil, SEAMUS USA, Tilde Festival Melb Australia, ABC Classic FM. ICMC International Computer Music Conference Perth Australia, Echofluxx 14 to19 Festivals Prague, Generative Arts Festivals in Ravena, Florence, Rome & Milan Italy, Asian Music Festivals in Tokyo, The Melbourne International Arts Festival, Futura Music Festival Paris France, Mona Foma Festival Hobart, The International Clarinet Festivals in Japan and Canada also Seoul and Australian International Computer Music Festivals. She has a PhD in Composition from UTAS and a Master of Music in Composition from The University of Melbourne.
www.brigid.com.au

",100,,https://vimeo.com/397733269,music,C4_Thursday,TBA,,,https://vimeo.com/397733269,
103,Virtual Nature: Inner Forest,Susannah Langley|Jessica Laraine Williams|Ann Borda,https://www.dropbox.com/s/qq4a2v5hq6lcbfx/ACMCPaperJW_SL_AB_v2.mp4?dl=0,"Studies have revealed compelling relationships between experiences of the natural environment and positive health outcomes in adult communities, primarily around lowering stress responses and increasing feelings of wellbeing. These psychosocial health benefits are frequently described via key theoretical frameworks, including the biophilia hypothesis, attention-restoration theory and stress-reduction theory. A number of studies have evaluated technological nature and human wellbeing; however, the wellbeing benefits of immersive, multisensory virtual reality (VR) and augmented reality (AR) nature experiences are still emerging in the research. Additionally, broadening evidence around composed or conceptualised nature and human wellbeing urges new possibilities for artistic, abstract and creative experiences.
Inner Forest is a virtual nature artwork in development, adapted from its original immersive VR proposal to augmented reality (AR) as a result of COVID-19 impacts and restrictions. This AR format is self-directed, enabling accessibility and inclusion for diverse user groups, ages and abilities and is designed to be experienced on demand. Each experience of the artwork is unique to audience interactions/contributions whilst engaged with the AR environment. Nature-evocative AR vision, audio and haptic elements will be demonstrated at the ACMC 2020. Conference attendees are invited to view the Inner Forest artwork online via the supplied link, and contributors Susannah Langley and Jessica Laraine Williams (The University of Melbourne) will be demonstrating the work and discussing its background rationale in their paper. The design of the artwork is aligned with Browning et al.’s 2014 Biophilic Design guidelines, which support contemporary approaches to nature that include living organisms, non-living components, and designed elements. Inner Forest ultimately aims to offer audiences with a creative, playful experience of virtual nature art usable in both actual nature and in scenarios where actual nature is inaccessible. Scaling of the project will include delivery via a mobile phone app, haptic touch booklet and aromatherapy diffuser to elicit a multi-sensory, portable experience of a virtual nature ecosystem. Future scaling for the artwork involves staging immersive virtual nature experiences using 360 degree/CAVE equipment at the Virtual Reality Learning Studio (VRLS), The University of Melbourne.
 
For further information on Inner Forest artwork on Vimeo, please use https://vimeo.com/431305714 and Inner Forest AR mobile app prototype, on Vimeo, https://vimeo.com/424461493 .
","Susannah Langley is a visual artist whose practice is rooted in experimental drawing, installation and sound, often using unconventional media such as conductive material, found objects, field recording and virtual reality to explore ideas of history, memory, movement, feeling, and space. 
Since 2013, she has collaborated on works that people can move through, and touch, to summon stories and soundscapes, and primarily taken the form of virtual reality experiences and large scale installations. These works have been featured in exhibitions, festivals, prizes and residencies both nationally and internationally. Susannah was the winner of the 2017 Paramor Art + Innovation Prize, Casula Powerhouse. In addition to her arts practice, she also delivers creative tech-based workshops to a variety of age groups. 
Susannah is a current Master Researcher at the Faculty of Fine Arts and Music, University of Melbourne researching drawing and sound in a virtual environment. In addition, she is a current artist in residence with the Centre of Projection Art, Melbourne and recent resident at Testing Grounds Studios, Southbank Melbourne. Website: https://www.susannahlangley.com/ Email:susannah.langley@student.unimelb.edu.au

Jessica Laraine Williams is a transdisciplinary researcher, visual artist, writer and part time PhD candidate at the Faculty of Fine Arts and Music, The University of Melbourne. In 2020, she is undertaking an academic associate/sessional tutor role with the Faculty, teaching within Critical and Theoretical Studies. Jess has been working for a decade in her physiotherapist profession, including hospital, rehabilitation and management roles. She now specialises in aged care physiotherapy part time. Jess is undertaking her doctoral research into methodologies of posthumanism in art. This includes collaborations with researchers across and beyond her home faculty and institution. Her broader creative work relates to interests in performative identity, relational cartography (systems) and institutional critique. Jess holds first class Honours degrees in both the Bachelor of Physiotherapy (Monash University) and the Bachelor of Fine Art (The Faculty of Fine Arts and Music, The University of Melbourne). She has written for art publications both academic and popular, such as the Conversation and Art+Australia.Website: www.jlogos.net  Email: jess.williams@unimelb.edu.au

Ann Borda is Associate Professor in the Centre for the Digital Transformation of Health at The University of Melbourne and Fellow of the Australasian Institute of Digital Health. Ann has a PhD in information science from University College London which has served as a springboard for her commitment towards transdisciplinary scholarship.  She has held senior positions in computing and data-intensive research initiatives in the UK and Australia. Ann has extensive experience in mentorship, open knowledge, and social innovations in both smart health and cultural heritage.   Such interests were fostered during her time at the Science Museum London in building digital collections to support communication in public science and medical discovery, and in managing an open source software portfolio across several UK higher education institutions.  Ann presently sits on the Research and Policy Committee of the Climate and Health Alliance.  Among many knowledge exchange events, she has co-organised public forums on automation, well-being and society with the Alan Turing Institute. Recently Ann received an EPIC grant under the EU Horizon 2020 programme (ICT) to investigate advancing approaches to health and biomedical citizen science methods, platforms and capabilities. 
Website:  http://www.findanexpert.unimelb.edu.au/display/person197899
",https://www.dropbox.com/s/fv04o6l1ask5g69/Inner%20Forest_Gertrude%20Street%20Project%20Festival_2019.jpg?dl=0,https://vimeo.com/431414088,https://www.susannahlangley.com/ ,music,TBA,TBA,,,https://vimeo.com/431414088,